<!--Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

[[open-in-colab]]

# Tour rÃ¡pido

Modelos de difusÃ£o sÃ£o treinados para remover o ruÃ­do Gaussiano aleatÃ³rio passo a passo para gerar uma amostra de interesse, como uma imagem ou Ã¡udio. Isso despertou um tremendo interesse em IA generativa, e vocÃª provavelmente jÃ¡ viu exemplos de imagens geradas por difusÃ£o na internet. ğŸ§¨ Diffusers Ã© uma biblioteca que visa tornar os modelos de difusÃ£o amplamente acessÃ­veis a todos.

Seja vocÃª um desenvolvedor ou um usuÃ¡rio, esse tour rÃ¡pido irÃ¡ introduzir vocÃª ao ğŸ§¨ Diffusers e ajudar vocÃª a comeÃ§ar a gerar rapidamente! HÃ¡ trÃªs componentes principais da biblioteca para conhecer:

- O [`DiffusionPipeline`] Ã© uma classe de alto nÃ­vel de ponta a ponta desenhada para gerar rapidamente amostras de modelos de difusÃ£o prÃ©-treinados para inferÃªncia.
- [Modelos](./api/models) prÃ©-treinados populares e mÃ³dulos que podem ser usados como blocos de construÃ§Ã£o para criar sistemas de difusÃ£o.
- VÃ¡rios [Agendadores](./api/schedulers/overview) diferentes - algoritmos que controlam como o ruÃ­do Ã© adicionado para treinamento, e como gerar imagens sem o ruÃ­do durante a inferÃªncia.

Esse tour rÃ¡pido mostrarÃ¡ como usar o [`DiffusionPipeline`] para inferÃªncia, e entÃ£o mostrarÃ¡ como combinar um modelo e um agendador para replicar o que estÃ¡ acontecendo dentro do [`DiffusionPipeline`].

> [!TIP]
> Esse tour rÃ¡pido Ã© uma versÃ£o simplificada da introduÃ§Ã£o ğŸ§¨ Diffusers [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) para ajudar vocÃª a comeÃ§ar rÃ¡pido. Se vocÃª quer aprender mais sobre o objetivo do ğŸ§¨ Diffusers, filosofia de design, e detalhes adicionais sobre a API principal, veja o notebook!

Antes de comeÃ§ar, certifique-se de ter todas as bibliotecas necessÃ¡rias instaladas:

```py
# uncomment to install the necessary libraries in Colab
#!pip install --upgrade diffusers accelerate transformers
```

- [ğŸ¤— Accelerate](https://huggingface.co/docs/accelerate/index) acelera o carregamento do modelo para geraÃ§Ã£o e treinamento.
- [ğŸ¤— Transformers](https://huggingface.co/docs/transformers/index) Ã© necessÃ¡rio para executar os modelos mais populares de difusÃ£o, como o [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview).

## DiffusionPipeline

O [`DiffusionPipeline`] Ã© a forma mais fÃ¡cil de usar um sistema de difusÃ£o prÃ©-treinado para geraÃ§Ã£o. Ã‰ um sistema de ponta a ponta contendo o modelo e o agendador. VocÃª pode usar o [`DiffusionPipeline`] pronto para muitas tarefas. DÃª uma olhada na tabela abaixo para algumas tarefas suportadas, e para uma lista completa de tarefas suportadas, veja a tabela [Resumo do ğŸ§¨ Diffusers](./api/pipelines/overview#diffusers-summary).

| **Tarefa**                             | **DescriÃ§Ã£o**                                                                                                             | **Pipeline**                                                                       |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| Unconditional Image Generation         | gera uma imagem a partir do ruÃ­do Gaussiano                                                                               | [unconditional_image_generation](./using-diffusers/unconditional_image_generation) |
| Text-Guided Image Generation           | gera uma imagem a partir de um prompt de texto                                                                            | [conditional_image_generation](./using-diffusers/conditional_image_generation)     |
| Text-Guided Image-to-Image Translation | adapta uma imagem guiada por um prompt de texto                                                                           | [img2img](./using-diffusers/img2img)                                               |
| Text-Guided Image-Inpainting           | preenche a parte da mÃ¡scara da imagem, dado a imagem, a mÃ¡scara e o prompt de texto                                       | [inpaint](./using-diffusers/inpaint)                                               |
| Text-Guided Depth-to-Image Translation | adapta as partes de uma imagem guiada por um prompt de texto enquanto preserva a estrutura por estimativa de profundidade | [depth2img](./using-diffusers/depth2img)                                           |

Comece criando uma instÃ¢ncia do [`DiffusionPipeline`] e especifique qual checkpoint do pipeline vocÃª gostaria de baixar.
VocÃª pode usar o [`DiffusionPipeline`] para qualquer [checkpoint](https://huggingface.co/models?library=diffusers&sort=downloads) armazenado no Hugging Face Hub.
Nesse quicktour, vocÃª carregarÃ¡ o checkpoint [`stable-diffusion-v1-5`](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5) para geraÃ§Ã£o de texto para imagem.

> [!WARNING]
> Para os modelos de [Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion), por favor leia cuidadosamente a [licenÃ§a](https://huggingface.co/spaces/CompVis/stable-diffusion-license) primeiro antes de rodar o modelo. ğŸ§¨ Diffusers implementa uma verificaÃ§Ã£o de seguranÃ§a: [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) para prevenir conteÃºdo ofensivo ou nocivo, mas as capacidades de geraÃ§Ã£o de imagem aprimorada do modelo podem ainda produzir conteÃºdo potencialmente nocivo.

Para carregar o modelo com o mÃ©todo [`~DiffusionPipeline.from_pretrained`]:

```python
>>> from diffusers import DiffusionPipeline

>>> pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", use_safetensors=True)
```

O [`DiffusionPipeline`] baixa e armazena em cache todos os componentes de modelagem, tokenizaÃ§Ã£o, e agendamento. VocÃª verÃ¡ que o pipeline do Stable Diffusion Ã© composto pelo [`UNet2DConditionModel`] e [`PNDMScheduler`] entre outras coisas:

```py
>>> pipeline
StableDiffusionPipeline {
  "_class_name": "StableDiffusionPipeline",
  "_diffusers_version": "0.13.1",
  ...,
  "scheduler": [
    "diffusers",
    "PNDMScheduler"
  ],
  ...,
  "unet": [
    "diffusers",
    "UNet2DConditionModel"
  ],
  "vae": [
    "diffusers",
    "AutoencoderKL"
  ]
}
```

NÃ³s fortemente recomendamos rodar o pipeline em uma placa de vÃ­deo, pois o modelo consiste em aproximadamente 1.4 bilhÃµes de parÃ¢metros.
VocÃª pode mover o objeto gerador para uma placa de vÃ­deo, assim como vocÃª faria no PyTorch:

```python
>>> pipeline.to("cuda")
```

Agora vocÃª pode passar o prompt de texto para o `pipeline` para gerar uma imagem, e entÃ£o acessar a imagem sem ruÃ­do. Por padrÃ£o, a saÃ­da da imagem Ã© embrulhada em um objeto [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=image#the-image-class).

```python
>>> image = pipeline("An image of a squirrel in Picasso style").images[0]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/image_of_squirrel_painting.png"/>
</div>

Salve a imagem chamando o `save`:

```python
>>> image.save("image_of_squirrel_painting.png")
```

### Pipeline local

VocÃª tambÃ©m pode utilizar o pipeline localmente. A Ãºnica diferenÃ§a Ã© que vocÃª precisa baixar os pesos primeiro:

```bash
!git lfs install
!git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5
```

Assim carregue os pesos salvos no pipeline:

```python
>>> pipeline = DiffusionPipeline.from_pretrained("./stable-diffusion-v1-5", use_safetensors=True)
```

Agora vocÃª pode rodar o pipeline como vocÃª faria na seÃ§Ã£o acima.

### Troca dos agendadores

Agendadores diferentes tem diferentes velocidades de retirar o ruÃ­do e compensaÃ§Ãµes de qualidade. A melhor forma de descobrir qual funciona melhor para vocÃª Ã© testar eles! Uma das principais caracterÃ­sticas do ğŸ§¨ Diffusers Ã© permitir que vocÃª troque facilmente entre agendadores. Por exemplo, para substituir o [`PNDMScheduler`] padrÃ£o com o [`EulerDiscreteScheduler`], carregue ele com o mÃ©todo [`~diffusers.ConfigMixin.from_config`]:

```py
>>> from diffusers import EulerDiscreteScheduler

>>> pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", use_safetensors=True)
>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)
```

Tente gerar uma imagem com o novo agendador e veja se vocÃª nota alguma diferenÃ§a!

Na prÃ³xima seÃ§Ã£o, vocÃª irÃ¡ dar uma olhada mais de perto nos componentes - o modelo e o agendador - que compÃµe o [`DiffusionPipeline`] e aprender como usar esses componentes para gerar uma imagem de um gato.

## Modelos

A maioria dos modelos recebe uma amostra de ruÃ­do, e em cada _timestep_ ele prevÃª o _noise residual_ (outros modelos aprendem a prever a amostra anterior diretamente ou a velocidade ou [`v-prediction`](https://github.com/huggingface/diffusers/blob/5e5ce13e2f89ac45a0066cb3f369462a3cf1d9ef/src/diffusers/schedulers/scheduling_ddim.py#L110)), a diferenÃ§a entre uma imagem menos com ruÃ­do e a imagem de entrada. VocÃª pode misturar e combinar modelos para criar outros sistemas de difusÃ£o.

Modelos sÃ£o inicializados com o mÃ©todo [`~ModelMixin.from_pretrained`] que tambÃ©m armazena em cache localmente os pesos do modelo para que seja mais rÃ¡pido na prÃ³xima vez que vocÃª carregar o modelo. Para o tour rÃ¡pido, vocÃª irÃ¡ carregar o [`UNet2DModel`], um modelo bÃ¡sico de geraÃ§Ã£o de imagem incondicional com um checkpoint treinado em imagens de gato:

```py
>>> from diffusers import UNet2DModel

>>> repo_id = "google/ddpm-cat-256"
>>> model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)
```

Para acessar os parÃ¢metros do modelo, chame `model.config`:

```py
>>> model.config
```

A configuraÃ§Ã£o do modelo Ã© um dicionÃ¡rio ğŸ§Š congelado ğŸ§Š, o que significa que esses parÃ¢metros nÃ£o podem ser mudados depois que o modelo Ã© criado. Isso Ã© intencional e garante que os parÃ¢metros usados para definir a arquitetura do modelo no inÃ­cio permaneÃ§am os mesmos, enquanto outros parÃ¢metros ainda podem ser ajustados durante a geraÃ§Ã£o.

Um dos parÃ¢metros mais importantes sÃ£o:

- `sample_size`: a dimensÃ£o da altura e largura da amostra de entrada.
- `in_channels`: o nÃºmero de canais de entrada da amostra de entrada.
- `down_block_types` e `up_block_types`: o tipo de blocos de downsampling e upsampling usados para criar a arquitetura UNet.
- `block_out_channels`: o nÃºmero de canais de saÃ­da dos blocos de downsampling; tambÃ©m utilizado como uma order reversa do nÃºmero de canais de entrada dos blocos de upsampling.
- `layers_per_block`: o nÃºmero de blocks ResNet presentes em cada block UNet.

Para usar o modelo para geraÃ§Ã£o, crie a forma da imagem com ruÃ­do Gaussiano aleatÃ³rio. Deve ter um eixo `batch` porque o modelo pode receber mÃºltiplos ruÃ­dos aleatÃ³rios, um eixo `channel` correspondente ao nÃºmero de canais de entrada, e um eixo `sample_size` para a altura e largura da imagem:

```py
>>> import torch

>>> torch.manual_seed(0)

>>> noisy_sample = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)
>>> noisy_sample.shape
torch.Size([1, 3, 256, 256])
```

Para geraÃ§Ã£o, passe a imagem com ruÃ­do para o modelo e um `timestep`. O `timestep` indica o quÃ£o ruidosa a imagem de entrada Ã©, com mais ruÃ­do no inÃ­cio e menos no final. Isso ajuda o modelo a determinar sua posiÃ§Ã£o no processo de difusÃ£o, se estÃ¡ mais perto do inÃ­cio ou do final. Use o mÃ©todo `sample` para obter a saÃ­da do modelo:

```py
>>> with torch.no_grad():
...     noisy_residual = model(sample=noisy_sample, timestep=2).sample
```

Para geraÃ§Ã£o de exemplos reais, vocÃª precisarÃ¡ de um agendador para guiar o processo de retirada do ruÃ­do. Na prÃ³xima seÃ§Ã£o, vocÃª irÃ¡ aprender como acoplar um modelo com um agendador.

## Agendadores

Agendadores gerenciam a retirada do ruÃ­do de uma amostra ruidosa para uma amostra menos ruidosa dado a saÃ­da do modelo - nesse caso, Ã© o `noisy_residual`.

> [!TIP]
> ğŸ§¨ Diffusers Ã© uma caixa de ferramentas para construir sistemas de difusÃ£o. Enquanto o [`DiffusionPipeline`] Ã© uma forma conveniente de comeÃ§ar com um sistema de difusÃ£o prÃ©-construÃ­do, vocÃª tambÃ©m pode escolher seus prÃ³prios modelos e agendadores separadamente para construir um sistema de difusÃ£o personalizado.

Para o tour rÃ¡pido, vocÃª irÃ¡ instanciar o [`DDPMScheduler`] com o mÃ©todo [`~diffusers.ConfigMixin.from_config`]:

```py
>>> from diffusers import DDPMScheduler

>>> scheduler = DDPMScheduler.from_config(repo_id)
>>> scheduler
DDPMScheduler {
  "_class_name": "DDPMScheduler",
  "_diffusers_version": "0.13.1",
  "beta_end": 0.02,
  "beta_schedule": "linear",
  "beta_start": 0.0001,
  "clip_sample": true,
  "clip_sample_range": 1.0,
  "num_train_timesteps": 1000,
  "prediction_type": "epsilon",
  "trained_betas": null,
  "variance_type": "fixed_small"
}
```

> [!TIP]
> ğŸ’¡ Perceba como o agendador Ã© instanciado de uma configuraÃ§Ã£o. Diferentemente de um modelo, um agendador nÃ£o tem pesos treinÃ¡veis e Ã© livre de parÃ¢metros!

Um dos parÃ¢metros mais importante sÃ£o:

- `num_train_timesteps`: o tamanho do processo de retirar ruÃ­do ou em outras palavras, o nÃºmero de _timesteps_ necessÃ¡rios para o processo de ruÃ­dos Gausianos aleatÃ³rios dentro de uma amostra de dados.
- `beta_schedule`: o tipo de agendados de ruÃ­do para o uso de geraÃ§Ã£o e treinamento.
- `beta_start` e `beta_end`: para comeÃ§ar e terminar os valores de ruÃ­do para o agendador de ruÃ­do.

Para predizer uma imagem com um pouco menos de ruÃ­do, passe o seguinte para o mÃ©todo do agendador [`~diffusers.DDPMScheduler.step`]: saÃ­da do modelo, `timestep`, e a atual `amostra`.

```py
>>> less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample).prev_sample
>>> less_noisy_sample.shape
```

O `less_noisy_sample` pode ser passado para o prÃ³ximo `timestep` onde ele ficarÃ¡ ainda com menos ruÃ­do! Vamos juntar tudo agora e visualizar o processo inteiro de retirada de ruÃ­do.

Comece, criando a funÃ§Ã£o que faÃ§a o pÃ³s-processamento e mostre a imagem sem ruÃ­do como uma `PIL.Image`:

```py
>>> import PIL.Image
>>> import numpy as np


>>> def display_sample(sample, i):
...     image_processed = sample.cpu().permute(0, 2, 3, 1)
...     image_processed = (image_processed + 1.0) * 127.5
...     image_processed = image_processed.numpy().astype(np.uint8)

...     image_pil = PIL.Image.fromarray(image_processed[0])
...     display(f"Image at step {i}")
...     display(image_pil)
```

Para acelerar o processo de retirada de ruÃ­do, mova a entrada e o modelo para uma GPU:

```py
>>> model.to("cuda")
>>> noisy_sample = noisy_sample.to("cuda")
```

Agora, crie um loop de retirada de ruÃ­do que prediz o residual da amostra menos ruidosa, e computa a amostra menos ruidosa com o agendador:

```py
>>> import tqdm

>>> sample = noisy_sample

>>> for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):
...     # 1. predict noise residual
...     with torch.no_grad():
...         residual = model(sample, t).sample

...     # 2. compute less noisy image and set x_t -> x_t-1
...     sample = scheduler.step(residual, t, sample).prev_sample

...     # 3. optionally look at image
...     if (i + 1) % 50 == 0:
...         display_sample(sample, i + 1)
```

Sente-se e assista o gato ser gerado do nada alÃ©m de ruÃ­do! ğŸ˜»

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/diffusion-quicktour.png"/>
</div>

## PrÃ³ximos passos

Esperamos que vocÃª tenha gerado algumas imagens legais com o ğŸ§¨ Diffusers neste tour rÃ¡pido! Para suas prÃ³ximas etapas, vocÃª pode

- Treine ou faÃ§a a configuraÃ§Ã£o fina de um modelo para gerar suas prÃ³prias imagens no tutorial de [treinamento](./tutorials/basic_training).
- Veja exemplos oficiais e da comunidade de [scripts de treinamento ou configuraÃ§Ã£o fina](https://github.com/huggingface/diffusers/tree/main/examples#-diffusers-examples) para os mais variados casos de uso.
- Aprenda sobre como carregar, acessar, mudar e comparar agendadores no guia [Usando diferentes agendadores](./using-diffusers/schedulers).
- Explore engenharia de prompt, otimizaÃ§Ãµes de velocidade e memÃ³ria, e dicas e truques para gerar imagens de maior qualidade com o guia [Stable Diffusion](./stable_diffusion).
- Se aprofunde em acelerar ğŸ§¨ Diffusers com guias sobre [PyTorch otimizado em uma GPU](./optimization/fp16), e guias de inferÃªncia para rodar [Stable Diffusion em Apple Silicon (M1/M2)](./optimization/mps) e [ONNX Runtime](./optimization/onnx).
