<!--Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

[[open-in-colab]]

# Tour r√°pido

Modelos de difus√£o s√£o treinados para remover o ru√≠do Gaussiano aleat√≥rio passo a passo para gerar uma amostra de interesse, como uma imagem ou √°udio. Isso despertou um tremendo interesse em IA generativa, e voc√™ provavelmente j√° viu exemplos de imagens geradas por difus√£o na internet. üß® Diffusers √© uma biblioteca que visa tornar os modelos de difus√£o amplamente acess√≠veis a todos.

Seja voc√™ um desenvolvedor ou um usu√°rio, esse tour r√°pido ir√° introduzir voc√™ ao üß® Diffusers e ajudar voc√™ a come√ßar a gerar rapidamente! H√° tr√™s componentes principais da biblioteca para conhecer:

- O [`DiffusionPipeline`] √© uma classe de alto n√≠vel de ponta a ponta desenhada para gerar rapidamente amostras de modelos de difus√£o pr√©-treinados para infer√™ncia.
- [Modelos](./api/models) pr√©-treinados populares e m√≥dulos que podem ser usados como blocos de constru√ß√£o para criar sistemas de difus√£o.
- V√°rios [Agendadores](./api/schedulers/overview) diferentes - algoritmos que controlam como o ru√≠do √© adicionado para treinamento, e como gerar imagens sem o ru√≠do durante a infer√™ncia.

Esse tour r√°pido mostrar√° como usar o [`DiffusionPipeline`] para infer√™ncia, e ent√£o mostrar√° como combinar um modelo e um agendador para replicar o que est√° acontecendo dentro do [`DiffusionPipeline`].

<Tip>

Esse tour r√°pido √© uma vers√£o simplificada da introdu√ß√£o üß® Diffusers [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) para ajudar voc√™ a come√ßar r√°pido. Se voc√™ quer aprender mais sobre o objetivo do üß® Diffusers, filosofia de design, e detalhes adicionais sobre a API principal, veja o notebook!

</Tip>

Antes de come√ßar, certifique-se de ter todas as bibliotecas necess√°rias instaladas:

```py
# uncomment to install the necessary libraries in Colab
#!pip install --upgrade diffusers accelerate transformers
```

- [ü§ó Accelerate](https://huggingface.co/docs/accelerate/index) acelera o carregamento do modelo para gera√ß√£o e treinamento.
- [ü§ó Transformers](https://huggingface.co/docs/transformers/index) √© necess√°rio para executar os modelos mais populares de difus√£o, como o [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview).

## DiffusionPipeline

O [`DiffusionPipeline`] √© a forma mais f√°cil de usar um sistema de difus√£o pr√©-treinado para gera√ß√£o. √â um sistema de ponta a ponta contendo o modelo e o agendador. Voc√™ pode usar o [`DiffusionPipeline`] pronto para muitas tarefas. D√™ uma olhada na tabela abaixo para algumas tarefas suportadas, e para uma lista completa de tarefas suportadas, veja a tabela [Resumo do üß® Diffusers](./api/pipelines/overview#diffusers-summary).

| **Tarefa**                             | **Descri√ß√£o**                                                                                                             | **Pipeline**                                                                       |
| -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| Unconditional Image Generation         | gera uma imagem a partir do ru√≠do Gaussiano                                                                               | [unconditional_image_generation](./using-diffusers/unconditional_image_generation) |
| Text-Guided Image Generation           | gera uma imagem a partir de um prompt de texto                                                                            | [conditional_image_generation](./using-diffusers/conditional_image_generation)     |
| Text-Guided Image-to-Image Translation | adapta uma imagem guiada por um prompt de texto                                                                           | [img2img](./using-diffusers/img2img)                                               |
| Text-Guided Image-Inpainting           | preenche a parte da m√°scara da imagem, dado a imagem, a m√°scara e o prompt de texto                                       | [inpaint](./using-diffusers/inpaint)                                               |
| Text-Guided Depth-to-Image Translation | adapta as partes de uma imagem guiada por um prompt de texto enquanto preserva a estrutura por estimativa de profundidade | [depth2img](./using-diffusers/depth2img)                                           |

Comece criando uma inst√¢ncia do [`DiffusionPipeline`] e especifique qual checkpoint do pipeline voc√™ gostaria de baixar.
Voc√™ pode usar o [`DiffusionPipeline`] para qualquer [checkpoint](https://huggingface.co/models?library=diffusers&sort=downloads) armazenado no Hugging Face Hub.
Nesse quicktour, voc√™ carregar√° o checkpoint [`stable-diffusion-v1-5`](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5) para gera√ß√£o de texto para imagem.

<Tip warning={true}>

Para os modelos de [Stable Diffusion](https://huggingface.co/CompVis/stable-diffusion), por favor leia cuidadosamente a [licen√ßa](https://huggingface.co/spaces/CompVis/stable-diffusion-license) primeiro antes de rodar o modelo. üß® Diffusers implementa uma verifica√ß√£o de seguran√ßa: [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) para prevenir conte√∫do ofensivo ou nocivo, mas as capacidades de gera√ß√£o de imagem aprimorada do modelo podem ainda produzir conte√∫do potencialmente nocivo.

</Tip>

Para carregar o modelo com o m√©todo [`~DiffusionPipeline.from_pretrained`]:

```python
>>> from diffusers import DiffusionPipeline

>>> pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", use_safetensors=True)
```

O [`DiffusionPipeline`] baixa e armazena em cache todos os componentes de modelagem, tokeniza√ß√£o, e agendamento. Voc√™ ver√° que o pipeline do Stable Diffusion √© composto pelo [`UNet2DConditionModel`] e [`PNDMScheduler`] entre outras coisas:

```py
>>> pipeline
StableDiffusionPipeline {
  "_class_name": "StableDiffusionPipeline",
  "_diffusers_version": "0.13.1",
  ...,
  "scheduler": [
    "diffusers",
    "PNDMScheduler"
  ],
  ...,
  "unet": [
    "diffusers",
    "UNet2DConditionModel"
  ],
  "vae": [
    "diffusers",
    "AutoencoderKL"
  ]
}
```

N√≥s fortemente recomendamos rodar o pipeline em uma placa de v√≠deo, pois o modelo consiste em aproximadamente 1.4 bilh√µes de par√¢metros.
Voc√™ pode mover o objeto gerador para uma placa de v√≠deo, assim como voc√™ faria no PyTorch:

```python
>>> pipeline.to("cuda")
```

Agora voc√™ pode passar o prompt de texto para o `pipeline` para gerar uma imagem, e ent√£o acessar a imagem sem ru√≠do. Por padr√£o, a sa√≠da da imagem √© embrulhada em um objeto [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=image#the-image-class).

```python
>>> image = pipeline("An image of a squirrel in Picasso style").images[0]
>>> image
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/image_of_squirrel_painting.png"/>
</div>

Salve a imagem chamando o `save`:

```python
>>> image.save("image_of_squirrel_painting.png")
```

### Pipeline local

Voc√™ tamb√©m pode utilizar o pipeline localmente. A √∫nica diferen√ßa √© que voc√™ precisa baixar os pesos primeiro:

```bash
!git lfs install
!git clone https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5
```

Assim carregue os pesos salvos no pipeline:

```python
>>> pipeline = DiffusionPipeline.from_pretrained("./stable-diffusion-v1-5", use_safetensors=True)
```

Agora voc√™ pode rodar o pipeline como voc√™ faria na se√ß√£o acima.

### Troca dos agendadores

Agendadores diferentes tem diferentes velocidades de retirar o ru√≠do e compensa√ß√µes de qualidade. A melhor forma de descobrir qual funciona melhor para voc√™ √© testar eles! Uma das principais caracter√≠sticas do üß® Diffusers √© permitir que voc√™ troque facilmente entre agendadores. Por exemplo, para substituir o [`PNDMScheduler`] padr√£o com o [`EulerDiscreteScheduler`], carregue ele com o m√©todo [`~diffusers.ConfigMixin.from_config`]:

```py
>>> from diffusers import EulerDiscreteScheduler

>>> pipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", use_safetensors=True)
>>> pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)
```

Tente gerar uma imagem com o novo agendador e veja se voc√™ nota alguma diferen√ßa!

Na pr√≥xima se√ß√£o, voc√™ ir√° dar uma olhada mais de perto nos componentes - o modelo e o agendador - que comp√µe o [`DiffusionPipeline`] e aprender como usar esses componentes para gerar uma imagem de um gato.

## Modelos

A maioria dos modelos recebe uma amostra de ru√≠do, e em cada _timestep_ ele prev√™ o _noise residual_ (outros modelos aprendem a prever a amostra anterior diretamente ou a velocidade ou [`v-prediction`](https://github.com/huggingface/diffusers/blob/5e5ce13e2f89ac45a0066cb3f369462a3cf1d9ef/src/diffusers/schedulers/scheduling_ddim.py#L110)), a diferen√ßa entre uma imagem menos com ru√≠do e a imagem de entrada. Voc√™ pode misturar e combinar modelos para criar outros sistemas de difus√£o.

Modelos s√£o inicializados com o m√©todo [`~ModelMixin.from_pretrained`] que tamb√©m armazena em cache localmente os pesos do modelo para que seja mais r√°pido na pr√≥xima vez que voc√™ carregar o modelo. Para o tour r√°pido, voc√™ ir√° carregar o [`UNet2DModel`], um modelo b√°sico de gera√ß√£o de imagem incondicional com um checkpoint treinado em imagens de gato:

```py
>>> from diffusers import UNet2DModel

>>> repo_id = "google/ddpm-cat-256"
>>> model = UNet2DModel.from_pretrained(repo_id, use_safetensors=True)
```

Para acessar os par√¢metros do modelo, chame `model.config`:

```py
>>> model.config
```

A configura√ß√£o do modelo √© um dicion√°rio üßä congelado üßä, o que significa que esses par√¢metros n√£o podem ser mudados depois que o modelo √© criado. Isso √© intencional e garante que os par√¢metros usados para definir a arquitetura do modelo no in√≠cio permane√ßam os mesmos, enquanto outros par√¢metros ainda podem ser ajustados durante a gera√ß√£o.

Um dos par√¢metros mais importantes s√£o:

- `sample_size`: a dimens√£o da altura e largura da amostra de entrada.
- `in_channels`: o n√∫mero de canais de entrada da amostra de entrada.
- `down_block_types` e `up_block_types`: o tipo de blocos de downsampling e upsampling usados para criar a arquitetura UNet.
- `block_out_channels`: o n√∫mero de canais de sa√≠da dos blocos de downsampling; tamb√©m utilizado como uma order reversa do n√∫mero de canais de entrada dos blocos de upsampling.
- `layers_per_block`: o n√∫mero de blocks ResNet presentes em cada block UNet.

Para usar o modelo para gera√ß√£o, crie a forma da imagem com ru√≠do Gaussiano aleat√≥rio. Deve ter um eixo `batch` porque o modelo pode receber m√∫ltiplos ru√≠dos aleat√≥rios, um eixo `channel` correspondente ao n√∫mero de canais de entrada, e um eixo `sample_size` para a altura e largura da imagem:

```py
>>> import torch

>>> torch.manual_seed(0)

>>> noisy_sample = torch.randn(1, model.config.in_channels, model.config.sample_size, model.config.sample_size)
>>> noisy_sample.shape
torch.Size([1, 3, 256, 256])
```

Para gera√ß√£o, passe a imagem com ru√≠do para o modelo e um `timestep`. O `timestep` indica o qu√£o ruidosa a imagem de entrada √©, com mais ru√≠do no in√≠cio e menos no final. Isso ajuda o modelo a determinar sua posi√ß√£o no processo de difus√£o, se est√° mais perto do in√≠cio ou do final. Use o m√©todo `sample` para obter a sa√≠da do modelo:

```py
>>> with torch.no_grad():
...     noisy_residual = model(sample=noisy_sample, timestep=2).sample
```

Para gera√ß√£o de exemplos reais, voc√™ precisar√° de um agendador para guiar o processo de retirada do ru√≠do. Na pr√≥xima se√ß√£o, voc√™ ir√° aprender como acoplar um modelo com um agendador.

## Agendadores

Agendadores gerenciam a retirada do ru√≠do de uma amostra ruidosa para uma amostra menos ruidosa dado a sa√≠da do modelo - nesse caso, √© o `noisy_residual`.

<Tip>

üß® Diffusers √© uma caixa de ferramentas para construir sistemas de difus√£o. Enquanto o [`DiffusionPipeline`] √© uma forma conveniente de come√ßar com um sistema de difus√£o pr√©-constru√≠do, voc√™ tamb√©m pode escolher seus pr√≥prios modelos e agendadores separadamente para construir um sistema de difus√£o personalizado.

</Tip>

Para o tour r√°pido, voc√™ ir√° instanciar o [`DDPMScheduler`] com o m√©todo [`~diffusers.ConfigMixin.from_config`]:

```py
>>> from diffusers import DDPMScheduler

>>> scheduler = DDPMScheduler.from_config(repo_id)
>>> scheduler
DDPMScheduler {
  "_class_name": "DDPMScheduler",
  "_diffusers_version": "0.13.1",
  "beta_end": 0.02,
  "beta_schedule": "linear",
  "beta_start": 0.0001,
  "clip_sample": true,
  "clip_sample_range": 1.0,
  "num_train_timesteps": 1000,
  "prediction_type": "epsilon",
  "trained_betas": null,
  "variance_type": "fixed_small"
}
```

<Tip>

üí° Perceba como o agendador √© instanciado de uma configura√ß√£o. Diferentemente de um modelo, um agendador n√£o tem pesos trein√°veis e √© livre de par√¢metros!

</Tip>

Um dos par√¢metros mais importante s√£o:

- `num_train_timesteps`: o tamanho do processo de retirar ru√≠do ou em outras palavras, o n√∫mero de _timesteps_ necess√°rios para o processo de ru√≠dos Gausianos aleat√≥rios dentro de uma amostra de dados.
- `beta_schedule`: o tipo de agendados de ru√≠do para o uso de gera√ß√£o e treinamento.
- `beta_start` e `beta_end`: para come√ßar e terminar os valores de ru√≠do para o agendador de ru√≠do.

Para predizer uma imagem com um pouco menos de ru√≠do, passe o seguinte para o m√©todo do agendador [`~diffusers.DDPMScheduler.step`]: sa√≠da do modelo, `timestep`, e a atual `amostra`.

```py
>>> less_noisy_sample = scheduler.step(model_output=noisy_residual, timestep=2, sample=noisy_sample).prev_sample
>>> less_noisy_sample.shape
```

O `less_noisy_sample` pode ser passado para o pr√≥ximo `timestep` onde ele ficar√° ainda com menos ru√≠do! Vamos juntar tudo agora e visualizar o processo inteiro de retirada de ru√≠do.

Comece, criando a fun√ß√£o que fa√ßa o p√≥s-processamento e mostre a imagem sem ru√≠do como uma `PIL.Image`:

```py
>>> import PIL.Image
>>> import numpy as np


>>> def display_sample(sample, i):
...     image_processed = sample.cpu().permute(0, 2, 3, 1)
...     image_processed = (image_processed + 1.0) * 127.5
...     image_processed = image_processed.numpy().astype(np.uint8)

...     image_pil = PIL.Image.fromarray(image_processed[0])
...     display(f"Image at step {i}")
...     display(image_pil)
```

Para acelerar o processo de retirada de ru√≠do, mova a entrada e o modelo para uma GPU:

```py
>>> model.to("cuda")
>>> noisy_sample = noisy_sample.to("cuda")
```

Agora, crie um loop de retirada de ru√≠do que prediz o residual da amostra menos ruidosa, e computa a amostra menos ruidosa com o agendador:

```py
>>> import tqdm

>>> sample = noisy_sample

>>> for i, t in enumerate(tqdm.tqdm(scheduler.timesteps)):
...     # 1. predict noise residual
...     with torch.no_grad():
...         residual = model(sample, t).sample

...     # 2. compute less noisy image and set x_t -> x_t-1
...     sample = scheduler.step(residual, t, sample).prev_sample

...     # 3. optionally look at image
...     if (i + 1) % 50 == 0:
...         display_sample(sample, i + 1)
```

Sente-se e assista o gato ser gerado do nada al√©m de ru√≠do! üòª

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/diffusion-quicktour.png"/>
</div>

## Pr√≥ximos passos

Esperamos que voc√™ tenha gerado algumas imagens legais com o üß® Diffusers neste tour r√°pido! Para suas pr√≥ximas etapas, voc√™ pode

- Treine ou fa√ßa a configura√ß√£o fina de um modelo para gerar suas pr√≥prias imagens no tutorial de [treinamento](./tutorials/basic_training).
- Veja exemplos oficiais e da comunidade de [scripts de treinamento ou configura√ß√£o fina](https://github.com/huggingface/diffusers/tree/main/examples#-diffusers-examples) para os mais variados casos de uso.
- Aprenda sobre como carregar, acessar, mudar e comparar agendadores no guia [Usando diferentes agendadores](./using-diffusers/schedulers).
- Explore engenharia de prompt, otimiza√ß√µes de velocidade e mem√≥ria, e dicas e truques para gerar imagens de maior qualidade com o guia [Stable Diffusion](./stable_diffusion).
- Se aprofunde em acelerar üß® Diffusers com guias sobre [PyTorch otimizado em uma GPU](./optimization/fp16), e guias de infer√™ncia para rodar [Stable Diffusion em Apple Silicon (M1/M2)](./optimization/mps) e [ONNX Runtime](./optimization/onnx).
