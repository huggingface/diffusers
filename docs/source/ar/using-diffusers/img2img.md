# Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©
[[open-in-colab]]

Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù€ [Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©](conditional_image_generation)ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù…ÙˆØ¬Ù‡ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªÙ…Ø±ÙŠØ± ØµÙˆØ±Ø© Ø£ÙˆÙ„ÙŠØ© ÙƒÙ†Ù‚Ø·Ø© Ø¨Ø¯Ø§ÙŠØ© Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ù†ØªØ´Ø§Ø±. ÙŠØªÙ… ØªØ±Ù…ÙŠØ² Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø­Ø© Ø®ÙÙŠØ© ÙˆÙŠØªÙ… Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø¥Ù„ÙŠÙ‡Ø§. Ø«Ù… ØªØ£Ø®Ø° Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®ÙÙŠ Ù„Ù„Ø§Ù†ØªØ´Ø§Ø± Ù…ÙˆØ¬Ù‡ ÙˆØ§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø®ÙÙŠØ© Ø§Ù„ØµØ§Ø®Ø¨Ø©ØŒ ÙˆÙŠØªÙˆÙ‚Ø¹ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø§Ù„Ù…Ø¶Ø§ÙØ©ØŒ ÙˆÙŠØ²ÙŠÙ„ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø§Ù„Ø®ÙÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø®ÙÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©. Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ ÙŠÙ‚ÙˆÙ… ÙÙƒ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¨ØªØ±Ø¬Ù…Ø© Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø®ÙÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©.

Ù…Ø¹ ğŸ¤— DiffusersØŒ Ù‡Ø°Ø§ Ø³Ù‡Ù„ Ù…Ø«Ù„ 1-2-3:

1. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ ÙÙŠ ÙØ¦Ø© [`AutoPipelineForImage2Image`]Ø› ÙŠÙ‚ÙˆÙ… Ù‡Ø°Ø§ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨ØªØ¹ÙŠÙŠÙ† ØªØ­Ù…ÙŠÙ„ ÙØ¦Ø© Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´:

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()
```

<Tip>
Ø³ØªÙ„Ø§Ø­Ø¸ Ø·ÙˆØ§Ù„ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ù†Ø³ØªØ®Ø¯Ù… [`~DiffusionPipeline.enable_model_cpu_offload`] Ùˆ [`~DiffusionPipeline.enable_xformers_memory_efficient_attention`]ØŒ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ²ÙŠØ§Ø¯Ø© Ø³Ø±Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… PyTorch 2.0ØŒ ÙÙ„Ø³Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~DiffusionPipeline.enable_xformers_memory_efficient_attention`] Ø¹Ù„Ù‰ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨Ùƒ Ù„Ø£Ù†Ù‡ Ø³ÙŠÙƒÙˆÙ† Ø¨Ø§Ù„ÙØ¹Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù‡ØªÙ…Ø§Ù… PyTorch 2.0 Ø§Ù„Ø£ØµÙ„ÙŠ [scaled-dot product](../optimization/torch2.0#scaled-dot-product-attention).
</Tip>

2. Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ù„ØªÙ…Ø±ÙŠØ±Ù‡Ø§ Ø¥Ù„Ù‰ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨:

```py
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png")
```

3. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ ÙˆØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø©:

```py
prompt = "cat wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k"
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500>Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500>Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©</figcaption>
</div>
</div>

## Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø´Ø¹Ø¨ÙŠØ©

Ø£ÙƒØ«Ø± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ± Ø´ÙŠÙˆØ¹Ù‹Ø§ Ù‡ÙŠ [Stable Diffusion v1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)ØŒ [Stable Diffusion XL (SDXL)](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)ØŒ Ùˆ [Kandinsky 2.2](https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder). ØªØ®ØªÙ„Ù Ù†ØªØ§Ø¦Ø¬ Ù†Ù…Ø§Ø°Ø¬ Stable Diffusion Ùˆ Kandinsky Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª ÙÙŠ Ø¨Ù†ÙŠØªÙ‡Ø§ ÙˆØ¹Ù…Ù„ÙŠØ© ØªØ¯Ø±ÙŠØ¨Ù‡Ø§Ø› ÙŠÙ…ÙƒÙ†Ùƒ Ø¹Ù…ÙˆÙ…Ù‹Ø§ ØªÙˆÙ‚Ø¹ Ø£Ù† ØªÙ†ØªØ¬ SDXL ØµÙˆØ±Ù‹Ø§ Ø°Ø§Øª Ø¬ÙˆØ¯Ø© Ø£Ø¹Ù„Ù‰ Ù…Ù† Stable Diffusion v1.5. Ø¯Ø¹Ù†Ø§ Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© Ø³Ø±ÙŠØ¹Ø© Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ù†ØªØ§Ø¦Ø¬Ù‡Ø§.

### Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø± v1.5

Stable Diffusion v1.5 Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù†ØªØ´Ø§Ø± Ø®ÙÙŠ ÙŠØªÙ… ØªÙ‡ÙŠØ¦ØªÙ‡ Ù…Ù† Ù†Ù‚Ø·Ø© ØªÙØªÙŠØ´ Ù…Ø¨ÙƒØ±Ø©ØŒ ÙˆÙŠØªÙ… Ø¶Ø¨Ø· Ø¯Ù‚ØªÙ‡ Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ¨Ø± Ù„Ù€ 595 Ø£Ù„Ù Ø®Ø·ÙˆØ© Ø¹Ù„Ù‰ ØµÙˆØ± 512x512. Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ Ù„Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©ØŒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙˆØ±Ø© Ø£ÙˆÙ„ÙŠØ© Ù„ØªÙ…Ø±ÙŠØ±Ù‡Ø§ Ø¥Ù„Ù‰ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨. Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ ÙˆØ§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©:

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500>Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdv1.5.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500>Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©</figcaption>
</div>
</div>

### Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø± XL (SDXL)

SDXL Ù‡Ùˆ Ø¥ØµØ¯Ø§Ø± Ø£ÙƒØ«Ø± Ù‚ÙˆØ© Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ Stable Diffusion. ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ø£Ø³Ø§Ø³ÙŠÙ‹Ø§ Ø£ÙƒØ¨Ø±ØŒ ÙˆÙ†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù…Ø­Ø³Ù†Ù‹Ø§ Ø¥Ø¶Ø§ÙÙŠÙ‹Ø§ Ù„Ø²ÙŠØ§Ø¯Ø© Ø¬ÙˆØ¯Ø© Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ. Ø§Ù‚Ø±Ø£ Ø¯Ù„ÙŠÙ„ [SDXL](sdxl) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¯Ù„ÙŠÙ„ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„ØªÙŠ ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ù„Ø¥Ù†ØªØ§Ø¬ ØµÙˆØ± Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©.

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, strength=0.5).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl-init.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500>Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500>Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©</figcaption>
</div>
</div>

### ÙƒØ§Ù†Ø¯ÙŠÙ†Ø³ÙƒÙŠ 2.2

ÙŠØ®ØªÙ„Ù Ù†Ù…ÙˆØ°Ø¬ ÙƒØ§Ù†Ø¯ÙŠÙ†Ø³ÙƒÙŠ Ø¹Ù† Ù†Ù…Ø§Ø°Ø¬ Stable Diffusion Ù„Ø£Ù†Ù‡ ÙŠØ³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„Ù„ØµÙˆØ± Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„ØµÙˆØ±. ØªØ³Ø§Ø¹Ø¯ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø§Ø°Ø§Ø© Ø£ÙØ¶Ù„ Ø¨ÙŠÙ† Ø§Ù„Ù†Øµ ÙˆØ§Ù„ØµÙˆØ±ØŒ Ù…Ù…Ø§ ÙŠØ³Ù…Ø­ Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø®ÙÙŠ Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ± Ø£ÙØ¶Ù„.

Ø£Ø¨Ø³Ø· Ø·Ø±ÙŠÙ‚Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Kandinsky 2.2 Ù‡ÙŠ:

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500>Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-kandinsky.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500>Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©</figcaption>
</div>
</div>

## ØªÙƒÙˆÙŠÙ† Ù…Ø¹Ù„Ù…Ø§Øª Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨

Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙƒÙˆÙŠÙ†Ù‡Ø§ ÙÙŠ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ ÙˆØ§Ù„ØªÙŠ Ø³ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø¹Ù…Ù„ÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± ÙˆØ¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø©. Ø¯Ø¹Ù†Ø§ Ù†Ù„Ù‚ÙŠ Ù†Ø¸Ø±Ø© ÙØ§Ø­ØµØ© Ø¹Ù„Ù‰ Ù…Ø§ ØªÙØ¹Ù„Ù‡ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª ÙˆÙƒÙŠÙ ÙŠØ¤Ø«Ø± ØªØºÙŠÙŠØ±Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬.
### Ø§Ù„Ù‚ÙˆØ©

`strength` Ù‡ÙŠ Ø£Ø­Ø¯ Ø£Ù‡Ù… Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ù…Ø±Ø§Ø¹Ø§ØªÙ‡Ø§ØŒ ÙˆØ³ÙŠÙƒÙˆÙ† Ù„Ù‡Ø§ ØªØ£Ø«ÙŠØ± ÙƒØ¨ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§. ÙÙ‡ÙŠ ØªØ­Ø¯Ø¯ Ù…Ø¯Ù‰ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù†Ø§ØªØ¬Ø© Ù…Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©. ÙˆØ¨Ø¹Ø¨Ø§Ø±Ø© Ø£Ø®Ø±Ù‰:

- ğŸ“ˆ ØªØ¹Ø·ÙŠ Ù‚ÙŠÙ…Ø© Ø£Ø¹Ù„Ù‰ Ù„Ù€ `strength` Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø²ÙŠØ¯Ù‹Ø§ Ù…Ù† "Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹" Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©Ø› ÙˆØªØ¹Ù†ÙŠ Ù‚ÙŠÙ…Ø© `strength` ØªØ³Ø§ÙˆÙŠ 1.0 ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø¥Ù„Ù‰ Ø­Ø¯ ÙƒØ¨ÙŠØ±

- ğŸ“‰ ØªØ¹Ù†ÙŠ Ù‚ÙŠÙ…Ø© Ø£Ù‚Ù„ Ù„Ù€ `strength` Ø£Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù†Ø§ØªØ¬Ø© Ø£ÙƒØ«Ø± ØªØ´Ø§Ø¨Ù‡Ø§ Ù…Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©

ÙŠØ±ØªØ¨Ø· Ù…Ø¹Ù„Ù… `strength` Ùˆ`num_inference_steps` Ù„Ø£Ù† `strength` ØªØ­Ø¯Ø¯ Ø¹Ø¯Ø¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¶Ø¬ÙŠØ¬ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¥Ø¶Ø§ÙØªÙ‡Ø§. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒØ§Ù† `num_inference_steps` Ù‡Ùˆ 50 Ùˆ`strength` Ù‡Ùˆ 0.8ØŒ ÙÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø¥Ø¶Ø§ÙØ© 40 (50 * 0.8) Ø®Ø·ÙˆØ© Ø¶Ø¬ÙŠØ¬ Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø«Ù… Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶Ø¬ÙŠØ¬ Ù„Ù€ 40 Ø®Ø·ÙˆØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ø­Ø¯ÙŠØ«Ù‹Ø§.

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
"runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, strength=0.8).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

<div class="flex flex-row gap-4">
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-strength-0.4.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">strength = 0.4</figcaption>
</div>
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-strength-0.6.png"/>
<figcaption class="mt-â‚‚ text-center text-sm text-gray-500">strength = 0.6</figcaption>
</div>
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-strength-1.0.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">strength = 1.0</figcaption>
</div>
</div>

### Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„ØªÙˆØ¬ÙŠÙ‡

ÙŠÙØ³ØªØ®Ø¯Ù… Ù…Ø¹Ù„Ù… `guidance_scale` Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù…Ø¯Ù‰ ØªÙˆØ§ÙÙ‚ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© ÙˆØ·Ù„Ø¨ Ø§Ù„Ù†Øµ. ØªØ¹Ù†ÙŠ Ù‚ÙŠÙ…Ø© Ø£Ø¹Ù„Ù‰ Ù„Ù€ `guidance_scale` Ø£Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ø£ÙƒØ«Ø± ØªÙˆØ§ÙÙ‚Ø§ Ù…Ø¹ Ø§Ù„Ø·Ù„Ø¨ØŒ ÙÙŠ Ø­ÙŠÙ† Ø£Ù† Ù‚ÙŠÙ…Ø© Ø£Ù‚Ù„ Ù„Ù€ `guidance_scale` ØªØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù„Ø¯ÙŠÙ‡Ø§ Ù…Ø³Ø§Ø­Ø© Ø£ÙƒØ¨Ø± Ù„Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø¹Ù† Ø§Ù„Ø·Ù„Ø¨.

ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† `guidance_scale` Ùˆ`strength` Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙÙŠ Ù…Ø¯Ù‰ ØªØ¹Ø¨ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù‚Ù… Ø¨Ø¯Ù…Ø¬ `strength + guidance_scale` Ø¹Ø§Ù„ÙŠÙ‹Ø§ Ù„Ù„Ø¥Ø¨Ø¯Ø§Ø¹ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø²ÙŠØ¬Ù‹Ø§ Ù…Ù† `strength` Ø§Ù„Ù…Ù†Ø®ÙØ¶ Ùˆ`guidance_scale` Ø§Ù„Ù…Ù†Ø®ÙØ¶ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© ØªØ´Ø¨Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© ÙˆÙ„ÙƒÙ†Ù‡Ø§ Ù„ÙŠØ³Øª Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø´ÙƒÙ„ ØµØ§Ø±Ù… Ø¨Ø§Ù„Ø·Ù„Ø¨.

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
"runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, guidance_scale=8.0).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

<div class="flex flex-row gap-4">
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-guidance-0.1.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">guidance_scale = 0.1</figcaption>
</div>
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-guidance-3.0.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">guidance_scale = 5.0</figcaption>
</div>
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-guidance-7.5.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">guidance_scale = 10.0</figcaption>
</div>
</div>

### Ø·Ù„Ø¨ Ø³Ù„Ø¨ÙŠ

ÙŠØ¤Ø¯ÙŠ Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø³Ù„Ø¨ÙŠ Ø¥Ù„Ù‰ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¹Ø¯Ù… ØªØ¶Ù…ÙŠÙ† Ø£Ø´ÙŠØ§Ø¡ ÙÙŠ ØµÙˆØ±Ø©ØŒ ÙˆÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø© Ø£Ùˆ ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¶Ù…ÙŠÙ† Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ø³Ù„Ø¨ÙŠØ© Ù…Ø«Ù„ "ØªÙØ§ØµÙŠÙ„ Ø³ÙŠØ¦Ø©" Ø£Ùˆ "Ø¶Ø¨Ø§Ø¨ÙŠ" Ù„ØªØ´Ø¬ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø°Ø§Øª Ø¬ÙˆØ¯Ø© Ø£Ø¹Ù„Ù‰. Ø£Ùˆ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ ØµÙˆØ±Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯Ù‡Ø§ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©.

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
"stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=negative_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

<div class="flex flex-row gap-4">
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-negative-1.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"</figcaption>
</div>
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-negative-2.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">negative_prompt = "jungle"</figcaption>
</div>
</div>

## Ø®Ø·ÙˆØ· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„Ø©

Ù‡Ù†Ø§Ùƒ Ø¨Ø¹Ø¶ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø«ÙŠØ±Ø© Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„Ù‡Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø®Ù„Ø§Ù Ù…Ø¬Ø±Ø¯ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© (Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ù‡Ø°Ø§ Ø±Ø§Ø¦Ø¹ Ø£ÙŠØ¶Ù‹Ø§). ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø¶ÙŠ Ù‚Ø¯Ù…Ù‹Ø§ ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø®Ø·ÙˆØ· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø£Ø®Ø±Ù‰.

### Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©

ÙŠØ³Ù…Ø­ Ø±Ø¨Ø· Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„Ù†Øµ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© ÙƒØµÙˆØ±Ø© Ø£ÙˆÙ„ÙŠØ© Ù„Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©. Ù‡Ø°Ø§ Ù…ÙÙŠØ¯ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„ØµÙØ±. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø¨ØªØ³Ù„Ø³Ù„ Ù†Ù…ÙˆØ°Ø¬ Stable Diffusion ÙˆÙ†Ù…ÙˆØ°Ø¬ Kandinsky.

Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©:

```py
from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image
import torch
from diffusers.utils import make_image_grid

pipeline = AutoPipelineForText2Image.from_pretrained(
"runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

text2image = pipeline("Astronaut in a jungle, cold color palette, muted colors, detailed, 8k").images[0]
text2image
```

Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ± Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ø¥Ù„Ù‰ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©:

```py
pipeline = AutoPipelineForImage2Image.from_pretrained(
"kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image2image = pipeline("Astronaut in a jungle, cold color palette, muted colors, detailed, 8k", image=text2image).images[0]
make_image_grid([text2image, image2image], rows=1, cols=2)
```

### Ø±Ø¨Ø· Ø¹Ø¯Ø© Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù„Ù„ØµÙˆØ± Ù…Ø¹ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶
ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø±Ø¨Ø· Ø¹Ø¯Ø© Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù„Ù„ØµÙˆØ± Ù…Ø¹ Ø¨Ø¹Ø¶Ù‡Ø§ Ø§Ù„Ø¨Ø¹Ø¶ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø£ÙƒØ«Ø± Ø¥Ø«Ø§Ø±Ø© Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…. ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ù…ÙÙŠØ¯Ù‹Ø§ Ù„Ø£Ø¯Ø§Ø¡ Ù†Ù‚Ù„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø¨Ø´ÙƒÙ„ ØªÙƒØ±Ø§Ø±ÙŠ Ø¹Ù„Ù‰ ØµÙˆØ±Ø©ØŒ Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± GIF Ù‚ØµÙŠØ±Ø©ØŒ Ø£Ùˆ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ù„ØµÙˆØ±Ø©ØŒ Ø£Ùˆ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ù…Ù† ØµÙˆØ±Ø©.

Ø§Ø¨Ø¯Ø£ Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø©:

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
"runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, output_type="latent").images[0]
```

<Tip>
Ù…Ù† Ø§Ù„Ù…Ù‡Ù… ØªØ­Ø¯ÙŠØ¯ `output_type="latent"` ÙÙŠ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© Ù„ØªØ¬Ù†Ø¨ Ø®Ø·ÙˆØ© Ø§Ù„ØªØ±Ù…ÙŠØ² ÙˆØ§Ù„ÙÙƒ ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©. ÙŠØ¹Ù…Ù„ Ù‡Ø°Ø§ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© ØªØ³ØªØ®Ø¯Ù… Ù†ÙØ³ VAE.
</Tip>

Ù…Ø±Ø± Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø®ÙÙŠ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ Ø§Ù„ØªØ§Ù„ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ø¹Ù„Ù‰ [Ù†Ù…Ø· ÙƒØªØ§Ø¨ Ù‡Ø²Ù„ÙŠ](https://huggingface.co/ogkalu/Comic-Diffusion):

```py
pipeline = AutoPipelineForImage2Image.from_pretrained(
"ogkalu/Comic-Diffusion"ØŒ torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# ÙŠØ¬Ø¨ ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…Ø² "charliebo artstyle" ÙÙŠ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ Ù‡Ø°Ù‡
image = pipeline("Astronaut in a jungle, charliebo artstyle"ØŒ image=image, output_type="latent").images[0]
```

ÙƒØ±Ø± Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¹Ù„Ù‰ [Ù†Ù…Ø· ÙÙ† Ø§Ù„Ø¨ÙƒØ³Ù„](https://huggingface.co/kohbanye/pixel-art-style):

```py
pipeline = AutoPipelineForImage2Image.from_pretrained(
"kohbanye/pixel-art-style"ØŒ torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# ÙŠØ¬Ø¨ ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…Ø² "pixelartstyle" ÙÙŠ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ Ù‡Ø°Ù‡
image = pipeline("Astronaut in a jungle, pixelartstyle"ØŒ image=image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

### Ø£Ù†Ø¨ÙˆØ¨ Ø§Ù„ØµÙˆØ± Ø¥Ù„Ù‰ Ø£Ù†Ø¨ÙˆØ¨ ØªÙƒØ¨ÙŠØ± Ø¥Ù„Ù‰ ØµÙˆØ±Ø© ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ø¯Ù‚Ø©
Ù‡Ù†Ø§Ùƒ Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰ Ù„Ø±Ø¨Ø· Ø£Ù†Ø¨ÙˆØ¨ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ÙˆÙ‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ù†Ø¨ÙˆØ¨ ØªÙƒØ¨ÙŠØ± ÙˆØµÙˆØ±Ø© ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ø¯Ù‚Ø© Ù„Ø²ÙŠØ§Ø¯Ø© Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø­Ù‚Ù‹Ø§.

Ø§Ø¨Ø¯Ø£ Ø¨Ø£Ù†Ø¨ÙˆØ¨ ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø©:

```py
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
"runwayml/stable-diffusion-v1-5"ØŒ torch_dtype=torch.float16ØŒ variant="fp16"ØŒ use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image_1 = pipeline(prompt, image=init_image, output_type="latent").images[0]
```

<Tip>
Ù…Ù† Ø§Ù„Ù…Ù‡Ù… ØªØ­Ø¯ÙŠØ¯ `output_type="latent"` ÙÙŠ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø®ÙÙŠØ© Ù„ØªØ¬Ù†Ø¨ Ø®Ø·ÙˆØ© Ø§Ù„ØªØ±Ù…ÙŠØ² ÙˆØ§Ù„ÙÙƒ ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©. ÙŠØ¹Ù…Ù„ Ù‡Ø°Ø§ ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© ØªØ³ØªØ®Ø¯Ù… Ù†ÙØ³ VAE.
</Tip>

Ù‚Ù… Ø¨ØªÙˆØµÙŠÙ„Ù‡ Ø¨Ø£Ù†Ø¨ÙˆØ¨ ØªÙƒØ¨ÙŠØ± Ù„Ø²ÙŠØ§Ø¯Ø© Ø¯Ù‚Ø© Ø§Ù„ØµÙˆØ±Ø©:

```py
from diffusers import StableDiffusionLatentUpscalePipeline

upscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(
"stabilityai/sd-x2-latent-upscaler"ØŒ torch_dtype=torch.float16ØŒ variant="fp16"ØŒ use_safetensors=True
)
upscaler.enable_model_cpu_offload()
upscaler.enable_xformers_memory_efficient_attention()

image_2 = upscaler(promptØŒ image=image_1ØŒ output_type="latent").images[0]
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ù‚Ù… Ø¨ØªÙˆØµÙŠÙ„Ù‡ Ø¨Ø£Ù†Ø¨ÙˆØ¨ ÙØ§Ø¦Ù‚ Ø§Ù„Ø¯Ù‚Ø© Ù„Ø²ÙŠØ§Ø¯Ø© ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©:

```py
from diffusers import StableDiffusionUpscalePipeline

super_res = StableDiffusionUpscalePipeline.from_pretrained(
"stabilityai/stable-diffusion-x4-upscaler"ØŒ torch_dtype=torch.float16ØŒ variant="fp16"ØŒ use_safetensors=True
)
super_res.enable_model_cpu_offload()
super_res.enable_xformers_memory_efficient_attention()

image_3 = super_res(promptØŒ image=image_2).images[0]
make_image_grid([init_image, image_3.resize((512, 512))], rows=1, cols=2)
```

## Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±
ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© ØªØ¨Ø¯Ùˆ Ø¨Ø§Ù„Ø¶Ø¨Ø· ÙƒÙ…Ø§ ØªØ±ÙŠØ¯ Ø£Ù…Ø±Ù‹Ø§ ØµØ¹Ø¨Ù‹Ø§ØŒ ÙˆÙ‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³Ø¨Ø¨ ÙÙŠ Ø£Ù† ØªÙ‚Ù†ÙŠØ§Øª ÙˆÙ†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ù…ÙÙŠØ¯Ø© Ø¬Ø¯Ù‹Ø§. ÙÙŠ Ø­ÙŠÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… `negative_prompt` Ù„Ù„ØªØ­ÙƒÙ… Ø¬Ø²Ø¦ÙŠÙ‹Ø§ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±ØŒ Ù‡Ù†Ø§Ùƒ Ø·Ø±Ù‚ Ø£ÙƒØ«Ø± Ù‚ÙˆØ© Ù…Ø«Ù„ ÙˆØ²Ù† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© ÙˆØ´Ø¨ÙƒØ§Øª Ø§Ù„ØªØ­ÙƒÙ….

### ÙˆØ²Ù† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©
ÙŠØ³Ù…Ø­ ÙˆØ²Ù† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ø¨ØªØºÙŠÙŠØ± Ø­Ø¬Ù… ØªÙ…Ø«ÙŠÙ„ ÙƒÙ„ Ù…ÙÙ‡ÙˆÙ… ÙÙŠ Ù…Ø·Ø§Ù„Ø¨Ø©. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙÙŠ Ù…Ø·Ø§Ù„Ø¨Ø© Ù…Ø«Ù„ "Ø±Ø§Ø¦Ø¯ ÙØ¶Ø§Ø¡ ÙÙŠ Ø§Ù„ØºØ§Ø¨Ø©ØŒ Ù„ÙˆØ­Ø© Ø£Ù„ÙˆØ§Ù† Ø¨Ø§Ø±Ø¯Ø©ØŒ Ø£Ù„ÙˆØ§Ù† Ø®Ø§ÙØªØ©ØŒ Ù…ÙØµÙ„Ø©ØŒ 8k"ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø®ØªÙŠØ§Ø± Ø²ÙŠØ§Ø¯Ø© Ø£Ùˆ ØªÙ‚Ù„ÙŠÙ„ ØªØ¶Ù…ÙŠÙ† "Ø±Ø§Ø¦Ø¯ Ø§Ù„ÙØ¶Ø§Ø¡" Ùˆ"Ø§Ù„ØºØ§Ø¨Ø©". ØªÙˆÙØ± Ù…ÙƒØªØ¨Ø© [Compel](https://github.com/damian0815/compel) Ø¨Ù†Ø§Ø¡ Ø¬Ù…Ù„Ø© Ø¨Ø³ÙŠØ·Ù‹Ø§ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© ÙˆØ¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª. ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª ÙÙŠ Ø¯Ù„ÙŠÙ„ [ÙˆØ²Ù† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©](weighted_prompts).

Ù„Ø¯Ù‰ [`AutoPipelineForImage2Image`] Ù…Ø¹Ù„Ù…Ø© `prompt_embeds` (Ùˆ`negative_prompt_embeds` Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… Ù…Ø·Ø§Ù„Ø¨Ø© Ø³Ù„Ø¨ÙŠØ©) Ø­ÙŠØ« ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ± Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„ØªÙŠ ØªØ­Ù„ Ù…Ø­Ù„ Ù…Ø¹Ù„Ù…Ø© `prompt`.

```py
from diffusers import AutoPipelineForImage2Image
import torch

pipeline = AutoPipelineForImage2Image.from_pretrained(
"runwayml/stable-diffusion-v1-5"ØŒ torch_dtype=torch.float16ØŒ variant="fp16"ØŒ use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image = pipeline(prompt_embeds=prompt_embedsØŒ # ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ù…Ù† Compel
negative_prompt_embeds=negative_prompt_embedsØŒ # ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ù…Ù† Compel
image=init_imageØŒ
).images[0]
```

### ControlNet

ØªÙˆÙØ± ControlNets Ø·Ø±ÙŠÙ‚Ø© Ø£ÙƒØ«Ø± Ù…Ø±ÙˆÙ†Ø© ÙˆØ¯Ù‚Ø© Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± Ù„Ø£Ù†Ùƒ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ³ØªØ®Ø¯Ù… ØµÙˆØ±Ø© Ø´Ø±Ø·ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ©. ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø´Ø±Ø·ÙŠØ© ØµÙˆØ±Ø© Canny Ø£Ùˆ Ø®Ø±ÙŠØ·Ø© Ø¹Ù…Ù‚ Ø£Ùˆ ØªØ¬Ø²Ø¦Ø© ØµÙˆØ±Ø©ØŒ ÙˆØ­ØªÙ‰ Ø§Ù„Ø®Ø±Ø¨Ø´Ø§Øª! Ø£ÙŠÙ‹Ø§ ÙƒØ§Ù† Ù†ÙˆØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø´Ø±Ø·ÙŠØ© Ø§Ù„ØªÙŠ ØªØ®ØªØ§Ø±Ù‡Ø§ØŒ ÙŠÙ‚ÙˆÙ… ControlNet Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠÙ‡Ø§.

Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø´ØªØ±Ø· ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø±ÙŠØ·Ø© Ø¹Ù…Ù‚ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ© ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©.

```py
from diffusers.utils import load_image, make_image_grid

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)
init_image = init_image.resize((958, 960)) # ØªØºÙŠÙŠØ± Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø£Ø¨Ø¹Ø§Ø¯ ØµÙˆØ±Ø© Ø§Ù„Ø¹Ù…Ù‚
depth_image = load_image("https://huggingface.co/lllyasviel/control_v11f1p_sd15_depth/resolve/main/images/control.png")
make_image_grid([init_image, depth_image], rows=1, cols=2)
```

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ControlNet Ø§Ù„Ù…Ø´Ø±ÙˆØ· Ø¨Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø¹Ù…Ù‚ Ùˆ [`AutoPipelineForImage2Image`]:

```py
from diffusers import ControlNetModel, AutoPipelineForImage2Image
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11f1p_sd15_depth", torch_dtype=torch.float16, variant="fp16", use_safetensors=True)
pipeline = AutoPipelineForImage2Image.from_pretrained(
"runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# Ø§Ø­Ø°Ù Ø§Ù„Ø³Ø·Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† xFormers Ù…Ø«Ø¨ØªÙ‹Ø§ Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ PyTorch 2.0 Ø£Ùˆ Ø£Ø¹Ù„Ù‰ Ù…Ø«Ø¨ØªÙ‹Ø§
pipeline.enable_xformers_memory_efficient_attention()
```

Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø´Ø±ÙˆØ·Ø© Ø¨Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¹Ù…Ù‚ ÙˆØ§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© ÙˆØ§Ù„Ù†Øµ Ø§Ù„ÙØ¹Ù„ÙŠ:

```py
prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
image_control_net = pipeline(prompt, image=init_image, control_image=depth_image).images[0]
make_image_grid([init_image, depth_image, image_control_net], rows=1, cols=3)
```

<div class="flex flex-row gap-4">
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©</figcaption>
</div>
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/lllyasviel/control_v11f1p_sd15_depth/resolve/main/images/control.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ØµÙˆØ±Ø© Ø§Ù„Ø¹Ù…Ù‚</figcaption>
</div>
<div class="flex-1">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-controlnet.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ØµÙˆØ±Ø© ControlNet</figcaption>
</div>
</div>

Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø·Ø¨Ù‚ [Ù†Ù…Ø·Ù‹Ø§](https://huggingface.co/nitrosocke/elden-ring-diffusion) Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ Ù…Ù† ControlNet Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø±Ø¨Ø·Ù‡Ø§ Ø¨Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©:

```py
pipeline = AutoPipelineForImage2Image.from_pretrained(
"nitrosocke/elden-ring-diffusion", torch_dtype=torch.float16,
)
pipeline.enable_model_cpu_offload()
# Ø§Ø­Ø°Ù Ø§Ù„Ø³Ø·Ø± Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† xFormers Ù…Ø«Ø¨ØªÙ‹Ø§ Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ PyTorch 2.0 Ø£Ùˆ Ø£Ø¹Ù„Ù‰ Ù…Ø«Ø¨ØªÙ‹Ø§
pipeline.enable_xformers_memory_efficient_attention()

prompt = "elden ring style astronaut in a jungle" # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù…ÙŠØ² "elden ring style" ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„ÙØ¹Ù„ÙŠ
negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"

image_elden_ring = pipeline(prompt, negative_prompt=negative_prompt, image=image_control_net, strength=0.45, guidance_scale=10.5).images[0]
make_image_grid([init_image, depth_image, image_control_net, image_elden_ring], rows=2, cols=2)
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-elden-ring.png">
</div>

## ØªØ­Ø³ÙŠÙ†

Ø¥Ù† ØªØ´ØºÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø¨Ø§Ù‡Ø¸ Ø§Ù„ØªÙƒÙ„ÙØ© ÙˆÙƒØ«ÙŠÙ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ù„Ø­ÙˆØ³Ø¨Ø©ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø¹Ø¶ Ø§Ù„Ø­ÙŠÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†ÙŠØ©ØŒ Ù…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† ØªÙ…Ø§Ù…Ù‹Ø§ ØªØ´ØºÙŠÙ„Ù‡Ø§ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª GPU Ù„Ù„Ù…Ø³ØªÙ‡Ù„ÙƒÙŠÙ† ÙˆØ§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø¬Ø§Ù†ÙŠØ©. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…Ø«Ù„ [Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù†Ù‚Ø·ÙŠ Ø§Ù„Ù…ÙÙ…ÙÙŠØ²](../optimization/torch2.0#scaled-dot-product-attention) Ù…Ù† PyTorch 2.0 Ø£Ùˆ [xFormers](../optimization/xformers) (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø­Ø¯Ù‡Ù…Ø§ØŒ ÙˆÙ„ÙƒÙ† Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø§Ø¬Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„ÙŠÙ‡Ù…Ø§). ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª GPU Ø£Ø«Ù†Ø§Ø¡ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÙƒÙˆÙ†Ø§Øª Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ø§Ù„Ø£Ø®Ø±Ù‰ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© CPU.

```diff
+ pipeline.enable_model_cpu_offload()
+ pipeline.enable_xformers_memory_efficient_attention()
```

Ù…Ø¹ [`torch.compile`](../optimization/torch2.0#torchcompile)ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø²ÙŠØ§Ø¯Ø© Ø³Ø±Ø¹Ø© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù„Ø¯ÙŠÙƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ù„Ù UNet Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¨Ù‡:

```py
pipeline.unet = torch.compile(pipeline.unet, mode="reduce-overhead", fullgraph=True)
```

Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ØŒ Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ø£Ø¯Ù„Ø© [ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©](../optimization/memory) Ùˆ [Torch 2.0](../optimization/torch2.0).