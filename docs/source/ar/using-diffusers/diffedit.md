# DiffEdit 

[[open-in-colab]]

Ù„Ø§ ØªØªØ·Ù„Ø¨ Ø¹Ù…Ù„ÙŠØ§Øª ØªØ­Ø±ÙŠØ± Ø§Ù„ØµÙˆØ± Ø¹Ø§Ø¯Ø© Ø³ÙˆÙ‰ ØªÙˆÙÙŠØ± Ù‚Ù†Ø§Ø¹ Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„ØªØ­Ø±ÙŠØ±. ÙŠÙ‚ÙˆÙ… DiffEdit ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ù„Ùƒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù†ØµÙŠØŒ Ù…Ù…Ø§ ÙŠØ³Ù‡Ù„ Ø¹Ù…Ù„ÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø±Ø§Ù…Ø¬ ØªØ­Ø±ÙŠØ± Ø§Ù„ØµÙˆØ±. ØªØ¹Ù…Ù„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© DiffEdit ÙÙŠ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª:

1. ÙŠÙ‚ÙˆÙ… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø¨ØªÙ†Ù‚ÙŠØ© ØµÙˆØ±Ø© Ù…Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Øµ Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆÙ†Øµ Ù…Ø±Ø¬Ø¹ÙŠØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ø¶ÙˆØ¶Ø§Ø¡ Ù…Ø®ØªÙ„ÙØ© Ù„Ù…Ù†Ø§Ø·Ù‚ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©Ø› ÙˆÙŠÙØ³ØªØ®Ø¯Ù… Ø§Ù„ÙØ±Ù‚ Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ Ù‚Ù†Ø§Ø¹ Ù„ØªØ­Ø¯ÙŠØ¯ Ø£ÙŠ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØºÙŠÙŠØ± Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù†Øµ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù….

2. ÙŠØªÙ… ØªØ±Ù…ÙŠØ² Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø© Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø­Ø© Ø§Ù„ÙƒØ§Ù…Ù†Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DDIM.

3. ÙŠØªÙ… ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ· Ø¹Ù„Ù‰ Ù†Øµ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ù†Ø§Ø¹ ÙƒØ¯Ù„ÙŠÙ„ Ø¨Ø­ÙŠØ« ØªØ¸Ù„ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª Ø®Ø§Ø±Ø¬ Ø§Ù„Ù‚Ù†Ø§Ø¹ ÙƒÙ…Ø§ Ù‡ÙŠ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø©.

Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… DiffEdit Ù„ØªØ­Ø±ÙŠØ± Ø§Ù„ØµÙˆØ± Ø¯ÙˆÙ† Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ ÙŠØ¯ÙˆÙŠÙ‹Ø§.

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

```py
# Ù‚Ù… Ø¨Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© ÙÙŠ Colab
#! pip install -q diffusers transformers accelerate
```

ÙŠØªØ·Ù„Ø¨ [`StableDiffusionDiffEditPipeline`] Ù‚Ù†Ø§Ø¹ ØµÙˆØ±Ø© ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ÙƒØ§Ù…Ù†Ø§Øª Ø§Ù„Ù…Ø¹ÙƒÙˆØ³Ø© Ø¬Ø²Ø¦ÙŠÙ‹Ø§. ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„Ø¯Ø§Ù„Ø© [`~StableDiffusionDiffEditPipeline.generate_mask`]`ØŒ ÙˆÙŠØªØ¶Ù…Ù† Ù…Ø¹Ù„Ù…ØªÙŠÙ†ØŒ `source_prompt` Ùˆ`target_prompt`. ØªØ­Ø¯Ø¯ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù…Ø§ Ø³ÙŠØªÙ… ØªØ­Ø±ÙŠØ±Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ ØªØºÙŠÙŠØ± ÙˆØ¹Ø§Ø¡ Ù…Ù† *Ø§Ù„ÙÙˆØ§ÙƒÙ‡* Ø¥Ù„Ù‰ ÙˆØ¹Ø§Ø¡ Ù…Ù† *Ø§Ù„ÙƒÙ…Ø«Ø±Ù‰*ØŒ ÙØ³ØªÙƒÙˆÙ†:

```py
source_prompt = "ÙˆØ¹Ø§Ø¡ Ù…Ù† Ø§Ù„ÙÙˆØ§ÙƒÙ‡"
target_prompt = "ÙˆØ¹Ø§Ø¡ Ù…Ù† Ø§Ù„ÙƒÙ…Ø«Ø±Ù‰"
```

ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙƒØ§Ù…Ù†Ø§Øª Ø§Ù„Ù…Ø¹ÙƒÙˆØ³Ø© Ø¬Ø²Ø¦ÙŠÙ‹Ø§ Ù…Ù† Ø§Ù„Ø¯Ø§Ù„Ø© [`~StableDiffusionDiffEditPipeline.invert`]`ØŒ ÙˆÙ…Ù† Ø§Ù„Ø¬ÙŠØ¯ Ø¹Ù…ÙˆÙ…Ù‹Ø§ ØªØ¶Ù…ÙŠÙ† `prompt` Ø£Ùˆ *caption* Ù„ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ØªÙˆØ¬ÙŠÙ‡ Ø¹Ù…Ù„ÙŠØ© Ø£Ø®Ø° Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø¹ÙƒØ³ÙŠØ© Ù„Ù„ÙƒØ§Ù…Ù†. ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù‡Ùˆ `source_prompt` Ø§Ù„Ø®Ø§Øµ Ø¨ÙƒØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø±Ø¨Ø© Ø£ÙˆØµØ§Ù Ù†ØµÙŠØ© Ø£Ø®Ø±Ù‰!

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ØŒ ÙˆÙ…Ø®Ø·Ø· Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©ØŒ ÙˆÙ…Ø®Ø·Ø· Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø¹ÙƒØ³ÙŠØ©ØŒ ÙˆØªÙ…ÙƒÙŠÙ† Ø¨Ø¹Ø¶ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©:

```py
import torch
from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline

pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
"stabilityai/stable-diffusion-2-1",
torch_dtype=torch.float16,
safety_checker=None,
use_safetensors=True,
)
pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)
pipeline.enable_model_cpu_offload()
pipeline.enable_vae_slicing()
```

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªØ­Ø±ÙŠØ±Ù‡Ø§:

```py
from diffusers.utils import load_image, make_image_grid

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))
raw_image
```

Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`~StableDiffusionDiffEditPipeline.generate_mask`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø§Ù„ØµÙˆØ±Ø©. Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ…Ø±ÙŠØ± `source_prompt` Ùˆ`target_prompt` Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø³ÙŠØªÙ… ØªØ­Ø±ÙŠØ±Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©:

```py
from PIL import Image

source_prompt = "ÙˆØ¹Ø§Ø¡ Ù…Ù† Ø§Ù„ÙÙˆØ§ÙƒÙ‡"
target_prompt = "Ø³Ù„Ø© Ù…Ù† Ø§Ù„ÙƒÙ…Ø«Ø±Ù‰"
mask_image = pipeline.generate_mask(
image=raw_image,
source_prompt=source_prompt,
target_prompt=target_prompt,
)
Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L").resize((768, 768))
```

Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙƒØ§Ù…Ù†Ø§Øª Ø§Ù„Ø¹ÙƒØ³ÙŠØ© ÙˆÙ…Ø±Ø± Ù„Ù‡Ø§ ØªØ¹Ù„ÙŠÙ‚Ù‹Ø§ ÙŠØµÙ Ø§Ù„ØµÙˆØ±Ø©:

```py
inv_latents = pipeline.invert(prompt=source_prompt, image=raw_image).latents
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù‚Ù†Ø§Ø¹ Ø§Ù„ØµÙˆØ±Ø© ÙˆØ§Ù„ÙƒØ§Ù…Ù†Ø§Øª Ø§Ù„Ù…Ø¹ÙƒÙˆØ³Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨. ÙŠØµØ¨Ø­ `target_prompt` Ù‡Ùˆ `prompt` Ø§Ù„Ø¢Ù†ØŒ ÙˆÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… `source_prompt` ÙƒÙ€ `negative_prompt`:

```py
output_image = pipeline(
prompt=target_prompt,
mask_image=mask_image,
image_latents=inv_latents,
negative_prompt=source_prompt,
).images[0]
mask_image = Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L").resize((768, 768))
make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://github.com/Xiang-cd/DiffEdit-stable-diffusion/blob/main/assets/target.png?raw=true"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø±ÙŠØ±</figcaption>
</div>
</div>

## Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„Ù‡Ø¯Ù

ÙŠÙ…ÙƒÙ† Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„Ù‡Ø¯Ù ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§ ÙŠØ¯ÙˆÙŠÙ‹Ø§.

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Flan-T5 ÙˆÙ…ØµÙ†Ù Ø§Ù„Ø±Ù…ÙˆØ² Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Transformers:

```py
import torch
from transformers import AutoTokenizer, T5ForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto", torch_dtype=torch.float16)
```

Ù‚Ø¯Ù… Ø¨Ø¹Ø¶ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù„Ø·Ù„Ø¨ Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØµØ¯Ø± ÙˆØ§Ù„Ù‡Ø¯Ù.

```py
source_concept = "bowl"
target_concept = "basket"

source_text = f"Provide a caption for images containing a {source_concept}. "
"The captions should be in English and should be no longer than 150 characters."

target_text = f"Provide a caption for images containing a {target_concept}. "
"The captions should be in English and should be no longer than 150 characters."
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø§Øª:

```py
@torch.no_grad()
def generate_prompts(input_prompt):
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids.to("cuda")

outputs = model.generate(
input_idsØŒ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© = 0.8ØŒ num_return_sequences=16ØŒ do_sample=TrueØŒ max_new_tokens=128ØŒ top_k=10
)
return tokenizer.batch_decode(outputs, skip_special_tokens=True)

source_prompts = generate_prompts(source_text)
target_prompts = generate_prompts(target_text)
print(source_prompts)
print(target_prompts)
```

<Tip>
Ø¥Ø·Ù„Ø¹ Ø¹Ù„Ù‰ [Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯](https://huggingface.co/docs/transformers/main/en/generation_strategies)
 Ø¥Ø°Ø§ ÙƒÙ†Øª Ù…Ù‡ØªÙ…Ù‹Ø§ Ø¨Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø¥Ù†Ø´Ø§Ø¡ Ù†Øµ Ø°ÙŠ Ø¬ÙˆØ¯Ø© Ù…Ø®ØªÙ„ÙØ©.
</Tip>

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†ØµÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨ÙˆØ§Ø³Ø·Ø© [`StableDiffusionDiffEditPipeline`] Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ. Ø³ØªØ³ØªØ®Ø¯Ù… Ù…Ø´ÙØ± Ø§Ù„Ù†Øµ Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ù†ØµÙŠØ©:

```py
import torch
from diffusers import StableDiffusionDiffEditPipeline

pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
"stabilityai/stable-diffusion-2-1"ØŒ torch_dtype=torch.float16ØŒ use_safetensors=True
)
pipeline.enable_model_cpu_offload()
pipeline.enable_vae_slicing()

@torch.no_grad()
def embed_prompts(sentences, tokenizer, text_encoder, device="cuda"):
embeddings = []
for sent in sentences:
text_inputs = tokenizer(
sentØŒ
padding="max_length"ØŒ
max_length=tokenizer.model_max_lengthØŒ
truncation=TrueØŒ
return_tensors="pt"ØŒ
)
text_input_ids = text_inputs.input_ids
prompt_embeds = text_encoder(text_input_ids.to(device)ØŒ attention_mask=None)[0]
embeddings.append(prompt_embeds)
return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)

source_embeds = embed_prompts(source_prompts, pipeline.tokenizer, pipeline.text_encoder)
target_embeds = embed_prompts(target_prompts, pipeline.tokenizer, pipeline.text_encoder)
```

Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ø§Ù„ØªÙŠÙ† [`~StableDiffusionDiffEditPipeline.generate_mask`] Ùˆ [`~StableDiffusionDiffEditPipeline.invert`]`ØŒ ÙˆØ§Ù„Ø£Ù†Ø¨ÙˆØ¨ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©:

```diff
from diffusers import DDIMInverseScheduler, DDIMScheduler
from diffusers.utils import load_image, make_image_grid
from PIL import Image

pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))

mask_image = pipeline.generate_mask(
image=raw_imageØŒ
-     source_prompt=source_promptØŒ
-     target_prompt=target_promptØŒ
+     source_prompt_embeds=source_embedsØŒ
+     target_prompt_embeds=target_embedsØŒ
)

inv_latents = pipeline.invert(
-     prompt=source_promptØŒ
+     prompt_embeds=source_embedsØŒ
image=raw_imageØŒ
).latents

output_image = pipeline(
mask_image=mask_imageØŒ
image_latents=inv_latentsØŒ
-     prompt=target_promptØŒ
-     negative_prompt=source_promptØŒ
+     prompt_embeds=target_embedsØŒ
+     negative_prompt_embeds=source_embedsØŒ
).images[0]
mask_image = Image.fromarray((mask_image.squeeze()*255).astype("uint8")ØŒ "L")
make_image_grid([raw_imageØŒ mask_imageØŒ output_image]ØŒ rows=1ØŒ cols=3)
```
## Ø¥Ù†Ø´Ø§Ø¡ Ø¹Ù†ÙˆØ§Ù† ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ø§Ù†Ø¹ÙƒØ§Ø³

Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… `source_prompt` ÙƒØ¹Ù†ÙˆØ§Ù† ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§Ù†Ø­Ø±Ø§ÙØ§Øª Ø§Ù„Ø¬Ø²Ø¦ÙŠØ©ØŒ Ø¥Ù„Ø§ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ [BLIP](https://huggingface.co/docs/transformers/model_doc/blip) Ù„ØªÙˆÙ„ÙŠØ¯ Ø¹Ù†ÙˆØ§Ù† ØªÙˆØ¶ÙŠØ­ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§.

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BLIP ÙˆÙ…Ø¹Ø§Ù„Ø¬ BLIP Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Transformers:

```py
import torch
from transformers import BlipForConditionalGeneration, BlipProcessor

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base", torch_dtype=torch.float16, low_cpu_mem_usage=True)
```

Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªÙˆÙ„ÙŠØ¯ Ø¹Ù†ÙˆØ§Ù† ØªÙˆØ¶ÙŠØ­ÙŠ Ù…Ù† ØµÙˆØ±Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„:

```py
@torch.no_grad()
def generate_caption(images, caption_generator, caption_processor):
    text = "a photograph of"

    inputs = caption_processor(images, text, return_tensors="pt").to(device="cuda", dtype=caption_generator.dtype)
    caption_generator.to("cuda")
    outputs = caption_generator.generate(**inputs, max_new_tokens=128)

    # offload caption generator
    caption_generator.to("cpu")

    caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return caption
```

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ø¥Ø¯Ø®Ø§Ù„ ÙˆÙ‚Ù… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø¹Ù†ÙˆØ§Ù† ØªÙˆØ¶ÙŠØ­ÙŠ Ù„Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© `generate_caption`:

```py
from diffusers.utils import load_image

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))
caption = generate_caption(raw_image, model, processor)
```

<div class="flex justify-center">
<figure>
<img class="rounded-xl" src="https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"/>
<figcaption class="text-center">Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù…ÙˆÙ„Ø¯: "ØµÙˆØ±Ø© ÙÙˆØªÙˆØºØ±Ø§ÙÙŠØ© Ù„ÙØ§ÙƒÙ‡Ø© ÙÙŠ ÙˆØ¹Ø§Ø¡ Ø¹Ù„Ù‰ Ø·Ø§ÙˆÙ„Ø©"</figcaption>
</figure>
</div>

Ø§Ù„Ø¢Ù†ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ ÙÙŠ Ø¯Ø§Ù„Ø© [`~StableDiffusionDiffEditPipeline.invert`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø§Ù†Ø­Ø±Ø§ÙØ§Øª Ø§Ù„Ø¬Ø²Ø¦ÙŠØ©!