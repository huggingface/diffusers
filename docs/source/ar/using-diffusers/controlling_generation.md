# التحكم في التوليد

سعى المجتمع منذ فترة طويلة إلى التحكم في المخرجات التي تولدها نماذج الانتشار، وأصبح هذا الآن موضوعًا بحثيًا نشطًا. في العديد من نماذج الانتشار الشائعة، يمكن أن تؤدي التغييرات الطفيفة في المدخلات، سواء الصور أو مطالبات النص، إلى تغيير المخرجات بشكل جذري. في عالم مثالي، نريد أن نتمكن من التحكم في كيفية الحفاظ على الدلالات وتغييرها.

تتعلق معظم الأمثلة على الحفاظ على الدلالات بالقدرة على رسم خريطة دقيقة لتغيير في المدخلات إلى تغيير في المخرجات. أي أن إضافة صفة إلى موضوع في موجه يحافظ على الصورة بأكملها، ولا يعدل سوى الموضوع الذي تم تغييره. أو، تنوع الصور لموضوع معين يحافظ على وضع الموضوع.

بالإضافة إلى ذلك، هناك خصائص للصور المولدة التي نود أن نؤثر عليها إلى ما بعد الحفاظ على الدلالات. أي بشكل عام، نود أن تكون مخرجاتنا ذات جودة جيدة، وتلتزم بأسلوب معين، أو تكون واقعية.

سنقوم بتوثيق بعض التقنيات التي تدعمها "diffusers" للتحكم في توليد نماذج الانتشار. الكثير من ذلك هو أحدث ما توصل إليه البحث ويمكن أن يكون دقيقًا جدًا. إذا كان هناك شيء يحتاج إلى توضيح أو لديك اقتراح، فلا تتردد في فتح مناقشة على [المنتدى](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63) أو [قضية GitHub](https://github.com/huggingface/diffusers/issues).

نقدم شرحًا عالي المستوى لكيفية التحكم في التوليد، بالإضافة إلى مقتطف من الجوانب التقنية. للحصول على تفسيرات أكثر تعمقًا للجوانب التقنية، فإن الأوراق الأصلية التي ترتبط من الأنابيب هي دائمًا أفضل مصادر.

وفقًا لحالة الاستخدام، يجب اختيار تقنية وفقًا لذلك. في كثير من الحالات، يمكن الجمع بين هذه التقنيات. على سبيل المثال، يمكن للمرء أن يجمع بين الانقلاب النصي مع SEGA لتوفير المزيد من الإرشادات الدلالية للمخرجات التي تم إنشاؤها باستخدام الانقلاب النصي.

ما لم يذكر خلاف ذلك، فإن هذه التقنيات تعمل مع النماذج الموجودة ولا تتطلب أوزانها الخاصة.

1. [InstructPix2Pix](#instruct-pix2pix)
2. [Pix2Pix Zero](#pix2pix-zero)
3. [Attend and Excite](#attend-and-excite)
4. [الإرشاد الدلالي](#semantic-guidance-sega)
5. [إرشادات الاهتمام الذاتي](#self-attention-guidance-sag)
6. [Depth2Image](#depth2image)
7. [MultiDiffusion Panorama](#multidiffusion-panorama)
8. [DreamBooth](#dreambooth)
9. [الانقلاب النصي](#textual-inversion)
10. [ControlNet](#controlnet)
11. [وزن المطالبة](#prompt-weighting)
12. [الانتشار المخصص](#custom-diffusion)
13. [تحرير النماذج](#model-editing)
14. [DiffEdit](#diffedit)
15. [T2I-Adapter](#t2i-adapter)
16. [FABRIC](#fabric)

للراحة، نقدم جدولًا للإشارة إلى الطرق التي تعمل بالاستدلال فقط والطرق التي تتطلب الضبط الدقيق/التدريب.

| الأسلوب | الاستدلال فقط | يتطلب التدريب / <br>الضبط الدقيق | التعليقات |
| :----: | :----------: | :--------------------------: | :----: |
| [InstructPix2Pix](#instruct-pix2pix) | ✅ | ❌ | يمكن أيضًا ضبط دقة InstructPix2Pix بشكل أفضل <br>الأداء على تعليمات التحرير المحددة. |
| [Pix2Pix Zero](#pix2pix-zero) | ✅ | ❌ | - |
| [Attend and Excite](#attend-and-excite) | ✅ | ❌ | - |
| [الإرشاد الدلالي](#semantic-guidance-sega) | ✅ | ❌ | - |
| [إرشادات الاهتمام الذاتي](#self-attention-guidance-sag) | ✅ | ❌ | - |
| [Depth2Image](#depth2image) | ✅ | ❌ | - |
| [MultiDiffusion Panorama](#multidiffusion-panorama) | ✅ | ❌ | - |
| [DreamBooth](#dreambooth) | ❌ | ✅ | - |
| [الانقلاب النصي](#textual-inversion) | ❌ | ✅ | - |
| [ControlNet](#controlnet) | ✅ | ❌ | يمكن تدريب ControlNet / ضبطها بدقة <br>على شرط مخصص. |
| [وزن المطالبة](#prompt-weighting) | ✅ | ❌ | - |
| [الانتشار المخصص](#custom-diffusion) | ❌ | ✅ | - |
| [تحرير النماذج](#model-editing) | ✅ | ❌ | - |
| [DiffEdit](#diffedit) | ✅ | ❌ | - |
| [T2I-Adapter](#t2i-adapter) | ✅ | ❌ | - |
| [FABRIC](#fabric) | ✅ | ❌ | - |

## InstructPix2Pix

[ورقة](<https://arxiv.org/abs/2211.09800>)

[InstructPix2Pix](../api/pipelines/pix2pix) يتم ضبطها بدقة من الانتشار المستقر لدعم تحرير الصور المدخلة. فهو يأخذ كمدخلات صورة وموجهًا يصف عملية التحرير، ويخرج الصورة المعدلة.

تم تدريب InstructPix2Pix بشكل صريح للعمل بشكل جيد مع مطالبات تشبه [InstructGPT](https://openai.com/blog/instruction-following/) .

## Pix2Pix Zero

[ورقة](<https://arxiv.org/abs/2302.03027>)

يسمح [Pix2Pix Zero](../api/pipelines/pix2pix_zero) بتعديل صورة بحيث يتم ترجمة مفهوم أو موضوع واحد إلى آخر مع الحفاظ على الدلالات العامة للصورة.

تتم توجيه عملية إزالة الضوضاء من تضمين مفهوم واحد إلى آخر. يتم تحسين الوسائط الوسيطة أثناء عملية إزالة الضوضاء لدفع خرائط الاهتمام نحو خرائط الاهتمام المرجعية. خرائط الاهتمام المرجعية هي من عملية إزالة ضوضاء الصورة المدخلة ويتم استخدامها لتشجيع الحفاظ على الدلالات.

يمكن استخدام Pix2Pix Zero لتحرير الصور الاصطناعية وكذلك الصور الحقيقية.

- لتحرير الصور الاصطناعية، قم أولاً بتوليد صورة بالنظر إلى التعليق التوضيحي. بعد ذلك، نقوم بتوليد تعليقات توضيحية للصور للمفهوم الذي يجب تحريره وللمفهوم المستهدف الجديد. يمكننا استخدام نموذج مثل [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) لهذا الغرض. بعد ذلك، يتم إنشاء تضمينات "متوسطة" للتعليق التوضيحي لكل من المفهوم المصدر والهدف من خلال الترميز النصي. أخيرًا، يتم استخدام خوارزمية pix2pix-zero لتحرير الصورة الاصطناعية.

- لتحرير صورة حقيقية، قم أولاً بتوليد تعليق توضيحي للصورة باستخدام نموذج مثل [BLIP](https://huggingface.co/docs/transformers/model_doc/blip). بعد ذلك، قم بتطبيق عكس DDIM على التعليق التوضيحي والصورة لتوليد وسائط "عكسية". مثلما كان من قبل، يتم إنشاء تضمينات "متوسطة" للتعليق التوضيحي لكل من المفهوم المصدر والهدف، وأخيرًا يتم استخدام خوارزمية pix2pix-zero بالاقتران مع الوسائط "العكسية" لتحرير الصورة.

<Tip>

Pix2Pix Zero هو أول نموذج يسمح بتحرير الصور "بدون تصوير". وهذا يعني أن النموذج
يمكنه تحرير صورة في أقل من دقيقة على وحدة معالجة رسومات المستهلك كما هو موضح [هنا](../api/pipelines/pix2pix_zero#usage-example).

</Tip>

كما ذكرنا سابقًا، يتضمن Pix2Pix Zero تحسين الوسائط (وليس أي من UNet أو VAE أو الترميز النصي) لتوجيه التوليد نحو مفهوم محدد. وهذا يعني أن الأنبوب بأكمله قد يتطلب ذاكرة أكبر من [StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img) القياسي.

<Tip>

هناك تمييز مهم بين الأساليب مثل InstructPix2Pix و Pix2Pix Zero وهو أن السابق
ينطوي على ضبط دقيق للأوزان المسبقة التدريب في حين أن الأخير لا يفعل ذلك. وهذا يعني أنك يمكن
تطبيق Pix2Pix Zero على أي من نماذج الانتشار المستقر المتاحة.

</Tip>
## Attend and Excite

[Paper](https://arxiv.org/abs/2301.13826)

تسمح [Attend and Excite](../api/pipelines/attend_and_excite) بتمثيل الموضوعات في النص الموجه بشكل أمين في الصورة النهائية.

يتم إدخال مجموعة من مؤشرات الرموز، والتي تتوافق مع الموضوعات في النص الموجه التي تحتاج إلى أن تكون موجودة في الصورة. أثناء إزالة التشويش، يتم ضمان أن يكون لكل مؤشر رمز عتبة اهتمام دنيا لرقعة واحدة على الأقل من الصورة. يتم تحسين الوسائط المخفية بشكل تكراري أثناء عملية إزالة التشويش لتعزيز اهتمام رمز الموضوع الأكثر إهمالا حتى يتم تجاوز عتبة الاهتمام لجميع رموز الموضوع.

مثل Pix2Pix Zero، تتضمن Attend and Excite أيضًا حلقة تحسين مصغرة (مع عدم المساس بالأوزان المسبقة التدريب) في خط أنابيبها وقد تتطلب ذاكرة أكبر من [StableDiffusionPipeline](../api/pipelines/stable_diffusion/text2img) المعتادة.

## Semantic Guidance (SEGA)

[Paper](https://arxiv.org/abs/2301.12247)

تسمح [SEGA](../api/pipelines/semantic_stable_diffusion) بتطبيق أو إزالة مفهوم أو أكثر من صورة. يمكن أيضًا التحكم في قوة المفهوم. على سبيل المثال، يمكن استخدام مفهوم الابتسامة لزيادة أو تقليل ابتسامة صورة شخصية بشكل تدريجي.

على غرار كيفية تقديم الإرشادات الخالية من التصنيف إرشادات عبر إدخالات النص الفارغ، توفر SEGA إرشادات بشأن النصوص المفاهيمية. يمكن تطبيق العديد من هذه النصوص المفاهيمية في نفس الوقت. يمكن أن يضيف كل نص مفهوم إما إضافة مفهومه أو إزالته اعتمادًا على ما إذا كان الإرشاد مطبقًا بشكل إيجابي أو سلبي.

على عكس Pix2Pix Zero أو Attend and Excite، تتفاعل SEGA مباشرة مع عملية الانتشار بدلاً من إجراء أي تحسين صريح يعتمد على التدرجات.

## Self-attention Guidance (SAG)

[Paper](https://arxiv.org/abs/2210.00939)

[Self-attention Guidance](../api/pipelines/self_attention_guidance) يحسن الجودة العامة للصور.

يوفر SAG إرشادات من تنبؤات غير مشروطة على تفاصيل الترددات العالية للصور المشروطة بالكامل. يتم استخراج تفاصيل الترددات العالية من خرائط الاهتمام الذاتي لـ UNet.

## Depth2Image

[Project](https://huggingface.co/stabilityai/stable-diffusion-2-depth)

[Depth2Image](../api/pipelines/stable_diffusion/depth2img) تمت معايرته بدقة من Stable Diffusion للحفاظ بشكل أفضل على الدلالات للتباين الموجه بالنص الصورة.

فهي تفرض شرطًا على تقدير العمق الأحادي للصورة الأصلية.

## MultiDiffusion Panorama

[Paper](https://arxiv.org/abs/2302.08113)

يحدد [MultiDiffusion Panorama](../api/pipelines/panorama) عملية توليد جديدة عبر نموذج انتشار مسبق التدريب. تربط هذه العملية بين طرق التوليد المتعددة للانتشار التي يمكن تطبيقها بسهولة لتوليد صور عالية الجودة ومتنوعة. تلتزم النتائج بالضوابط التي يوفرها المستخدم، مثل نسبة العرض إلى الارتفاع المرغوبة (على سبيل المثال، بانوراما)، وإشارات التوجيه المكاني، والتي تتراوح من أقنعة التجزئة المحكمة إلى صناديق الحدود.

يتيح MultiDiffusion Panorama إمكانية إنشاء صور عالية الجودة بنسب عرض إلى ارتفاع عشوائية (مثل المناظر الطبيعية).

## ضبط النماذج الخاصة بك

بالإضافة إلى النماذج المدربة مسبقًا، تحتوي Diffusers على نصوص تدريب لضبط دقة النماذج على البيانات التي يوفرها المستخدم.

## DreamBooth

[Project](https://dreambooth.github.io/)

[DreamBooth](../training/dreambooth) يضبط دقة نموذج لتعليمه حول موضوع جديد. على سبيل المثال، يمكن استخدام بضع صور لشخص ما لتوليد صور لذلك الشخص بأساليب مختلفة.

## Textual Inversion

[Paper](https://arxiv.org/abs/2208.01618)

[Textual Inversion](../training/text_inversion) يضبط دقة نموذج لتعليمه حول مفهوم جديد. على سبيل المثال، يمكن استخدام بضع صور لأسلوب فني لتوليد صور بهذا الأسلوب.

## ControlNet

[Paper](https://arxiv.org/abs/2302.05543)

[ControlNet](../api/pipelines/controlnet) عبارة عن شبكة مساعدة تضيف شرطًا إضافيًا.

هناك 8 شبكات ControlNet أساسية مسبقة التدريب تم تدريبها على عمليات ضبط مختلفة مثل اكتشاف الحواف، والخربشات، وخرائط العمق، والتجزئات الدلالية.

## Prompt Weighting

[Prompt weighting](../using-diffusers/weighted_prompts) هي تقنية بسيطة تضع المزيد من وزن الاهتمام على أجزاء معينة من إدخال النص.

## Custom Diffusion

[Paper](https://arxiv.org/abs/2212.04488)

[Custom Diffusion](../training/custom_diffusion) يقوم بضبط دقة خرائط الاهتمام المتقاطع لنموذج انتشار موجه بالنص إلى الصورة مسبق التدريب فقط. كما أنه يسمح أيضًا بإجراء عكس نصي إضافي. فهو يدعم التدريب متعدد المفاهيم بشكل افتراضي. مثل DreamBooth وTextual Inversion، يستخدم Custom Diffusion أيضًا لتعليم نموذج انتشار موجه مسبق التدريب بالنص إلى الصورة حول مفاهيم جديدة لتوليد مخرجات تتضمن مفهوم (المفاهيم) محل الاهتمام.

## Model Editing

[Paper](https://arxiv.org/abs/2303.08084)

يساعدك [خط أنابيب تحرير النموذج الموجه بالنص إلى الصورة](../api/pipelines/model_editing) على التخفيف من بعض الافتراضات الضمنية غير الصحيحة التي قد يقوم بها نموذج انتشار موجه مسبق التدريب بالنص إلى الصورة حول الموضوعات الموجودة في إدخال النص الموجه. على سبيل المثال، إذا قمت بتشغيل Stable Diffusion لتوليد صور لـ "حزمة من الورود"، فمن المرجح أن تكون الورود في الصور المولدة حمراء. يساعدك هذا الخط الأنابيب على تغيير هذا الافتراض.

## DiffEdit

[Paper](https://arxiv.org/abs/2210.11427)

يسمح [DiffEdit](../api/pipelines/diffedit) بالتحرير الدلالي لصور الإدخال إلى جانب إدخالات النص الموجه مع الحفاظ على صور الإدخال الأصلية قدر الإمكان.

## T2I-Adapter

[Paper](https://arxiv.org/abs/2302.08453)

[T2I-Adapter](../api/pipelines/stable_diffusion/adapter) عبارة عن شبكة مساعدة تضيف شرطًا إضافيًا.

هناك 8 محولات أساسية مسبقة التدريب تم تدريبها على عمليات ضبط مختلفة مثل اكتشاف الحواف، والرسومات، وخرائط العمق، والتجزئات الدلالية.

## Fabric

[Paper](https://arxiv.org/abs/2307.10159)

[Fabric](https://github.com/huggingface/diffusers/tree/442017ccc877279bcf24fbe92f92d3d0def191b6/examples/community#stable-diffusion-fabric-pipeline) هو نهج خالٍ من التدريب قابل للتطبيق على مجموعة واسعة من نماذج الانتشار الشائعة، والتي تستغل طبقة الاهتمام الذاتي الموجودة في أكثر الهندسات المعمارية استخدامًا لفرض عملية الانتشار على مجموعة من صور التعليقات.