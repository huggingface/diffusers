# ControlNet

ControlNet Ù‡Ùˆ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„ØµÙˆØ± Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙˆÙÙŠØ± Ø¯Ø®Ù„ Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„ØµÙˆØ±Ø©. Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ·Ø© (Canny edgeØŒ Ø±Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ ÙˆØ¶Ø¹ Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ØŒ Ø§Ù„Ø¹Ù…Ù‚ØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯) Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø±. ÙˆÙ‡Ø°Ø§ Ù…ÙÙŠØ¯ Ù„Ù„ØºØ§ÙŠØ© Ù„Ø£Ù†Ù‡ ÙŠÙ…Ù†Ø­Ùƒ ØªØ­ÙƒÙ…Ù‹Ø§ Ø£ÙƒØ¨Ø± ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±ØŒ Ù…Ù…Ø§ ÙŠØ³Ù‡Ù„ Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ù…Ø­Ø¯Ø¯Ø© Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ¬Ø±Ø¨Ø© Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ù†ØµÙŠØ© Ø£Ùˆ Ù‚ÙŠÙ… Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.

<Tip>
Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø³Ù… 3.5 Ù…Ù† ÙˆØ±Ù‚Ø© [ControlNet](https://huggingface.co/papers/2302.05543) Ø§Ù„Ø¥ØµØ¯Ø§Ø± 1 Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¨ØªÙ†ÙÙŠØ°Ø§Øª ControlNet Ù„Ù…Ø®ØªÙ„Ù Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ·Ø©. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ ControlNet Ø§Ù„Ù…Ø´Ø±ÙˆØ·Ø© Ø§Ù„Ø±Ø³Ù…ÙŠØ© ÙˆØ§Ù„Ù…Ø³ØªÙ‚Ø±Ø© Ø¹Ù„Ù‰ Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ [lllyasviel](https://huggingface.co/lllyasviel) HubØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ [Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹](https://huggingface.co/models?other=stable-diffusion&other=controlnet) Ø¹Ù„Ù‰ Hub.

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù†Ù…Ø§Ø°Ø¬ ControlNet SDXL (Stable Diffusion XL)ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ ÙÙŠ Ù…Ù†Ø¸Ù…Ø© ğŸ¤— [Diffusers](https://huggingface.co/diffusers) HubØŒ Ø£Ùˆ ÙŠÙ…ÙƒÙ†Ùƒ ØªØµÙØ­ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ [Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹](https://huggingface.co/models?other=stable-diffusion-xl&other=controlnet) Ø¹Ù„Ù‰ Hub.
</Tip>

ÙŠØ­ØªÙˆÙŠ Ù†Ù…ÙˆØ°Ø¬ ControlNet Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ØªÙŠÙ† Ù…Ù† Ø§Ù„Ø£ÙˆØ²Ø§Ù† (Ø£Ùˆ Ø§Ù„ÙƒØªÙ„) Ù…ØªØµÙ„Ø© Ø¨Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØµÙÙŠØ© Ø§Ù„ØµÙØ±ÙŠØ©:

- *Ù†Ø³Ø®Ø© Ù…Ø­Ù…ÙŠØ©* ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ø§ ØªØ¹Ù„Ù…Ù‡ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ù…Ø³Ø¨Ù‚ Ø§Ù„ÙƒØ¨ÙŠØ±
- *Ù†Ø³Ø®Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨* ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ø¹Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ

Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ù…ÙŠØ© ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³Ø¨Ù‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙØ¥Ù† ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ†ÙÙŠØ° ControlNet Ø¹Ù„Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø´Ø±Ø· Ø¬Ø¯ÙŠØ¯ Ø³Ø±ÙŠØ¹ Ù…Ø«Ù„ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¢Ø®Ø± Ù„Ø£Ù†Ùƒ Ù„Ø§ ØªØ¯Ø±Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„ØµÙØ±.

Ø³ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… ControlNet Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù† Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©ØŒ ÙˆÙ…Ù† ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø©ØŒ ÙˆØ§Ù„Ø·Ù„Ø§Ø¡ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØŒ ÙˆØ§Ù„Ù…Ø²ÙŠØ¯! Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø£Ù†ÙˆØ§Ø¹ Ù…Ø¯Ø®Ù„Ø§Øª ControlNet Ù„Ù„Ø§Ø®ØªÙŠØ§Ø± Ù…Ù† Ø¨ÙŠÙ†Ù‡Ø§ØŒ ÙˆÙ„ÙƒÙ† ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ø³Ù†Ø±ÙƒØ² ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ù…Ù†Ù‡Ø§. Ù„Ø§ ØªØªØ±Ø¯Ø¯ ÙÙŠ ØªØ¬Ø±Ø¨Ø© Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø£Ø®Ø±Ù‰!

Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

```py
# Ù‚Ù… Ø¨Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ù„ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© ÙÙŠ Colab
#! pip install -q diffusers transformers accelerate opencv-python
```

## Ù…Ù† Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù† Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØ±Ø©ØŒ Ø¹Ø§Ø¯Ø©Ù‹ Ù…Ø§ ÙŠØªÙ… ØªÙ…Ø±ÙŠØ± Ù…Ø·Ø§Ù„Ø¨Ø© Ù†ØµÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙˆÙ„ÙƒÙ† Ù…Ø¹ ControlNetØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø¯ÙŠØ¯ Ø¥Ø¯Ø®Ø§Ù„ Ø´Ø±Ø· Ø¥Ø¶Ø§ÙÙŠ. Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø´ØªØ±Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØµÙˆØ±Ø© CannyØŒ ÙˆÙ‡Ùˆ Ù…Ø®Ø·Ø· Ø£Ø¨ÙŠØ¶ Ù„ØµÙˆØ±Ø© Ø¹Ù„Ù‰ Ø®Ù„ÙÙŠØ© Ø³ÙˆØ¯Ø§Ø¡. Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©ØŒ ÙŠÙ…ÙƒÙ† Ù„Ù€ ControlNet Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙˆØ±Ø© Canny ÙƒØ¹Ù†ØµØ± ØªØ­ÙƒÙ… Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø®Ø·Ø·.

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© ÙˆØ§Ø³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø© [opencv-python](https://github.com/opencv/opencv-python) Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØµÙˆØ±Ø© Canny:

```py
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import cv2
import numpy as np

original_image = load_image(
"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"
)

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/vermeer_canny_edged.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ØµÙˆØ±Ø© Canny</figcaption>
</div>
</div>

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ControlNet Ø§Ù„Ù…Ø´Ø±ÙˆØ· Ø¹Ù„Ù‰ Ø§ÙƒØªØ´Ø§Ù Ø­Ø§ÙØ© Canny ÙˆÙ…Ø±Ø±Ù‡ Ø¥Ù„Ù‰ [`StableDiffusionControlNetPipeline`]. Ø§Ø³ØªØ®Ø¯Ù… [`UniPCMultistepScheduler`] Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆÙ‚Ù… Ø¨ØªÙ…ÙƒÙŠÙ† Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© Ù„Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
"runwayml/stable-diffusion-v1-5"ØŒ controlnet=controlnetØŒ torch_dtype=torch.float16ØŒ use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ø·Ø§Ù„Ø¨ØªÙƒ ÙˆØµÙˆØ±Ø© Canny Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨:

```py
output = pipe(
"the mona lisa"ØŒ image=canny_image
).images[0]
make_image_grid([original_image, canny_image, output], rows=1, cols=3)
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-text2img.png"/>
</div>

## Ù…Ù† ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø©

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ù…Ù† ØµÙˆØ±Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø©ØŒ Ø¹Ø§Ø¯Ø©Ù‹ Ù…Ø§ ÙŠØªÙ… ØªÙ…Ø±ÙŠØ± ØµÙˆØ±Ø© Ø£ÙˆÙ„ÙŠØ© ÙˆØ·Ù„Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©. Ù…Ø¹ ControlNetØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ± Ø¥Ø¯Ø®Ø§Ù„ Ø´Ø±Ø· Ø¥Ø¶Ø§ÙÙŠ Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø´ØªØ±Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¹Ù…Ù‚ØŒ ÙˆÙ‡ÙŠ ØµÙˆØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙƒØ§Ù†ÙŠØ©. Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©ØŒ ÙŠÙ…ÙƒÙ† Ù„Ù€ ControlNet Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¹Ù…Ù‚ ÙƒØ¹Ù†ØµØ± ØªØ­ÙƒÙ… Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© ØªØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙƒØ§Ù†ÙŠØ©.

Ø³ØªØ³ØªØ®Ø¯Ù… [`StableDiffusionControlNetImg2ImgPipeline`] Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ®ØªÙ„Ù Ø¹Ù† [`StableDiffusionControlNetPipeline`] Ù„Ø£Ù†Ù‡Ø§ ØªØ³Ù…Ø­ Ù„Ùƒ Ø¨ØªÙ…Ø±ÙŠØ± ØµÙˆØ±Ø© Ø£ÙˆÙ„ÙŠØ© ÙƒÙ†Ù‚Ø·Ø© Ø¨Ø¯Ø§ÙŠØ© Ù„Ø¹Ù…Ù„ÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©.

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ `depth-estimation` [`~transformers.Pipeline`] Ù…Ù† ğŸ¤— Transformers Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®Ø±ÙŠØ·Ø© Ø¹Ù…Ù‚ Ø§Ù„ØµÙˆØ±Ø©:

```py
import torch
import numpy as np

from transformers import pipeline
from diffusers.utils import load_image, make_image_grid

image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg"
)

def get_depth_map(image, depth_estimator):
image = depth_estimator(image)["depth"]
image = np.array(image)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
detected_map = torch.from_numpy(image).float() / 255.0
depth_map = detected_map.permute(2, 0, 1)
return depth_map

depth_estimator = pipeline("depth-estimation")
depth_map = get_depth_map(image, depth_estimator).unsqueeze(0).half().to("cuda")
```

Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ControlNet Ø§Ù„Ù…Ø´Ø±ÙˆØ· Ø¹Ù„Ù‰ Ø®Ø±Ø§Ø¦Ø· Ø§Ù„Ø¹Ù…Ù‚ ÙˆÙ…Ø±Ø±Ù‡ Ø¥Ù„Ù‰ [`StableDiffusionControlNetImg2ImgPipeline`]. Ø§Ø³ØªØ®Ø¯Ù… [`UniPCMultistepScheduler`] Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆÙ‚Ù… Ø¨ØªÙ…ÙƒÙŠÙ† Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© Ù„Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

```py
from diffusers import StableDiffusionControlNetImg2ImgPipeline, ControlNetModel, UniPCMultistepScheduler
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11f1p_sd15_depth"ØŒ torch_dtype=torch.float16ØŒ use_safetensors=True)
pipe = StableDiffusionControlNetImg2ImgPipeline.from_pretrained(
"runwayml/stable-diffusion-v1-5"ØŒ controlnet=controlnetØŒ torch_dtype=torch.float16ØŒ use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ø·Ø§Ù„Ø¨ØªÙƒØŒ ÙˆØ§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©ØŒ ÙˆØ®Ø±ÙŠØ·Ø© Ø§Ù„Ø¹Ù…Ù‚ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨:

```py
output = pipe(
"lego batman and robin"ØŒ image=image, control_image=depth_map,
).images[0]
make_image_grid([image, output], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img.jpg"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-img2img-2.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©</figcaption>
</div>
</div>

## Inpainting

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ØªÙ‚Ù†ÙŠØ© InpaintingØŒ ÙØ£Ù†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØµÙˆØ±Ø© Ø£ÙˆÙ„ÙŠØ©ØŒ ÙˆØµÙˆØ±Ø© Ù‚Ù†Ø§Ø¹ØŒ ÙˆÙˆØµÙ ÙŠØ­Ø¯Ø¯ Ù…Ø§ ÙŠØ¬Ø¨ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù‚Ù†Ø§Ø¹ Ø¨Ù‡. ØªØ³Ù…Ø­ Ù†Ù…Ø§Ø°Ø¬ ControlNet Ø¨Ø¥Ø¶Ø§ÙØ© ØµÙˆØ±Ø© ØªØ­ÙƒÙ… Ø£Ø®Ø±Ù‰ Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø¯Ø¹Ù†Ø§ Ù†Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ù†Ø§Ø¹ Inpainting. Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©ØŒ ÙŠÙ…ÙƒÙ† Ù„Ù€ ControlNet Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚Ù†Ø§Ø¹ Inpainting ÙƒÙˆØ³ÙŠÙ„Ø© ØªØ­ÙƒÙ… Ù„ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø© Ø¯Ø§Ø®Ù„ Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù‚Ù†Ø§Ø¹.

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ø£ÙˆÙ„ÙŠØ© ÙˆØµÙˆØ±Ø© Ù‚Ù†Ø§Ø¹:

```py
from diffusers.utils import load_image, make_image_grid

init_image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg"
)
init_image = init_image.resize((512, 512))

mask_image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg"
)
mask_image = mask_image.resize((512, 512))
make_image_grid([init_image, mask_image], rows=1, cols=2)
```

Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ø§Ù„Ø© Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙˆØ±Ø© Ø§Ù„ØªØ­ÙƒÙ… Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ© ÙˆØµÙˆØ±Ø© Ø§Ù„Ù‚Ù†Ø§Ø¹. Ø³ÙŠØ¤Ø¯ÙŠ Ù‡Ø°Ø§ Ø¥Ù„Ù‰ Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¨ÙƒØ³Ù„Ø§Øª ÙÙŠ `init_image` ÙƒØ¨ÙƒØ³Ù„Ø§Øª Ù…Ù‚Ù†Ø¹Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¨ÙƒØ³Ù„ Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„ ÙÙŠ `mask_image` Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø¹ØªØ¨Ø© Ù…Ø¹ÙŠÙ†Ø©.

```py
import numpy as np
import torch

def make_inpaint_condition(image, image_mask):
    image = np.array(image.convert("RGB")).astype(np.float32) / 255.0
    image_mask = np.array(image_mask.convert("L")).astype(np.float32) / 255.0

    assert image.shape[0:1] == image_mask.shape[0:1]
    image[image_mask > 0.5] = -1.0 # set as masked pixel
    image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)
    image = torch.from_numpy(image)
    return image

control_image = make_inpaint_condition(init_image, mask_image)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint.jpg"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-mask.jpg"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ØµÙˆØ±Ø© Ø§Ù„Ù‚Ù†Ø§Ø¹</figcaption>
</div>
</div>

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ControlNet Ø§Ù„Ù…Ø´Ø±ÙˆØ· Ø¨Ù€ Inpainting ÙˆÙ…Ø±Ø±Ù‡ Ø¥Ù„Ù‰ [`StableDiffusionControlNetInpaintPipeline`]. Ø§Ø³ØªØ®Ø¯Ù… [`UniPCMultistepScheduler`] Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆÙ‚Ù… Ø¨ØªÙ…ÙƒÙŠÙ† ØªÙØ±ÙŠØº Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

```py
from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel, UniPCMultistepScheduler

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(
"runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, use_safetensors=True
)

pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± ÙˆØµÙÙƒØŒ ÙˆØ§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©ØŒ ÙˆØµÙˆØ±Ø© Ø§Ù„Ù‚Ù†Ø§Ø¹ØŒ ÙˆØµÙˆØ±Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø¥Ù„Ù‰ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨:

```py
output = pipe(
"corgi face with large ears, detailed, pixar, animated, disney",
num_inference_steps=20,
eta=1.0,
image=init_image,
mask_image=mask_image,
control_image=control_image,
).images[0]
make_image_grid([init_image, mask_image, output], rows=1, cols=3)
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet-inpaint-result.png"/>
</div>

## ÙˆØ¶Ø¹ Ø§Ù„ØªØ®Ù…ÙŠÙ†

[ÙˆØ¶Ø¹ Ø§Ù„ØªØ®Ù…ÙŠÙ†](https://github.com/lllyasviel/ControlNet/discussions/188) Ù„Ø§ ÙŠØªØ·Ù„Ø¨ ØªÙˆÙÙŠØ± ÙˆØµÙ Ù„Ø´Ø¨ÙƒØ© Ø§Ù„ØªØ­ÙƒÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚! ÙˆÙ‡Ø°Ø§ ÙŠØ¬Ø¨Ø± Ù…Ø´ÙØ± ControlNet Ø¹Ù„Ù‰ Ø¨Ø°Ù„ Ù‚ØµØ§Ø±Ù‰ Ø¬Ù‡Ø¯Ù‡ Ù„Ù€ "ØªØ®Ù…ÙŠÙ†" Ù…Ø­ØªÙˆÙŠØ§Øª Ø®Ø±ÙŠØ·Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ù…Ø¯Ø®Ù„Ø© (Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¹Ù…Ù‚ØŒ ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙˆØ¶Ø¹ØŒ Canny edgeØŒ Ø¥Ù„Ø®).

ÙŠÙ‚ÙˆÙ… ÙˆØ¶Ø¹ Ø§Ù„ØªØ®Ù…ÙŠÙ† Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ø®Ù„ÙØ§Øª Ø§Ù„Ù†Ø§ØªØ¬Ø© Ø¹Ù† ControlNet ÙˆÙÙ‚Ù‹Ø§ Ù„Ù†Ø³Ø¨Ø© Ø«Ø§Ø¨ØªØ© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¹Ù…Ù‚ Ø§Ù„ÙƒØªÙ„Ø©. ÙŠÙ‚Ø§Ø¨Ù„ Ø§Ù„ÙƒØªÙ„Ø© Ø§Ù„Ø£Ø¹Ù…Ù‚ `DownBlock` 0.1ØŒ ÙˆÙ…Ø¹ Ø²ÙŠØ§Ø¯Ø© Ø¹Ù…Ù‚ Ø§Ù„ÙƒØªÙ„ØŒ ÙŠØ²ÙŠØ¯ Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ Ø¨Ø´ÙƒÙ„ Ø£Ø³Ù‘ÙŠ Ø¨Ø­ÙŠØ« ÙŠØµØ¨Ø­ Ù…Ù‚ÙŠØ§Ø³ Ø¥Ø®Ø±Ø§Ø¬ `MidBlock` 1.0.

<Tip>
Ù„Ø§ ÙŠØ¤Ø«Ø± ÙˆØ¶Ø¹ Ø§Ù„ØªØ®Ù…ÙŠÙ† Ø¹Ù„Ù‰ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆØµÙ ÙˆÙŠÙ…ÙƒÙ†Ùƒ Ù„Ø§ ØªØ²Ø§Ù„ ØªÙˆÙÙŠØ± ÙˆØµÙ Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ø°Ù„Ùƒ.
</Tip>

Ù‚Ù… Ø¨ØªØ¹ÙŠÙŠÙ† `guess_mode=True` ÙÙŠ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ØŒ ÙˆÙ…Ù† [Ø§Ù„Ù…Ø³ØªØ­Ø³Ù†](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode) ØªØ¹ÙŠÙŠÙ† Ù‚ÙŠÙ…Ø© `guidance_scale` Ø¨ÙŠÙ† 3.0 Ùˆ5.0.

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
from PIL import Image
import cv2

controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny", use_safetensors=True)
pipe = StableDiffusionControlNetPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", controlnet=controlnet, use_safetensors=True).to("cuda")

original_image = load_image("https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png")

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe("", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ø§Ø¯ÙŠ Ù…Ø¹ Ø§Ù„ÙˆØµÙ</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0_gm.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ÙˆØ¶Ø¹ Ø§Ù„ØªØ®Ù…ÙŠÙ† Ø¨Ø¯ÙˆÙ† ÙˆØµÙ</figcaption>
</div>
</div>

## ControlNet with Stable Diffusion XL

ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠØŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ù†Ù…Ø§Ø°Ø¬ ControlNet Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Stable Diffusion XL (SDXL)ØŒ ÙˆÙ„ÙƒÙ†Ù†Ø§ Ù‚Ù…Ù†Ø§ Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† ÙƒØ§Ù…Ù„ÙŠÙ† Ù…Ù† Ù†Ù…Ø§Ø°Ø¬ ControlNet Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ SDXL Ø§Ù„Ù…Ø´Ø±ÙˆØ·Ø© Ø¹Ù„Ù‰ ÙƒØ´Ù Ø­ÙˆØ§Ù ÙƒØ§Ù†ÙŠ (Canny edge detection) ÙˆØ®Ø±Ø§Ø¦Ø· Ø§Ù„Ø¹Ù…Ù‚ (depth maps). ÙƒÙ…Ø§ Ù†Ø¬Ø±ÙŠ ØªØ¬Ø§Ø±Ø¨ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø£ØµØºØ± Ù…Ù† Ù†Ù…Ø§Ø°Ø¬ ControlNet Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ SDXL Ù„ØªØ³Ù‡ÙŠÙ„ ØªØ´ØºÙŠÙ„Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ù…Ø­Ø¯ÙˆØ¯Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© Ø¹Ù„Ù‰ [Ù…Ù†Ø¸Ù…Ø© ğŸ¤— Diffusers Hub](https://huggingface.co/diffusers)!

Ù„Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…ÙˆØ°Ø¬ SDXL ControlNet Ø§Ù„Ù…Ø´Ø±ÙˆØ· Ø¹Ù„Ù‰ ØµÙˆØ± ÙƒØ§Ù†ÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ ØµÙˆØ±Ø©. Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© ÙˆØ¥Ø¹Ø¯Ø§Ø¯ ØµÙˆØ±Ø© ÙƒØ§Ù†ÙŠ:

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import cv2
import numpy as np
import torch

original_image = load_image(
"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
)

image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
make_image_grid([original_image, canny_image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/hf-logo-canny.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ØµÙˆØ±Ø© ÙƒØ§Ù†ÙŠ</figcaption>
</div>
</div>

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ SDXL ControlNet Ø§Ù„Ù…Ø´Ø±ÙˆØ· Ø¹Ù„Ù‰ ÙƒØ´Ù Ø­ÙˆØ§Ù ÙƒØ§Ù†ÙŠ ÙˆÙ…Ø±Ø±Ù‡ Ø¥Ù„Ù‰ [`StableDiffusionXLControlNetPipeline`]. ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªÙ…ÙƒÙŠÙ† Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ© (CPU) Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

```py
controlnet = ControlNetModel.from_pretrained(
"diffusers/controlnet-canny-sdxl-1.0",
torch_dtype=torch.float16,
use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
"stabilityai/stable-diffusion-xl-base-1.0",
controlnet=controlnet,
vae=vae,
torch_dtype=torch.float16,
use_safetensors=True
)
pipe.enable_model_cpu_offload()
```

Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø­Ø« (prompt) (ÙˆØ§Ù„Ù…Ø­Ø« Ø§Ù„Ø³Ù„Ø¨ÙŠ Ø¥Ù† ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù…Ù‡) ÙˆØµÙˆØ±Ø© ÙƒØ§Ù†ÙŠ Ø¥Ù„Ù‰ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ (pipeline):

<Tip>

ÙŠØ­Ø¯Ø¯ Ù…Ø¹Ø§Ù…Ù„ [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale) Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù…Ø®ØµØµ Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„ØªÙƒÙŠÙŠÙ. Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§ Ù‡ÙŠ 0.5 Ù„ØªØ­Ù‚ÙŠÙ‚ ØªØ¹Ù…ÙŠÙ… Ø¬ÙŠØ¯ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø±Ø¨Ø© Ø£Ø±Ù‚Ø§Ù… Ø£Ø®Ø±Ù‰!

</Tip>

```py
prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = 'low quality, bad quality, sketches'

image = pipe(
prompt,
negative_prompt=negative_prompt,
image=canny_image,
controlnet_conditioning_scale=0.5,
).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

<div class="flex justify-center">
<img class="rounded-xl" src="https://huggingface.co/diffusers/controlnet-canny-sdxl-1.0/resolve/main/out_hug_lab_7.png"/>
</div>

ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… [`StableDiffusionXLControlNetPipeline`] ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªØ®Ù…ÙŠÙ† Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø¥Ù„Ù‰ `True`:

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL
from diffusers.utils import load_image, make_image_grid
import numpy as np
import torch
import cv2
from PIL import Image

prompt = "aerial view, a futuristic research complex in a bright foggy jungle, hard lighting"
negative_prompt = "low quality, bad quality, sketches"

original_image = load_image(
"https://hf.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd_controlnet/hf-logo.png"
)

controlnet = ControlNetModel.from_pretrained(
"diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
)
vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
"stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.enable_model_cpu_offload()

image = np.array(original_image)
image = cv2.Canny(image, 100, 200)
image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)

image = pipe(
prompt, negative_prompt=negative_prompt, controlnet_conditioning_scale=0.5, image=canny_image, guess_mode=True,
).images[0]
make_image_grid([original_image, canny_image, image], rows=1, cols=3)
```

<Tip>

ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ø³ÙŠÙ† (refiner model) Ù…Ø¹ `StableDiffusionXLControlNetPipeline` Ù„ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø©ØŒ ØªÙ…Ø§Ù…Ù‹Ø§ ÙƒÙ…Ø§ ØªÙØ¹Ù„ Ù…Ø¹ `StableDiffusionXLPipeline` Ø§Ù„Ø¹Ø§Ø¯ÙŠ.

Ø±Ø§Ø¬Ø¹ Ù‚Ø³Ù… [ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø©](./sdxl#refine-image-quality) Ù„Ù…Ø¹Ø±ÙØ© ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ­Ø³ÙŠÙ†.

ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… `StableDiffusionXLControlNetPipeline` ÙˆÙ…Ø±Ø± `image` Ùˆ`controlnet_conditioning_scale`.

```py
base = StableDiffusionXLControlNetPipeline(...)
image = base(
prompt=prompt,
controlnet_conditioning_scale=0.5,
image=canny_image,
num_inference_steps=40,
denoising_end=0.8,
output_type="latent",
).images
# Ø§Ù„Ø¨Ø§Ù‚ÙŠ ÙƒÙ…Ø§ Ù‡Ùˆ ØªÙ…Ø§Ù…Ù‹Ø§ Ù…Ø¹ StableDiffusionXLPipeline
```

</Tip>


## MultiControlNet 

ÙŠÙ…ÙƒÙ†Ùƒ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø¹Ù…Ù„ÙŠØ§Øª Ø¶Ø¨Ø· ControlNet Ù…Ù† Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ù„Ø¥Ù†Ø´Ø§Ø¡ *MultiControlNet*. ÙˆÙ„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„ØŒ Ù…Ù† Ø§Ù„Ù…ÙÙŠØ¯ ØºØ§Ù„Ø¨Ù‹Ø§:

1. Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø¶Ø¨Ø· Ø¨Ø­ÙŠØ« Ù„Ø§ ØªØªØ¯Ø§Ø®Ù„ (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù‚Ù†Ø§Ø¹ Ù…Ù†Ø·Ù‚Ø© ØµÙˆØ±Ø© Canny Ø­ÙŠØ« ÙŠÙ‚Ø¹ Ø¶Ø¨Ø· Ø§Ù„ÙˆØ¶Ø¹)
2. ØªØ¬Ø±Ø¨Ø© Ù…Ø¹ Ù…Ø¹Ù„Ù…Ø© [`controlnet_conditioning_scale`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/controlnet#diffusers.StableDiffusionControlNetPipeline.__call__.controlnet_conditioning_scale) Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¨ ØªØ¹ÙŠÙŠÙ†Ù‡ Ù„ÙƒÙ„ Ø¥Ø¯Ø®Ø§Ù„ Ø¶Ø¨Ø·

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø³ØªØ¬Ù…Ø¹ Ø¨ÙŠÙ† ØµÙˆØ±Ø© Canny ÙˆØµÙˆØ±Ø© ØªÙ‚Ø¯ÙŠØ± ÙˆØ¶Ø¹ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©.

Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¶Ø¨Ø· ØµÙˆØ±Ø© Canny:

```py
from diffusers.utils import load_image, make_image_grid
from PIL import Image
import numpy as np
import cv2

original_image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"
)
image = np.array(original_image)

low_threshold = 100
high_threshold = 200

image = cv2.Canny(image, low_threshold, high_threshold)

# zero out middle columns of image where pose will be overlaid
zero_start = image.shape[1] // 4
zero_end = zero_start + image.shape[1] // 2
image[:, zero_start:zero_end] = 0

image = image[:, :, None]
image = np.concatenate([image, image, image], axis=2)
canny_image = Image.fromarray(image)
make_image_grid([original_image, canny_image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/landscape.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/landscape_canny_masked.png"/>
<figcaption class="mt-ompi text-center text-sm text-gray-500">ØµÙˆØ±Ø© Canny</figcaption>
</div>
</div>

Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø´Ø±ÙŠØŒ Ù‚Ù… Ø¨ØªØ«Ø¨ÙŠØª [controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux):

```py
# uncomment to install the necessary library in Colab
#! pip install -q controlnet-aux
```

Ù‚Ù… Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¶Ø¨Ø· ØªÙ‚Ø¯ÙŠØ± Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø´Ø±ÙŠ:

```py
from controlnet_aux import OpenposeDetector

openpose = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
original_image = load_image(
"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png"
)
openpose_image = openpose(original_image)
make_image_grid([original_image, openpose_image], rows=1, cols=2)
```

<div class="flex gap-4">
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/person.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©</figcaption>
</div>
<div>
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/person_pose.png"/>
<figcaption class="mt-2 text-center text-sm text-gray-500">ØµÙˆØ±Ø© Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø¨Ø´Ø±ÙŠ</figcaption>
</div>
</div>

Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ù†Ù…Ø§Ø°Ø¬ ControlNet Ø§Ù„ØªÙŠ ØªØªÙˆØ§ÙÙ‚ Ù…Ø¹ ÙƒÙ„ Ø¶Ø¨Ø·ØŒ ÙˆÙ…Ø±Ø±Ù‡Ø§ Ø¥Ù„Ù‰ [`StableDiffusionXLControlNetPipeline`]. Ø§Ø³ØªØ®Ø¯Ù… [`UniPCMultistepScheduler`] Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆÙ‚Ù… Ø¨ØªÙ…ÙƒÙŠÙ† ØªÙØ±ÙŠØº Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©.

```py
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, AutoencoderKL, UniPCMultistepScheduler
import torch

controlnets = [
ControlNetModel.from_pretrained(
"thibaud/controlnet-openpose-sdxl-1.0", torch_dtype=torch.float16
),
ControlNetModel.from_pretrained(
"diffusers/controlnet-canny-sdxl-1.0", torch_dtype=torch.float16, use_safetensors=True
),
]

vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16, use_safetensors=True)
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
"stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnets, vae=vae, torch_dtype=torch.float16, use_safetensors=True
)
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
pipe.enable_model_cpu_offload()
```

Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ± Ù…Ø·Ø§Ù„Ø¨ØªÙƒ (Ù…Ø·Ø§Ù„Ø¨Ø© Ø³Ù„Ø¨ÙŠØ© Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… ÙˆØ§Ø­Ø¯Ø©)ØŒ ÙˆØµÙˆØ±Ø© CannyØŒ ÙˆØµÙˆØ±Ø© Ø§Ù„ÙˆØ¶Ø¹ Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¨ÙˆØ¨:

```py
prompt = "a giant standing in a fantasy landscape, best quality"
negative_prompt = "monochrome, lowres, bad anatomy, worst quality, low quality"

generator = torch.manual_seed(1)

images = [openpose_image.resize((1024, 1024)), canny_image.resize((1024, 1024))]

images = pipe(
prompt,
image=images,
num_inference_steps=25,
generator=generator,
negative_prompt=negative_prompt,
num_images_per_prompt=3,
controlnet_conditioning_scale=[1.0, 0.8],
).images
make_image_grid([original_image, canny_image, openpose_image,
images[0].resize((512, 512)), images[1].resize((512, 512)), images[2].resize((512, 512))], rows=2, cols=3)
```

<div class="flex justify-center">
<img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/multicontrolnet.png"/>
</div>