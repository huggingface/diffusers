<!--Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# InstructPix2Pix

[InstructPix2Pix](https://hf.co/papers/2211.09800) æ˜¯ä¸€ä¸ªåŸºäº Stable Diffusion è®­ç»ƒçš„æ¨¡å‹ï¼Œç”¨äºæ ¹æ®äººç±»æä¾›çš„æŒ‡ä»¤ç¼–è¾‘å›¾åƒã€‚ä¾‹å¦‚ï¼Œæ‚¨çš„æç¤ºå¯ä»¥æ˜¯â€œå°†äº‘å˜æˆé›¨å¤©â€ï¼Œæ¨¡å‹å°†ç›¸åº”ç¼–è¾‘è¾“å…¥å›¾åƒã€‚è¯¥æ¨¡å‹ä»¥æ–‡æœ¬æç¤ºï¼ˆæˆ–ç¼–è¾‘æŒ‡ä»¤ï¼‰å’Œè¾“å…¥å›¾åƒä¸ºæ¡ä»¶ã€‚

æœ¬æŒ‡å—å°†æ¢ç´¢ [train_instruct_pix2pix.py](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py) è®­ç»ƒè„šæœ¬ï¼Œå¸®åŠ©æ‚¨ç†Ÿæ‚‰å®ƒï¼Œä»¥åŠå¦‚ä½•å°†å…¶é€‚åº”æ‚¨è‡ªå·±çš„ç”¨ä¾‹ã€‚

åœ¨è¿è¡Œè„šæœ¬ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä»æºä»£ç å®‰è£…åº“ï¼š

```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

ç„¶åå¯¼èˆªåˆ°åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ï¼Œå¹¶å®‰è£…è„šæœ¬æ‰€éœ€çš„ä¾èµ–é¡¹ï¼š

```bash
cd examples/instruct_pix2pix
pip install -r requirements.txt
```

> [!TIP]
> ğŸ¤— Accelerate æ˜¯ä¸€ä¸ªåº“ï¼Œç”¨äºå¸®åŠ©æ‚¨åœ¨å¤šä¸ª GPU/TPU ä¸Šæˆ–ä½¿ç”¨æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒã€‚å®ƒå°†æ ¹æ®æ‚¨çš„ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®è®­ç»ƒè®¾ç½®ã€‚æŸ¥çœ‹ ğŸ¤— Accelerate [å¿«é€Ÿå¯¼è§ˆ](https://huggingface.co/docs/accelerate/quicktour) ä»¥äº†è§£æ›´å¤šä¿¡æ¯ã€‚

åˆå§‹åŒ–ä¸€ä¸ª ğŸ¤— Accelerate ç¯å¢ƒï¼š

```bash
accelerate config
```

è¦è®¾ç½®ä¸€ä¸ªé»˜è®¤çš„ ğŸ¤— Accelerate ç¯å¢ƒï¼Œæ— éœ€é€‰æ‹©ä»»ä½•é…ç½®ï¼š

```bash
accelerate config default
```

æˆ–è€…ï¼Œå¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼ shellï¼Œä¾‹å¦‚ç¬”è®°æœ¬ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ï¼š

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

æœ€åï¼Œå¦‚æœæ‚¨æƒ³åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹ [åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†](create_dataset) æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºä¸è®­ç»ƒè„šæœ¬å…¼å®¹çš„æ•°æ®é›†ã€‚

> [!TIP]
> ä»¥ä¸‹éƒ¨åˆ†é‡ç‚¹ä»‹ç»äº†è®­ç»ƒè„šæœ¬ä¸­å¯¹äºç†è§£å¦‚ä½•ä¿®æ”¹å®ƒå¾ˆé‡è¦çš„éƒ¨åˆ†ï¼Œä½†å¹¶æœªè¯¦ç»†æ¶µç›–è„šæœ¬çš„æ¯ä¸ªæ–¹é¢ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œè¯·éšæ—¶é˜…è¯» [è„šæœ¬](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix.py)ï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ã€‚

## è„šæœ¬å‚æ•°

è®­ç»ƒè„šæœ¬æœ‰è®¸å¤šå‚æ•°å¯å¸®åŠ©æ‚¨è‡ªå®šä¹‰è®­ç»ƒè¿è¡Œã€‚æ‰€æœ‰
å‚æ•°åŠå…¶æè¿°å¯åœ¨ [`parse_args()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L65) å‡½æ•°ä¸­æ‰¾åˆ°ã€‚å¤§å¤šæ•°å‚æ•°éƒ½æä¾›äº†é»˜è®¤å€¼ï¼Œè¿™äº›å€¼æ•ˆæœç›¸å½“ä¸é”™ï¼Œä½†å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥åœ¨è®­ç»ƒå‘½ä»¤ä¸­è®¾ç½®è‡ªå·±çš„å€¼ã€‚

ä¾‹å¦‚ï¼Œè¦å¢åŠ è¾“å…¥å›¾åƒçš„åˆ†è¾¨ç‡ï¼š

```bash
accelerate launch train_instruct_pix2pix.py \
  --resolution=512 \
```

è®¸å¤šåŸºæœ¬å’Œé‡è¦çš„å‚æ•°åœ¨ [æ–‡æœ¬åˆ°å›¾åƒ](text2image#script-parameters) è®­ç»ƒæŒ‡å—ä¸­å·²æœ‰æè¿°ï¼Œå› æ­¤æœ¬æŒ‡å—ä»…å…³æ³¨ä¸ InstructPix2Pix ç›¸å…³çš„å‚æ•°ï¼š

- `--original_image_column`ï¼šç¼–è¾‘å‰çš„åŸå§‹å›¾åƒ
- `--edited_image_column`ï¼šç¼–è¾‘åçš„å›¾åƒ
- `--edit_prompt_column`ï¼šç¼–è¾‘å›¾åƒçš„æŒ‡ä»¤
- `--conditioning_dropout_prob`ï¼šè®­ç»ƒæœŸé—´ç¼–è¾‘å›¾åƒå’Œç¼–è¾‘æç¤ºçš„ dropout æ¦‚ç‡ï¼Œè¿™ä¸ºä¸€ç§æˆ–ä¸¤ç§æ¡ä»¶è¾“å…¥å¯ç”¨äº†æ— åˆ†ç±»å™¨å¼•å¯¼ï¼ˆCFGï¼‰

## è®­ç»ƒè„šæœ¬

æ•°æ®é›†é¢„å¤„ç†ä»£ç å’Œè®­ç»ƒå¾ªç¯å¯åœ¨ [`main()`](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L374) å‡½æ•°ä¸­æ‰¾åˆ°ã€‚è¿™æ˜¯æ‚¨å°†ä¿®æ”¹è®­ç»ƒè„šæœ¬ä»¥é€‚åº”è‡ªå·±ç”¨ä¾‹çš„åœ°æ–¹ã€‚

ä¸è„šæœ¬å‚æ•°ç±»ä¼¼ï¼Œ[æ–‡æœ¬åˆ°å›¾åƒ](text2image#training-script) è®­ç»ƒæŒ‡å—æä¾›äº†è®­ç»ƒè„šæœ¬çš„é€æ­¥è¯´æ˜ã€‚ç›¸åï¼Œæœ¬æŒ‡å—å°†æŸ¥çœ‹è„šæœ¬ä¸­ä¸ InstructPix2Pix ç›¸å…³çš„éƒ¨åˆ†ã€‚

è„šæœ¬é¦–å…ˆä¿®æ”¹ UNet çš„ç¬¬ä¸€ä¸ªå·ç§¯å±‚ä¸­çš„ [è¾“å…¥é€šé“æ•°](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L445)ï¼Œä»¥é€‚åº” InstructPix2Pix çš„é¢å¤–æ¡ä»¶å›¾åƒï¼š

```py
in_channels = 8
out_channels = unet.conv_in.out_channels
unet.register_to_config(in_channels=in_channels)

with torch.no_grad():
    new_conv_in = nn.Conv2d(
        in_channels, out_channels, unet.conv_in.kernel_size, unet.conv_in.stride, unet.conv_in.padding
    )
    new_conv_in.weight.zero_()
    new_conv_in.weight[:, :4, :, :].copy_(unet.conv_in.weight)
    unet.conv_in = new_conv_in
```

è¿™äº› UNet å‚æ•°ç”±ä¼˜åŒ–å™¨ [æ›´æ–°](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L545C1-L551C6)ï¼š

```py
optimizer = optimizer_cls(
    unet.parameters(),
    lr=args.learning_rate,
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

æ¥ä¸‹æ¥ï¼Œç¼–è¾‘åçš„å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤è¢« [é¢„å¤„ç†](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L624)å¹¶è¢«[tokenized](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L610C24-L610C24)ã€‚é‡è¦çš„æ˜¯ï¼Œå¯¹åŸå§‹å›¾åƒå’Œç¼–è¾‘åçš„å›¾åƒåº”ç”¨ç›¸åŒçš„å›¾åƒå˜æ¢ã€‚

```py
def preprocess_train(examples):
    preprocessed_images = preprocess_images(examples)

    original_images, edited_images = preprocessed_images.chunk(2)
    original_images = original_images.reshape(-1, 3, args.resolution, args.resolution)
    edited_images = edited_images.reshape(-1, 3, args.resolution, args.resolution)

    examples["original_pixel_values"] = original_images
    examples["edited_pixel_values"] = edited_images

    captions = list(examples[edit_prompt_column])
    examples["input_ids"] = tokenize_captions(captions)
    return examples
```

æœ€åï¼Œåœ¨[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/64603389da01082055a901f2883c4810d1144edb/examples/instruct_pix2pix/train_instruct_pix2pix.py#L730)ä¸­ï¼Œå®ƒé¦–å…ˆå°†ç¼–è¾‘åçš„å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´ï¼š

```py
latents = vae.encode(batch["edited_pixel_values"].to(weight_dtype)).latent_dist.sample()
latents = latents * vae.config.scaling_factor
```

ç„¶åï¼Œè„šæœ¬å¯¹åŸå§‹å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤åµŒå…¥åº”ç”¨ dropout ä»¥æ”¯æŒ CFGï¼ˆClassifier-Free Guidanceï¼‰ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè°ƒèŠ‚ç¼–è¾‘æŒ‡ä»¤å’ŒåŸå§‹å›¾åƒå¯¹ç¼–è¾‘åå›¾åƒçš„å½±å“ã€‚

```py
encoder_hidden_states = text_encoder(batch["input_ids"])[0]
original_image_embeds = vae.encode(batch["original_pixel_values"].to(weight_dtype)).latent_dist.mode()

if args.conditioning_dropout_prob is not None:
    random_p = torch.rand(bsz, device=latents.device, generator=generator)
    prompt_mask = random_p < 2 * args.conditioning_dropout_prob
    prompt_mask = prompt_mask.reshape(bsz, 1, 1)
    null_conditioning = text_encoder(tokenize_captions([""]).to(accelerator.device))[0]
    encoder_hidden_states = torch.where(prompt_mask, null_conditioning, encoder_hidden_states)

    image_mask_dtype = original_image_embeds.dtype
    image_mask = 1 - (
        (random_p >= args.conditioning_dropout_prob).to(image_mask_dtype)
        * (random_p < 3 * args.conditioning_dropout_prob).to(image_mask_dtype)
    )
    image_mask = image_mask.reshape(bsz, 1, 1, 1)
    original_image_embeds = image_mask * original_image_embeds
```

å·®ä¸å¤šå°±æ˜¯è¿™æ ·äº†ï¼é™¤äº†è¿™é‡Œæè¿°çš„ä¸åŒä¹‹å¤„ï¼Œè„šæœ¬çš„å…¶ä½™éƒ¨åˆ†ä¸[æ–‡æœ¬åˆ°å›¾åƒ](text2image#training-script)è®­ç»ƒè„šæœ¬éå¸¸ç›¸ä¼¼ï¼Œæ‰€ä»¥è¯·éšæ„æŸ¥çœ‹ä»¥è·å–æ›´å¤šç»†èŠ‚ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šå…³äºè®­ç»ƒå¾ªç¯å¦‚ä½•å·¥ä½œçš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹[ç†è§£ç®¡é“ã€æ¨¡å‹å’Œè°ƒåº¦å™¨](../using-diffusers/write_own_pipeline)æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹åˆ†è§£äº†å»å™ªè¿‡ç¨‹çš„åŸºæœ¬æ¨¡å¼ã€‚

## å¯åŠ¨è„šæœ¬

ä¸€æ—¦æ‚¨å¯¹è„šæœ¬çš„æ›´æ”¹æ„Ÿåˆ°æ»¡æ„ï¼Œæˆ–è€…å¦‚æœæ‚¨å¯¹é»˜è®¤é…ç½®æ²¡é—®é¢˜ï¼Œæ‚¨
å‡†å¤‡å¥½å¯åŠ¨è®­ç»ƒè„šæœ¬ï¼ğŸš€

æœ¬æŒ‡å—ä½¿ç”¨ [fusing/instructpix2pix-1000-samples](https://huggingface.co/datasets/fusing/instructpix2pix-1000-samples) æ•°æ®é›†ï¼Œè¿™æ˜¯ [åŸå§‹æ•°æ®é›†](https://huggingface.co/datasets/timbrooks/instructpix2pix-clip-filtered) çš„ä¸€ä¸ªè¾ƒå°ç‰ˆæœ¬ã€‚æ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºå¹¶ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ï¼ˆè¯·å‚é˜… [åˆ›å»ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†](create_dataset) æŒ‡å—ï¼‰ã€‚

å°† `MODEL_NAME` ç¯å¢ƒå˜é‡è®¾ç½®ä¸ºæ¨¡å‹åç§°ï¼ˆå¯ä»¥æ˜¯ Hub ä¸Šçš„æ¨¡å‹ ID æˆ–æœ¬åœ°æ¨¡å‹çš„è·¯å¾„ï¼‰ï¼Œå¹¶å°† `DATASET_ID` è®¾ç½®ä¸º Hub ä¸Šæ•°æ®é›†çš„åç§°ã€‚è„šæœ¬ä¼šåˆ›å»ºå¹¶ä¿å­˜æ‰€æœ‰ç»„ä»¶ï¼ˆç‰¹å¾æå–å™¨ã€è°ƒåº¦å™¨ã€æ–‡æœ¬ç¼–ç å™¨ã€UNet ç­‰ï¼‰åˆ°æ‚¨çš„ä»“åº“ä¸­çš„ä¸€ä¸ªå­æ–‡ä»¶å¤¹ã€‚

> [!TIP]
> ä¸ºäº†è·å¾—æ›´å¥½çš„ç»“æœï¼Œå°è¯•ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†è¿›è¡Œæ›´é•¿æ—¶é—´çš„è®­ç»ƒã€‚æˆ‘ä»¬åªåœ¨è¾ƒå°è§„æ¨¡çš„æ•°æ®é›†ä¸Šæµ‹è¯•è¿‡æ­¤è®­ç»ƒè„šæœ¬ã€‚
>
> <br>
>
> è¦ä½¿ç”¨ Weights and Biases ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Œè¯·å°† `--report_to=wandb` å‚æ•°æ·»åŠ åˆ°è®­ç»ƒå‘½ä»¤ä¸­ï¼Œå¹¶ä½¿ç”¨ `--val_image_url` æŒ‡å®šéªŒè¯å›¾åƒï¼Œä½¿ç”¨ `--validation_prompt` æŒ‡å®šéªŒè¯æç¤ºã€‚è¿™å¯¹äºè°ƒè¯•æ¨¡å‹éå¸¸æœ‰ç”¨ã€‚

å¦‚æœæ‚¨åœ¨å¤šä¸ª GPU ä¸Šè®­ç»ƒï¼Œè¯·å°† `--multi_gpu` å‚æ•°æ·»åŠ åˆ° `accelerate launch` å‘½ä»¤ä¸­ã€‚

```bash
accelerate launch --mixed_precision="fp16" train_instruct_pix2pix.py \
    --pretrained_model_name_or_path=$MODEL_NAME \
    --dataset_name=$DATASET_ID \
    --enable_xformers_memory_efficient_attention \
    --resolution=256 \
    --random_flip \
    --train_batch_size=4 \
    --gradient_accumulation_steps=4 \
    --gradient_checkpointing \
    --max_train_steps=15000 \
    --checkpointing_steps=5000 \
    --checkpoints_total_limit=1 \
    --learning_rate=5e-05 \
    --max_grad_norm=1 \
    --lr_warmup_steps=0 \
    --conditioning_dropout_prob=0.05 \
    --mixed_precision=fp16 \
    --seed=42 \
    --push_to_hub
```

è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ‚¨çš„æ–° InstructPix2Pix è¿›è¡Œæ¨ç†ï¼š

```py
import PIL
import requests
import torch
from diffusers import StableDiffusionInstructPix2PixPipeline
from diffusers.utils import load_image

pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained("your_cool_model", torch_dtype=torch.float16).to("cuda")
generator = torch.Generator("cuda").manual_seed(0)

image = load_image("https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png")
prompt = "add some ducks to the lake"
num_inference_steps = 20
image_guidance_scale = 1.5
guidance_scale = 10

edited_image = pipeline(
   prompt,
   image=image,
   num_inference_steps=num_inference_steps,
   image_guidance_scale=image_guidance_scale,
   guidance_scale=guidance_scale,
   generator=generator,
).images[0]
edited_image.save("edited_image.png")
```

æ‚¨åº”è¯¥å°è¯•ä¸åŒçš„ `num_inference_steps`ã€`image_guidance_scale` å’Œ `guidance_scale` å€¼ï¼Œä»¥æŸ¥çœ‹å®ƒä»¬å¦‚ä½•å½±å“æ¨ç†é€Ÿåº¦å’Œè´¨é‡ã€‚æŒ‡å¯¼æ¯”ä¾‹å‚æ•°
è¿™äº›å‚æ•°å°¤å…¶é‡è¦ï¼Œå› ä¸ºå®ƒä»¬æ§åˆ¶åŸå§‹å›¾åƒå’Œç¼–è¾‘æŒ‡ä»¤å¯¹ç¼–è¾‘åå›¾åƒçš„å½±å“ç¨‹åº¦ã€‚

## Stable Diffusion XL

Stable Diffusion XL (SDXL) æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒï¼Œå¹¶åœ¨å…¶æ¶æ„ä¸­æ·»åŠ äº†ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨ã€‚ä½¿ç”¨ [`train_instruct_pix2pix_sdxl.py`](https://github.com/huggingface/diffusers/blob/main/examples/instruct_pix2pix/train_instruct_pix2pix_sdxl.py) è„šæœ¬æ¥è®­ç»ƒ SDXL æ¨¡å‹ä»¥éµå¾ªå›¾åƒç¼–è¾‘æŒ‡ä»¤ã€‚

SDXL è®­ç»ƒè„šæœ¬åœ¨ [SDXL è®­ç»ƒ](sdxl) æŒ‡å—ä¸­æœ‰æ›´è¯¦ç»†çš„è®¨è®ºã€‚

## åç»­æ­¥éª¤

æ­å–œæ‚¨è®­ç»ƒäº†è‡ªå·±çš„ InstructPix2Pix æ¨¡å‹ï¼ğŸ¥³ è¦äº†è§£æ›´å¤šå…³äºè¯¥æ¨¡å‹çš„ä¿¡æ¯ï¼Œå¯èƒ½æœ‰åŠ©äºï¼š

- é˜…è¯» [Instruction-tuning Stable Diffusion with InstructPix2Pix](https://huggingface.co/blog/instruction-tuning-sd) åšå®¢æ–‡ç« ï¼Œäº†è§£æ›´å¤šæˆ‘ä»¬ä½¿ç”¨ InstructPix2Pix è¿›è¡Œçš„ä¸€äº›å®éªŒã€æ•°æ®é›†å‡†å¤‡ä»¥åŠä¸åŒæŒ‡ä»¤çš„ç»“æœã€‚