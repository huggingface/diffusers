<!--ç‰ˆæƒå£°æ˜ 2025 ç”± HuggingFace å›¢é˜Ÿæ‰€æœ‰ã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆï¼ˆ"è®¸å¯è¯"ï¼‰æˆæƒï¼›é™¤éç¬¦åˆè®¸å¯è¯è¦æ±‚ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æœ¬æ–‡ä»¶ã€‚
æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæœ¬è½¯ä»¶æŒ‰"åŸæ ·"åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯¦è§è®¸å¯è¯ä¸­è§„å®šçš„ç‰¹å®šè¯­è¨€æƒé™å’Œé™åˆ¶ã€‚
-->

# æ–‡æœ¬åè½¬ï¼ˆTextual Inversionï¼‰

[æ–‡æœ¬åè½¬](https://hf.co/papers/2208.01618)æ˜¯ä¸€ç§è®­ç»ƒæŠ€æœ¯ï¼Œä»…éœ€å°‘é‡ç¤ºä¾‹å›¾åƒå³å¯ä¸ªæ€§åŒ–å›¾åƒç”Ÿæˆæ¨¡å‹ã€‚è¯¥æŠ€æœ¯é€šè¿‡å­¦ä¹ å’Œæ›´æ–°æ–‡æœ¬åµŒå…¥ï¼ˆæ–°åµŒå…¥ä¼šç»‘å®šåˆ°æç¤ºä¸­å¿…é¡»ä½¿ç”¨çš„ç‰¹æ®Šè¯æ±‡ï¼‰æ¥åŒ¹é…æ‚¨æä¾›çš„ç¤ºä¾‹å›¾åƒã€‚

å¦‚æœåœ¨æ˜¾å­˜æœ‰é™çš„GPUä¸Šè®­ç»ƒï¼Œå»ºè®®åœ¨è®­ç»ƒå‘½ä»¤ä¸­å¯ç”¨`gradient_checkpointing`å’Œ`mixed_precision`å‚æ•°ã€‚æ‚¨è¿˜å¯ä»¥é€šè¿‡[xFormers](../optimization/xformers)ä½¿ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶æ¥å‡å°‘å†…å­˜å ç”¨ã€‚JAX/Flaxè®­ç»ƒä¹Ÿæ”¯æŒåœ¨TPUå’ŒGPUä¸Šè¿›è¡Œé«˜æ•ˆè®­ç»ƒï¼Œä½†ä¸æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹æˆ–xFormersã€‚åœ¨é…ç½®ä¸PyTorchç›¸åŒçš„æƒ…å†µä¸‹ï¼ŒFlaxè®­ç»ƒè„šæœ¬çš„é€Ÿåº¦è‡³å°‘åº”å¿«70%ï¼

æœ¬æŒ‡å—å°†æ¢ç´¢[textual_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/textual_inversion.py)è„šæœ¬ï¼Œå¸®åŠ©æ‚¨æ›´ç†Ÿæ‚‰å…¶å·¥ä½œåŸç†ï¼Œå¹¶äº†è§£å¦‚ä½•æ ¹æ®è‡ªèº«éœ€æ±‚è¿›è¡Œè°ƒæ•´ã€‚

è¿è¡Œè„šæœ¬å‰ï¼Œè¯·ç¡®ä¿ä»æºç å®‰è£…åº“ï¼š

```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

è¿›å…¥åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹ç›®å½•ï¼Œå¹¶å®‰è£…æ‰€éœ€ä¾èµ–ï¼š

<hfoptions id="installation">
<hfoption id="PyTorch">

```bash
cd examples/textual_inversion
pip install -r requirements.txt
```

</hfoption>
<hfoption id="Flax">

```bash
cd examples/textual_inversion
pip install -r requirements_flax.txt
```

</hfoption>
</hfoptions>

> [!TIP]
> ğŸ¤— Accelerate æ˜¯ä¸€ä¸ªå¸®åŠ©æ‚¨åœ¨å¤šGPU/TPUæˆ–æ··åˆç²¾åº¦ç¯å¢ƒä¸‹è®­ç»ƒçš„å·¥å…·åº“ã€‚å®ƒä¼šæ ¹æ®ç¡¬ä»¶å’Œç¯å¢ƒè‡ªåŠ¨é…ç½®è®­ç»ƒè®¾ç½®ã€‚æŸ¥çœ‹ğŸ¤— Accelerate [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/accelerate/quicktour)äº†è§£æ›´å¤šã€‚

åˆå§‹åŒ–ğŸ¤— Accelerateç¯å¢ƒï¼š

```bash
accelerate config
```

è¦è®¾ç½®é»˜è®¤çš„ğŸ¤— Accelerateç¯å¢ƒï¼ˆä¸é€‰æ‹©ä»»ä½•é…ç½®ï¼‰ï¼š

```bash
accelerate config default
```

å¦‚æœæ‚¨çš„ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼shellï¼ˆå¦‚notebookï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ï¼š

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

æœ€åï¼Œå¦‚æœæƒ³åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè¯·å‚é˜…[åˆ›å»ºè®­ç»ƒæ•°æ®é›†](create_dataset)æŒ‡å—ï¼Œäº†è§£å¦‚ä½•åˆ›å»ºé€‚ç”¨äºè®­ç»ƒè„šæœ¬çš„æ•°æ®é›†ã€‚

> [!TIP]
> ä»¥ä¸‹éƒ¨åˆ†é‡ç‚¹ä»‹ç»è®­ç»ƒè„šæœ¬ä¸­éœ€è¦ç†è§£çš„å…³é”®ä¿®æ”¹ç‚¹ï¼Œä½†æœªæ¶µç›–è„šæœ¬æ‰€æœ‰ç»†èŠ‚ã€‚å¦‚éœ€æ·±å…¥äº†è§£ï¼Œå¯éšæ—¶æŸ¥é˜…[è„šæœ¬æºç ](https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/textual_inversion.py)ï¼Œå¦‚æœ‰ç–‘é—®æ¬¢è¿åé¦ˆã€‚

## è„šæœ¬å‚æ•°

è®­ç»ƒè„šæœ¬åŒ…å«ä¼—å¤šå‚æ•°ï¼Œä¾¿äºæ‚¨å®šåˆ¶è®­ç»ƒè¿‡ç¨‹ã€‚æ‰€æœ‰å‚æ•°åŠå…¶è¯´æ˜éƒ½åˆ—åœ¨[`parse_args()`](https://github.com/huggingface/diffusers/blob/839c2a5ece0af4e75530cb520d77bc7ed8acf474/examples/textual_inversion/textual_inversion.py#L176)å‡½æ•°ä¸­ã€‚Diffusersä¸ºæ¯ä¸ªå‚æ•°æä¾›äº†é»˜è®¤å€¼ï¼ˆå¦‚è®­ç»ƒæ‰¹æ¬¡å¤§å°å’Œå­¦ä¹ ç‡ï¼‰ï¼Œä½†æ‚¨å¯ä»¥é€šè¿‡è®­ç»ƒå‘½ä»¤è‡ªç”±è°ƒæ•´è¿™äº›å€¼ã€‚

ä¾‹å¦‚ï¼Œå°†æ¢¯åº¦ç´¯ç§¯æ­¥æ•°å¢åŠ åˆ°é»˜è®¤å€¼1ä»¥ä¸Šï¼š

```bash
accelerate launch textual_inversion.py \
  --gradient_accumulation_steps=4
```

å…¶ä»–éœ€è¦æŒ‡å®šçš„åŸºç¡€é‡è¦å‚æ•°åŒ…æ‹¬ï¼š

- `--pretrained_model_name_or_path`ï¼šHubä¸Šçš„æ¨¡å‹åç§°æˆ–æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
- `--train_data_dir`ï¼šåŒ…å«è®­ç»ƒæ•°æ®é›†ï¼ˆç¤ºä¾‹å›¾åƒï¼‰çš„æ–‡ä»¶å¤¹è·¯å¾„
- `--output_dir`ï¼šè®­ç»ƒæ¨¡å‹ä¿å­˜ä½ç½®
- `--push_to_hub`ï¼šæ˜¯å¦å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€è‡³Hub
- `--checkpointing_steps`ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜æ£€æŸ¥ç‚¹çš„é¢‘ç‡ï¼›è‹¥è®­ç»ƒæ„å¤–ä¸­æ–­ï¼Œå¯é€šè¿‡åœ¨å‘½ä»¤ä¸­æ·»åŠ `--resume_from_checkpoint`ä»è¯¥æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
- `--num_vectors`ï¼šå­¦ä¹ åµŒå…¥çš„å‘é‡æ•°é‡ï¼›å¢åŠ æ­¤å‚æ•°å¯æå‡æ¨¡å‹æ•ˆæœï¼Œä½†ä¼šæé«˜è®­ç»ƒæˆæœ¬
- `--placeholder_token`ï¼šç»‘å®šå­¦ä¹ åµŒå…¥çš„ç‰¹æ®Šè¯æ±‡ï¼ˆæ¨ç†æ—¶éœ€åœ¨æç¤ºä¸­ä½¿ç”¨è¯¥è¯ï¼‰
- `--initializer_token`ï¼šå¤§è‡´æè¿°è®­ç»ƒç›®æ ‡çš„å•å­—è¯æ±‡ï¼ˆå¦‚ç‰©ä½“æˆ–é£æ ¼ï¼‰
- `--learnable_property`ï¼šè®­ç»ƒç›®æ ‡æ˜¯å­¦ä¹ æ–°"é£æ ¼"ï¼ˆå¦‚æ¢µé«˜ç”»é£ï¼‰è¿˜æ˜¯"ç‰©ä½“"ï¼ˆå¦‚æ‚¨çš„å® ç‰©ç‹—ï¼‰

## è®­ç»ƒè„šæœ¬

ä¸å…¶ä»–è®­ç»ƒè„šæœ¬ä¸åŒï¼Œtextual_inversion.pyåŒ…å«è‡ªå®šä¹‰æ•°æ®é›†ç±»[`TextualInversionDataset`](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L487)ï¼Œç”¨äºåˆ›å»ºæ•°æ®é›†ã€‚æ‚¨å¯ä»¥è‡ªå®šä¹‰å›¾åƒå°ºå¯¸ã€å ä½ç¬¦è¯æ±‡ã€æ’å€¼æ–¹æ³•ã€æ˜¯å¦è£å‰ªå›¾åƒç­‰ã€‚å¦‚éœ€ä¿®æ”¹æ•°æ®é›†åˆ›å»ºæ–¹å¼ï¼Œå¯è°ƒæ•´`TextualInversionDataset`ç±»ã€‚

æ¥ä¸‹æ¥ï¼Œåœ¨[`main()`](https://github.com/huggingface/diffusers/blob/839c2a5ece0af4e75530cb520d77bc7ed8acf474/examples/textual_inversion/textual_inversion.py#L573)å‡½æ•°ä¸­å¯æ‰¾åˆ°æ•°æ®é›†é¢„å¤„ç†ä»£ç å’Œè®­ç»ƒå¾ªç¯ã€‚

è„šæœ¬é¦–å…ˆåŠ è½½[tokenizer](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L616)ã€[schedulerå’Œæ¨¡å‹](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L622)ï¼š

```py
# åŠ è½½tokenizer
if args.tokenizer_name:
    tokenizer = CLIPTokenizer.from_pretrained(args.tokenizer_name)
elif args.pretrained_model_name_or_path:
    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer")

# åŠ è½½schedulerå’Œæ¨¡å‹
noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
text_encoder = CLIPTextModel.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision
)
vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision)
unet = UNet2DConditionModel.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="unet", revision=args.revision
)
```

éšåå°†ç‰¹æ®Š[å ä½ç¬¦è¯æ±‡](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L632)åŠ å…¥tokenizerï¼Œå¹¶è°ƒæ•´åµŒå…¥å±‚ä»¥é€‚é…æ–°è¯æ±‡ã€‚

æ¥ç€ï¼Œè„šæœ¬é€šè¿‡`TextualInversionDataset`[åˆ›å»ºæ•°æ®é›†](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L716)ï¼š

```py
train_dataset = TextualInversionDataset(
    data_root=args.train_data_dir,
    tokenizer=tokenizer,
    size=args.resolution,
    placeholder_token=(" ".join(tokenizer.convert_ids_to_tokens(placeholder_token_ids))),
    repeats=args.repeats,
    learnable_property=args.learnable_property,
    center_crop=args.center_crop,
    set="train",
)
train_dataloader = torch.utils.data.DataLoader(
    train_dataset, batch_size=args.train_batch_size, shuffle=True, num_workers=args.dataloader_num_workers
)
```

æœ€åï¼Œ[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/b81c69e489aad3a0ba73798c459a33990dc4379c/examples/textual_inversion/textual_inversion.py#L784)å¤„ç†ä»é¢„æµ‹å™ªå£°æ®‹å·®åˆ°æ›´æ–°ç‰¹æ®Šå ä½ç¬¦è¯æ±‡åµŒå…¥æƒé‡çš„æ‰€æœ‰æµç¨‹ã€‚

å¦‚éœ€æ·±å…¥äº†è§£è®­ç»ƒå¾ªç¯å·¥ä½œåŸç†ï¼Œè¯·å‚é˜…[ç†è§£ç®¡é“ã€æ¨¡å‹ä¸è°ƒåº¦å™¨](../using-diffusers/write_own_pipeline)æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹è§£æäº†å»å™ªè¿‡ç¨‹çš„åŸºæœ¬æ¨¡å¼ã€‚

## å¯åŠ¨è„šæœ¬

å®Œæˆæ‰€æœ‰ä¿®æ”¹æˆ–ç¡®è®¤é»˜è®¤é…ç½®åï¼Œå³å¯å¯åŠ¨è®­ç»ƒè„šæœ¬ï¼ğŸš€

æœ¬æŒ‡å—å°†ä¸‹è½½[çŒ«ç©å…·](https://huggingface.co/datasets/diffusers/cat_toy_example)çš„ç¤ºä¾‹å›¾åƒå¹¶å­˜å‚¨åœ¨ç›®å½•ä¸­ã€‚å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥åˆ›å»ºå’Œä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†ï¼ˆå‚è§[åˆ›å»ºè®­ç»ƒæ•°æ®é›†](create_dataset)æŒ‡å—ï¼‰ã€‚

```py
from huggingface_hub import snapshot_download

local_dir = "./cat"
snapshot_download(
    "diffusers/cat_toy_example", local_dir=local_dir, repo_type="dataset", ignore_patterns=".gitattributes"
)
```

è®¾ç½®ç¯å¢ƒå˜é‡`MODEL_NAME`ä¸ºHubä¸Šçš„æ¨¡å‹IDæˆ–æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œ`DATA_DIR`ä¸ºåˆšä¸‹è½½çš„çŒ«å›¾åƒè·¯å¾„ã€‚è„šæœ¬ä¼šå°†ä»¥ä¸‹æ–‡ä»¶ä¿å­˜è‡³æ‚¨çš„ä»“åº“ï¼š

- `learned_embeds.bin`ï¼šä¸ç¤ºä¾‹å›¾åƒå¯¹åº”çš„å­¦ä¹ åµŒå…¥å‘é‡
- `token_identifier.txt`ï¼šç‰¹æ®Šå ä½ç¬¦è¯æ±‡
- `type_of_concept.txt`ï¼šè®­ç»ƒæ¦‚å¿µç±»å‹ï¼ˆ"object"æˆ–"style"ï¼‰

> [!WARNING]
> åœ¨å•å—V100 GPUä¸Šå®Œæ•´è®­ç»ƒçº¦éœ€1å°æ—¶ã€‚

å¯åŠ¨è„šæœ¬å‰è¿˜æœ‰æœ€åä¸€æ­¥ã€‚å¦‚æœæƒ³å®æ—¶è§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œå¯ä»¥å®šæœŸä¿å­˜ç”Ÿæˆå›¾åƒã€‚åœ¨è®­ç»ƒå‘½ä»¤ä¸­æ·»åŠ ä»¥ä¸‹å‚æ•°ï¼š

```bash
--validation_prompt="A <cat-toy> train"
--num_validation_images=4
--validation_steps=100
```

<hfoptions id="training-inference">
<hfoption id="PyTorch">

```bash
export MODEL_NAME="stable-diffusion-v1-5/stable-diffusion-v1-5"
export DATA_DIR="./cat"

accelerate launch textual_inversion.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --train_data_dir=$DATA_DIR \
  --learnable_property="object" \
  --placeholder_token="<cat-toy>" \
  --initializer_token="toy" \
  --resolution=512 \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --max_train_steps=3000 \
  --learning_rate=5.0e-04 \
  --scale_lr \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --output_dir="textual_inversion_cat" \
  --push_to_hub
```

</hfoption>
<hfoption id="Flax">

```bash
export MODEL_NAME="duongna/stable-diffusion-v1-4-flax"
export DATA_DIR="./cat"

python textual_inversion_flax.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --train_data_dir=$DATA_DIR \
  --learnable_property="object" \
  --placeholder_token="<cat-toy>" \
  --initializer_token="toy" \
  --resolution=512 \
  --train_batch_size=1 \
  --max_train_steps=3000 \
  --learning_rate=5.0e-04 \
  --scale_lr \
  --output_dir="textual_inversion_cat" \
  --push_to_hub
```

</hfoption>
</hfoptions>

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥åƒè¿™æ ·ä½¿ç”¨æ–°æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

<hfoptions id="training-inference">
<hfoption id="PyTorch">

```py
from diffusers import StableDiffusionPipeline
import torch

pipeline = StableDiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16).to("cuda")
pipeline.load_textual_inversion("sd-concepts-library/cat-toy")
image = pipeline("A <cat-toy> train", num_inference_steps=50).images[0]
image.save("cat-train.png")
```

</hfoption>
<hfoption id="Flax">

Flaxä¸æ”¯æŒ[`~loaders.TextualInversionLoaderMixin.load_textual_inversion`]æ–¹æ³•ï¼Œä½†textual_inversion_flax.pyè„šæœ¬ä¼šåœ¨è®­ç»ƒå[ä¿å­˜](https://github.com/huggingface/diffusers/blob/c0f058265161178f2a88849e92b37ffdc81f1dcc/examples/textual_inversion/textual_inversion_flax.py#L636C2-L636C2)å­¦ä¹ åˆ°çš„åµŒå…¥ä½œä¸ºæ¨¡å‹çš„ä¸€éƒ¨åˆ†ã€‚è¿™æ„å‘³ç€æ‚¨å¯ä»¥åƒä½¿ç”¨å…¶ä»–Flaxæ¨¡å‹ä¸€æ ·è¿›è¡Œæ¨ç†ï¼š

```py
import jax
import numpy as np
from flax.jax_utils import replicate
from flax.training.common_utils import shard
from diffusers import FlaxStableDiffusionPipeline

model_path = "path-to-your-trained-model"
pipeline, params = FlaxStableDiffusionPipeline.from_pretrained(model_path, dtype=jax.numpy.bfloat16)

prompt = "A <cat-toy> train"
prng_seed = jax.random.PRNGKey(0)
num_inference_steps = 50

num_samples = jax.device_count()
prompt = num_samples * [prompt]
prompt_ids = pipeline.prepare_inputs(prompt)

# åˆ†ç‰‡è¾“å…¥å’Œéšæœºæ•°ç”Ÿæˆå™¨
params = replicate(params)
prng_seed = jax.random.split(prng_seed, jax.device_count())
prompt_ids = shard(prompt_ids)

images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images
images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))
image.save("cat-train.png")
```

</hfoption>
</hfoptions>

## åç»­æ­¥éª¤

æ­å–œæ‚¨æˆåŠŸè®­ç»ƒäº†è‡ªå·±çš„æ–‡æœ¬åè½¬æ¨¡å‹ï¼ğŸ‰ å¦‚éœ€äº†è§£æ›´å¤šä½¿ç”¨æŠ€å·§ï¼Œä»¥ä¸‹æŒ‡å—å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼š

- å­¦ä¹ å¦‚ä½•[åŠ è½½æ–‡æœ¬åè½¬åµŒå…¥](../using-diffusers/loading_adapters)ï¼Œå¹¶å°†å…¶ç”¨ä½œè´Ÿé¢åµŒå…¥
- å­¦ä¹ å¦‚ä½•å°†[æ–‡æœ¬åè½¬](textual_inversion_inference)åº”ç”¨äºStable Diffusion 1/2å’ŒStable Diffusion XLçš„æ¨ç†
