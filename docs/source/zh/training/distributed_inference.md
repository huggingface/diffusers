<!--ç‰ˆæƒæ‰€æœ‰ 2025 The HuggingFace Teamã€‚ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

æ ¹æ® Apache è®¸å¯è¯ 2.0 ç‰ˆæœ¬ï¼ˆâ€œè®¸å¯è¯â€ï¼‰æˆæƒï¼›é™¤ééµå®ˆè®¸å¯è¯ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æ­¤æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæ ¹æ®è®¸å¯è¯åˆ†å‘çš„è½¯ä»¶æŒ‰â€œåŸæ ·â€åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯·å‚é˜…è®¸å¯è¯äº†è§£å…·ä½“çš„è¯­è¨€ç®¡ç†æƒé™å’Œé™åˆ¶ã€‚
-->

# åˆ†å¸ƒå¼æ¨ç†

åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) æˆ– [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html) åœ¨å¤šä¸ª GPU ä¸Šè¿è¡Œæ¨ç†ï¼Œè¿™å¯¹äºå¹¶è¡Œç”Ÿæˆå¤šä¸ªæç¤ºéå¸¸æœ‰ç”¨ã€‚

æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate å’Œ PyTorch Distributed è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ã€‚

## ğŸ¤— Accelerate

ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) æ˜¯ä¸€ä¸ªæ—¨åœ¨ç®€åŒ–åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­è®­ç»ƒæˆ–è¿è¡Œæ¨ç†çš„åº“ã€‚å®ƒç®€åŒ–äº†è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒçš„è¿‡ç¨‹ï¼Œè®©æ‚¨å¯ä»¥ä¸“æ³¨äºæ‚¨çš„ PyTorch ä»£ç ã€‚

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª Python æ–‡ä»¶å¹¶åˆå§‹åŒ–ä¸€ä¸ª [`accelerate.PartialState`] æ¥åˆ›å»ºåˆ†å¸ƒå¼ç¯å¢ƒï¼›æ‚¨çš„è®¾ç½®ä¼šè‡ªåŠ¨æ£€æµ‹ï¼Œå› æ­¤æ‚¨æ— éœ€æ˜ç¡®å®šä¹‰ `rank` æˆ– `world_size`ã€‚å°† [`DiffusionPipeline`] ç§»åŠ¨åˆ° `distributed_state.device` ä»¥ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ª GPUã€‚

ç°åœ¨ä½¿ç”¨ [`~accelerate.PartialState.split_between_processes`] å®ç”¨ç¨‹åºä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨åœ¨è¿›ç¨‹æ•°ä¹‹é—´åˆ†å‘æç¤ºã€‚

```py
import torch
from accelerate import PartialState
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True
)
distributed_state = PartialState()
pipeline.to(distributed_state.device)

with distributed_state.split_between_processes(["a dog", "a cat"]) as prompt:
    result = pipeline(prompt).images[0]
    result.save(f"result_{distributed_state.process_index}.png")
```

ä½¿ç”¨ `--num_processes` å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„ GPU æ•°é‡ï¼Œå¹¶è°ƒç”¨ `accelerate launch` æ¥è¿è¡Œè„šæœ¬ï¼š

```bash
accelerate launch run_distributed.py --num_processes=2
```

> [!TIP]
> å‚è€ƒè¿™ä¸ªæœ€å°ç¤ºä¾‹ [è„šæœ¬](https://gist.github.com/sayakpaul/cfaebd221820d7b43fae638b4dfa01ba) ä»¥åœ¨å¤šä¸ª GPU ä¸Šè¿è¡Œæ¨ç†ã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [ä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate) æŒ‡å—ã€‚

## PyTorch Distributed

PyTorch æ”¯æŒ [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)ï¼Œå®ƒå¯ç”¨äº†æ•°æ®
å¹¶è¡Œæ€§ã€‚

é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª Python æ–‡ä»¶å¹¶å¯¼å…¥ `torch.distributed` å’Œ `torch.multiprocessing` æ¥è®¾ç½®åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼Œå¹¶ä¸ºæ¯ä¸ª GPU ä¸Šçš„æ¨ç†ç”Ÿæˆè¿›ç¨‹ã€‚æ‚¨è¿˜åº”è¯¥åˆå§‹åŒ–ä¸€ä¸ª [`DiffusionPipeline`]ï¼š

```py
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

from diffusers import DiffusionPipeline

sd = DiffusionPipeline.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16, use_safetensors=True
)
```

æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è¿è¡Œæ¨ç†ï¼›[`init_process_group`](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group) å¤„ç†åˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼ç¯å¢ƒï¼ŒæŒ‡å®šè¦ä½¿ç”¨çš„åç«¯ç±»å‹ã€å½“å‰è¿›ç¨‹çš„ `rank` ä»¥åŠå‚ä¸è¿›ç¨‹çš„æ•°é‡ `world_size`ã€‚å¦‚æœæ‚¨åœ¨ 2 ä¸ª GPU ä¸Šå¹¶è¡Œè¿è¡Œæ¨ç†ï¼Œé‚£ä¹ˆ `world_size` å°±æ˜¯ 2ã€‚

å°† [`DiffusionPipeline`] ç§»åŠ¨åˆ° `rank`ï¼Œå¹¶ä½¿ç”¨ `get_rank` ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…ä¸€ä¸ª GPUï¼Œå…¶ä¸­æ¯ä¸ªè¿›ç¨‹å¤„ç†ä¸åŒçš„æç¤ºï¼š

```py
def run_inference(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

    sd.to(rank)

    if torch.distributed.get_rank() == 0:
        prompt = "a dog"
    elif torch.distributed.get_rank() == 1:
        prompt = "a cat"

    image = sd(prompt).images[0]
    image.save(f"./{'_'.join(prompt)}.png")
```

è¦è¿è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œè°ƒç”¨ [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn) åœ¨ `world_size` å®šä¹‰çš„ GPU æ•°é‡ä¸Šè¿è¡Œ `run_inference` å‡½æ•°ï¼š

```py
def main():
    world_size = 2
    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)


if __name__ == "__main__":
    main()
```

å®Œæˆæ¨ç†è„šæœ¬åï¼Œä½¿ç”¨ `--nproc_per_node` å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„ GPU æ•°é‡ï¼Œå¹¶è°ƒç”¨ `torchrun` æ¥è¿è¡Œè„šæœ¬ï¼š

```bash
torchrun run_distributed.py --nproc_per_node=2
```

> [!TIP]
> æ‚¨å¯ä»¥åœ¨ [`DiffusionPipeline`] ä¸­ä½¿ç”¨ `device_map` å°†å…¶æ¨¡å‹çº§ç»„ä»¶åˆ†å¸ƒåœ¨å¤šä¸ªè®¾å¤‡ä¸Šã€‚è¯·å‚è€ƒ [è®¾å¤‡æ”¾ç½®](../tutorials/inference_with_big_models#device-placement) æŒ‡å—äº†è§£æ›´å¤šä¿¡æ¯ã€‚

## æ¨¡å‹åˆ†ç‰‡

ç°ä»£æ‰©æ•£ç³»ç»Ÿï¼Œå¦‚ [Flux](../api/pipelines/flux)ï¼Œéå¸¸å¤§ä¸”åŒ…å«å¤šä¸ªæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œ[Flux.1-Dev](https://hf.co/black-forest-labs/FLUX.1-dev) ç”±ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨ - [T5-XXL](https://hf.co/google/t5-v1_1-xxl) å’Œ [CLIP-L](https://hf.co/openai/clip-vit-large-patch14) - ä¸€ä¸ª [æ‰©æ•£å˜æ¢å™¨](../api/models/flux_transformer)ï¼Œä»¥åŠä¸€ä¸ª [VAE](../api/models/autoencoderkl) ç»„æˆã€‚å¯¹äºå¦‚æ­¤å¤§çš„æ¨¡å‹ï¼Œåœ¨æ¶ˆè´¹çº§ GPU ä¸Šè¿è¡Œæ¨ç†å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚

æ¨¡å‹åˆ†ç‰‡æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå½“æ¨¡å‹æ— æ³•å®¹çº³åœ¨å•ä¸ª GPU ä¸Šæ—¶ï¼Œå°†æ¨¡å‹åˆ†å¸ƒåœ¨å¤šä¸ª GPU ä¸Šã€‚ä¸‹é¢çš„ç¤ºä¾‹å‡è®¾æœ‰ä¸¤ä¸ª 16GB GPU å¯ç”¨äºæ¨ç†ã€‚

å¼€å§‹ä½¿ç”¨æ–‡æœ¬ç¼–ç å™¨è®¡ç®—æ–‡æœ¬åµŒå…¥ã€‚é€šè¿‡è®¾ç½® `device_map="balanced"` å°†æ–‡æœ¬ç¼–ç å™¨ä¿æŒåœ¨ä¸¤ä¸ªGPUä¸Šã€‚`balanced` ç­–ç•¥å°†æ¨¡å‹å‡åŒ€åˆ†å¸ƒåœ¨æ‰€æœ‰å¯ç”¨GPUä¸Šã€‚ä½¿ç”¨ `max_memory` å‚æ•°ä¸ºæ¯ä¸ªGPUä¸Šçš„æ¯ä¸ªæ–‡æœ¬ç¼–ç å™¨åˆ†é…æœ€å¤§å†…å­˜é‡ã€‚

> [!TIP]
> **ä»…** åœ¨æ­¤æ­¥éª¤åŠ è½½æ–‡æœ¬ç¼–ç å™¨ï¼æ‰©æ•£å˜æ¢å™¨å’ŒVAEåœ¨åç»­æ­¥éª¤ä¸­åŠ è½½ä»¥èŠ‚çœå†…å­˜ã€‚

```py
from diffusers import FluxPipeline
import torch

prompt = "a photo of a dog with cat-like look"

pipeline = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    transformer=None,
    vae=None,
    device_map="balanced",
    max_memory={0: "16GB", 1: "16GB"},
    torch_dtype=torch.bfloat16
)
with torch.no_grad():
    print("Encoding prompts.")
    prompt_embeds, pooled_prompt_embeds, text_ids = pipeline.encode_prompt(
        prompt=prompt, prompt_2=None, max_sequence_length=512
    )
```

ä¸€æ—¦æ–‡æœ¬åµŒå…¥è®¡ç®—å®Œæˆï¼Œä»GPUä¸­ç§»é™¤å®ƒä»¬ä»¥ä¸ºæ‰©æ•£å˜æ¢å™¨è…¾å‡ºç©ºé—´ã€‚

```py
import gc 

def flush():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()

del pipeline.text_encoder
del pipeline.text_encoder_2
del pipeline.tokenizer
del pipeline.tokenizer_2
del pipeline

flush()
```

æ¥ä¸‹æ¥åŠ è½½æ‰©æ•£å˜æ¢å™¨ï¼Œå®ƒæœ‰125äº¿å‚æ•°ã€‚è¿™æ¬¡ï¼Œè®¾ç½® `device_map="auto"` ä»¥è‡ªåŠ¨å°†æ¨¡å‹åˆ†å¸ƒåœ¨ä¸¤ä¸ª16GB GPUä¸Šã€‚`auto` ç­–ç•¥ç”± [Accelerate](https://hf.co/docs/accelerate/index) æ”¯æŒï¼Œå¹¶ä½œä¸º [å¤§æ¨¡å‹æ¨ç†](https://hf.co/docs/accelerate/concept_guides/big_model_inference) åŠŸèƒ½çš„ä¸€éƒ¨åˆ†å¯ç”¨ã€‚å®ƒé¦–å…ˆå°†æ¨¡å‹åˆ†å¸ƒåœ¨æœ€å¿«çš„è®¾å¤‡ï¼ˆGPUï¼‰ä¸Šï¼Œç„¶ååœ¨éœ€è¦æ—¶ç§»åŠ¨åˆ°è¾ƒæ…¢çš„è®¾å¤‡å¦‚CPUå’Œç¡¬ç›˜ã€‚å°†æ¨¡å‹å‚æ•°å­˜å‚¨åœ¨è¾ƒæ…¢è®¾å¤‡ä¸Šçš„æƒè¡¡æ˜¯æ¨ç†å»¶è¿Ÿè¾ƒæ…¢ã€‚

```py
from diffusers import AutoModel
import torch 

transformer = AutoModel.from_pretrained(
    "black-forest-labs/FLUX.1-dev", 
    subfolder="transformer",
    device_map="auto",
    torch_dtype=torch.bfloat16
)
```

> [!TIP]
> åœ¨ä»»ä½•æ—¶å€™ï¼Œæ‚¨å¯ä»¥å°è¯• `print(pipeline.hf_device_map)` æ¥æŸ¥çœ‹å„ç§æ¨¡å‹å¦‚ä½•åœ¨è®¾å¤‡ä¸Šåˆ†å¸ƒã€‚è¿™å¯¹äºè·Ÿè¸ªæ¨¡å‹çš„è®¾å¤‡æ”¾ç½®å¾ˆæœ‰ç”¨ã€‚æ‚¨ä¹Ÿå¯ä»¥å°è¯• `print(transformer.hf_device_map)` æ¥æŸ¥çœ‹å˜æ¢å™¨æ¨¡å‹å¦‚ä½•åœ¨è®¾å¤‡ä¸Šåˆ†ç‰‡ã€‚

å°†å˜æ¢å™¨æ¨¡å‹æ·»åŠ åˆ°ç®¡é“ä¸­ä»¥è¿›è¡Œå»å™ªï¼Œä½†å°†å…¶ä»–æ¨¡å‹çº§ç»„ä»¶å¦‚æ–‡æœ¬ç¼–ç å™¨å’ŒVAEè®¾ç½®ä¸º `None`ï¼Œå› ä¸ºæ‚¨è¿˜ä¸éœ€è¦å®ƒä»¬ã€‚

```py
pipeline = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    text_encoder=None,
    text_encoder_2=None,
    tokenizer=None,
    tokenizer_2=None,
    vae=None,
    transformer=transformer,
    torch_dtype=torch.bfloat16
)

print("Running denoising.")
height, width = 768, 1360
latents = pipeline(
   
     
prompt_embeds=prompt_embeds,
pooled_prompt_embeds=pooled_prompt_embeds,
num_inference_steps=50,
guidance_scale=3.5,
height=height,
width=width,
output_type="latent",
).images
```

ä»å†…å­˜ä¸­ç§»é™¤ç®¡é“å’Œå˜æ¢å™¨ï¼Œå› ä¸ºå®ƒä»¬ä¸å†éœ€è¦ã€‚

```py
del pipeline.transformer
del pipeline

flush()
```

æœ€åï¼Œä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒã€‚VAEé€šå¸¸è¶³å¤Ÿå°ï¼Œå¯ä»¥åœ¨å•ä¸ªGPUä¸ŠåŠ è½½ã€‚

```py
from diffusers import AutoencoderKL
from diffusers.image_processor import VaeImageProcessor
import torch 

vae = AutoencoderKL.from_pretrained(ckpt_id, subfolder="vae", torch_dtype=torch.bfloat16).to("cuda")
vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)
image_processor = VaeImageProcessor(vae_scale_factor=vae_scale_factor)

with torch.no_grad():
    print("è¿è¡Œè§£ç ä¸­ã€‚")
    latents = FluxPipeline._unpack_latents(latents, height, width, vae_scale_factor)
    latents = (latents / vae.config.scaling_factor) + vae.config.shift_factor

    image = vae.decode(latents, return_dict=False)[0]
    image = image_processor.postprocess(image, output_type="pil")
    image[0].save("split_transformer.png")
```

é€šè¿‡é€‰æ‹©æ€§åŠ è½½å’Œå¸è½½åœ¨ç‰¹å®šé˜¶æ®µæ‰€éœ€çš„æ¨¡å‹ï¼Œå¹¶å°†æœ€å¤§æ¨¡å‹åˆ†ç‰‡åˆ°å¤šä¸ªGPUä¸Šï¼Œå¯ä»¥åœ¨æ¶ˆè´¹çº§GPUä¸Šè¿è¡Œå¤§å‹æ¨¡å‹çš„æ¨ç†ã€‚