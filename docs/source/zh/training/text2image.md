<!--Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# æ–‡ç”Ÿå›¾

> [!WARNING]
> æ–‡ç”Ÿå›¾è®­ç»ƒè„šæœ¬ç›®å‰å¤„äºå®éªŒé˜¶æ®µï¼Œå®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆå’Œç¾éš¾æ€§é—å¿˜ç­‰é—®é¢˜ã€‚å»ºè®®å°è¯•ä¸åŒè¶…å‚æ•°ä»¥è·å¾—æœ€ä½³æ•°æ®é›†é€‚é…æ•ˆæœã€‚

Stable Diffusion ç­‰æ–‡ç”Ÿå›¾æ¨¡å‹èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆå¯¹åº”å›¾åƒã€‚

æ¨¡å‹è®­ç»ƒå¯¹ç¡¬ä»¶è¦æ±‚è¾ƒé«˜ï¼Œä½†å¯ç”¨ `gradient_checkpointing` å’Œ `mixed_precision` åï¼Œå¯åœ¨å•å—24GBæ˜¾å­˜GPUä¸Šå®Œæˆè®­ç»ƒã€‚å¦‚éœ€æ›´å¤§æ‰¹æ¬¡æˆ–æ›´å¿«è®­ç»ƒé€Ÿåº¦ï¼Œå»ºè®®ä½¿ç”¨30GBä»¥ä¸Šæ˜¾å­˜çš„GPUè®¾å¤‡ã€‚é€šè¿‡å¯ç”¨ [xFormers](../optimization/xformers) å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶å¯é™ä½æ˜¾å­˜å ç”¨ã€‚JAX/Flax è®­ç»ƒæ–¹æ¡ˆä¹Ÿæ”¯æŒTPU/GPUé«˜æ•ˆè®­ç»ƒï¼Œä½†ä¸æ”¯æŒæ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ¢¯åº¦ç´¯ç§¯å’ŒxFormersã€‚ä½¿ç”¨Flaxè®­ç»ƒæ—¶å»ºè®®é…å¤‡30GBä»¥ä¸Šæ˜¾å­˜GPUæˆ–TPU v3ã€‚

æœ¬æŒ‡å—å°†è¯¦è§£ [train_text_to_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py) è®­ç»ƒè„šæœ¬ï¼ŒåŠ©æ‚¨æŒæ¡å…¶åŸç†å¹¶é€‚é…è‡ªå®šä¹‰éœ€æ±‚ã€‚

è¿è¡Œè„šæœ¬å‰è¯·ç¡®ä¿å·²ä»æºç å®‰è£…åº“ï¼š

```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
```

ç„¶åè¿›å…¥åŒ…å«è®­ç»ƒè„šæœ¬çš„ç¤ºä¾‹ç›®å½•ï¼Œå®‰è£…å¯¹åº”ä¾èµ–ï¼š

<hfoptions id="installation">
<hfoption id="PyTorch">
```bash
cd examples/text_to_image
pip install -r requirements.txt
```
</hfoption>
<hfoption id="Flax">
```bash
cd examples/text_to_image
pip install -r requirements_flax.txt
```
</hfoption>
</hfoptions>

> [!TIP]
> ğŸ¤— Accelerate æ˜¯æ”¯æŒå¤šGPU/TPUè®­ç»ƒå’Œæ··åˆç²¾åº¦çš„å·¥å…·åº“ï¼Œèƒ½æ ¹æ®ç¡¬ä»¶ç¯å¢ƒè‡ªåŠ¨é…ç½®è®­ç»ƒå‚æ•°ã€‚å‚é˜… ğŸ¤— Accelerate [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/accelerate/quicktour) äº†è§£æ›´å¤šã€‚

åˆå§‹åŒ– ğŸ¤— Accelerate ç¯å¢ƒï¼š

```bash
accelerate config
```

è¦åˆ›å»ºé»˜è®¤é…ç½®ç¯å¢ƒï¼ˆä¸è¿›è¡Œäº¤äº’å¼é€‰æ‹©ï¼‰ï¼š

```bash
accelerate config default
```

è‹¥ç¯å¢ƒä¸æ”¯æŒäº¤äº’å¼shellï¼ˆå¦‚notebookï¼‰ï¼Œå¯ä½¿ç”¨ï¼š

```py
from accelerate.utils import write_basic_config

write_basic_config()
```

æœ€åï¼Œå¦‚éœ€åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¯·å‚é˜… [åˆ›å»ºè®­ç»ƒæ•°æ®é›†](create_dataset) æŒ‡å—äº†è§£å¦‚ä½•å‡†å¤‡é€‚é…è„šæœ¬çš„æ•°æ®é›†ã€‚

## è„šæœ¬å‚æ•°

> [!TIP]
> ä»¥ä¸‹é‡ç‚¹ä»‹ç»è„šæœ¬ä¸­å½±å“è®­ç»ƒæ•ˆæœçš„å…³é”®å‚æ•°ï¼Œå¦‚éœ€å®Œæ•´å‚æ•°è¯´æ˜å¯æŸ¥é˜… [è„šæœ¬æºç ](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py)ã€‚å¦‚æœ‰ç–‘é—®æ¬¢è¿åé¦ˆã€‚

è®­ç»ƒè„šæœ¬æä¾›ä¸°å¯Œå‚æ•°ä¾›è‡ªå®šä¹‰è®­ç»ƒæµç¨‹ï¼Œæ‰€æœ‰å‚æ•°åŠè¯´æ˜è¯¦è§ [`parse_args()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L193) å‡½æ•°ã€‚è¯¥å‡½æ•°ä¸ºæ¯ä¸ªå‚æ•°æä¾›é»˜è®¤å€¼ï¼ˆå¦‚æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ç­‰ï¼‰ï¼Œä¹Ÿå¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ã€‚

ä¾‹å¦‚ä½¿ç”¨fp16æ··åˆç²¾åº¦åŠ é€Ÿè®­ç»ƒï¼š

```bash
accelerate launch train_text_to_image.py \
  --mixed_precision="fp16"
```

åŸºç¡€é‡è¦å‚æ•°åŒ…æ‹¬ï¼š

- `--pretrained_model_name_or_path`: Hubæ¨¡å‹åç§°æˆ–æœ¬åœ°é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
- `--dataset_name`: Hubæ•°æ®é›†åç§°æˆ–æœ¬åœ°è®­ç»ƒæ•°æ®é›†è·¯å¾„
- `--image_column`: æ•°æ®é›†ä¸­å›¾åƒåˆ—å
- `--caption_column`: æ•°æ®é›†ä¸­æ–‡æœ¬åˆ—å
- `--output_dir`: æ¨¡å‹ä¿å­˜è·¯å¾„
- `--push_to_hub`: æ˜¯å¦å°†è®­ç»ƒæ¨¡å‹æ¨é€è‡³Hub
- `--checkpointing_steps`: æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜æ­¥æ•°ï¼›è®­ç»ƒä¸­æ–­æ—¶å¯æ·»åŠ  `--resume_from_checkpoint` ä»è¯¥æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

### Min-SNRåŠ æƒç­–ç•¥

[Min-SNR](https://huggingface.co/papers/2303.09556) åŠ æƒç­–ç•¥é€šè¿‡é‡æ–°å¹³è¡¡æŸå¤±å‡½æ•°åŠ é€Ÿæ¨¡å‹æ”¶æ•›ã€‚è®­ç»ƒè„šæœ¬æ”¯æŒé¢„æµ‹ `epsilon`ï¼ˆå™ªå£°ï¼‰æˆ– `v_prediction`ï¼Œè€ŒMin-SNRå…¼å®¹ä¸¤ç§é¢„æµ‹ç±»å‹ã€‚è¯¥ç­–ç•¥ä»…é™PyTorchç‰ˆæœ¬ï¼ŒFlaxè®­ç»ƒè„šæœ¬ä¸æ”¯æŒã€‚

æ·»åŠ  `--snr_gamma` å‚æ•°å¹¶è®¾ä¸ºæ¨èå€¼5.0ï¼š

```bash
accelerate launch train_text_to_image.py \
  --snr_gamma=5.0
```

å¯é€šè¿‡æ­¤ [Weights and Biases](https://wandb.ai/sayakpaul/text2image-finetune-minsnr) æŠ¥å‘Šæ¯”è¾ƒä¸åŒ `snr_gamma` å€¼çš„æŸå¤±æ›²é¢ã€‚å°æ•°æ®é›†ä¸ŠMin-SNRæ•ˆæœå¯èƒ½ä¸å¦‚å¤§æ•°æ®é›†æ˜¾è‘—ã€‚

## è®­ç»ƒè„šæœ¬è§£æ

æ•°æ®é›†é¢„å¤„ç†ä»£ç å’Œè®­ç»ƒå¾ªç¯ä½äº [`main()`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L490) å‡½æ•°ï¼Œè‡ªå®šä¹‰ä¿®æ”¹éœ€åœ¨æ­¤å¤„è¿›è¡Œã€‚

`train_text_to_image` è„šæœ¬é¦–å…ˆ [åŠ è½½è°ƒåº¦å™¨](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L543) å’Œåˆ†è¯å™¨ï¼Œæ­¤å¤„å¯æ›¿æ¢å…¶ä»–è°ƒåº¦å™¨ï¼š

```py
noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
tokenizer = CLIPTokenizer.from_pretrained(
    args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision
)
```

æ¥ç€ [åŠ è½½UNetæ¨¡å‹](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L619)ï¼š

```py
load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder="unet")
model.register_to_config(**load_model.config)

model.load_state_dict(load_model.state_dict())
```

éšåå¯¹æ•°æ®é›†çš„æ–‡æœ¬å’Œå›¾åƒåˆ—è¿›è¡Œé¢„å¤„ç†ã€‚[`tokenize_captions`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L724) å‡½æ•°å¤„ç†æ–‡æœ¬åˆ†è¯ï¼Œ[`train_transforms`](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L742) å®šä¹‰å›¾åƒå¢å¼ºç­–ç•¥ï¼ŒäºŒè€…é›†æˆäº `preprocess_train`ï¼š

```py
def preprocess_train(examples):
    images = [image.convert("RGB") for image in examples[image_column]]
    examples["pixel_values"] = [train_transforms(image) for image in images]
    examples["input_ids"] = tokenize_captions(examples)
    return examples
```

æœ€åï¼Œ[è®­ç»ƒå¾ªç¯](https://github.com/huggingface/diffusers/blob/8959c5b9dec1c94d6ba482c94a58d2215c5fd026/examples/text_to_image/train_text_to_image.py#L878) å¤„ç†å‰©ä½™æµç¨‹ï¼šå›¾åƒç¼–ç ä¸ºæ½œç©ºé—´ã€æ·»åŠ å™ªå£°ã€è®¡ç®—æ–‡æœ¬åµŒå…¥æ¡ä»¶ã€æ›´æ–°æ¨¡å‹å‚æ•°ã€ä¿å­˜å¹¶æ¨é€æ¨¡å‹è‡³Hubã€‚æƒ³æ·±å…¥äº†è§£è®­ç»ƒå¾ªç¯åŸç†ï¼Œå¯å‚é˜… [ç†è§£ç®¡é“ã€æ¨¡å‹ä¸è°ƒåº¦å™¨](../using-diffusers/write_own_pipeline) æ•™ç¨‹ï¼Œè¯¥æ•™ç¨‹è§£æäº†å»å™ªè¿‡ç¨‹çš„æ ¸å¿ƒé€»è¾‘ã€‚

## å¯åŠ¨è„šæœ¬

å®Œæˆæ‰€æœ‰é…ç½®åï¼Œå³å¯å¯åŠ¨è®­ç»ƒè„šæœ¬ï¼ğŸš€

<hfoptions id="training-inference">
<hfoption id="PyTorch">

ä»¥ [ç«å½±å¿è€…BLIPæ ‡æ³¨æ•°æ®é›†](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions) ä¸ºä¾‹è®­ç»ƒç”Ÿæˆç«å½±è§’è‰²ã€‚è®¾ç½®ç¯å¢ƒå˜é‡ `MODEL_NAME` å’Œ `dataset_name` æŒ‡å®šæ¨¡å‹å’Œæ•°æ®é›†ï¼ˆHubæˆ–æœ¬åœ°è·¯å¾„ï¼‰ã€‚å¤šGPUè®­ç»ƒéœ€åœ¨ `accelerate launch` å‘½ä»¤ä¸­æ·»åŠ  `--multi_gpu` å‚æ•°ã€‚

> [!TIP]
> ä½¿ç”¨æœ¬åœ°æ•°æ®é›†æ—¶ï¼Œè®¾ç½® `TRAIN_DIR` å’Œ `OUTPUT_DIR` ç¯å¢ƒå˜é‡ä¸ºæ•°æ®é›†è·¯å¾„å’Œæ¨¡å‹ä¿å­˜è·¯å¾„ã€‚

```bash
export MODEL_NAME="stable-diffusion-v1-5/stable-diffusion-v1-5"
export dataset_name="lambdalabs/naruto-blip-captions"

accelerate launch --mixed_precision="fp16"  train_text_to_image.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --dataset_name=$dataset_name \
  --use_ema \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --gradient_checkpointing \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --enable_xformers_memory_efficient_attention \
  --lr_scheduler="constant" --lr_warmup_steps=0 \
  --output_dir="sd-naruto-model" \
  --push_to_hub
```

</hfoption>
<hfoption id="Flax">

Flaxè®­ç»ƒæ–¹æ¡ˆåœ¨TPU/GPUä¸Šæ•ˆç‡æ›´é«˜ï¼ˆç”± [@duongna211](https://github.com/duongna21) å®ç°ï¼‰ï¼ŒTPUæ€§èƒ½æ›´ä¼˜ä½†GPUè¡¨ç°åŒæ ·å‡ºè‰²ã€‚

è®¾ç½®ç¯å¢ƒå˜é‡ `MODEL_NAME` å’Œ `dataset_name` æŒ‡å®šæ¨¡å‹å’Œæ•°æ®é›†ï¼ˆHubæˆ–æœ¬åœ°è·¯å¾„ï¼‰ã€‚

> [!TIP]
> ä½¿ç”¨æœ¬åœ°æ•°æ®é›†æ—¶ï¼Œè®¾ç½® `TRAIN_DIR` å’Œ `OUTPUT_DIR` ç¯å¢ƒå˜é‡ä¸ºæ•°æ®é›†è·¯å¾„å’Œæ¨¡å‹ä¿å­˜è·¯å¾„ã€‚

```bash
export MODEL_NAME="stable-diffusion-v1-5/stable-diffusion-v1-5"
export dataset_name="lambdalabs/naruto-blip-captions"

python train_text_to_image_flax.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --dataset_name=$dataset_name \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --output_dir="sd-naruto-model" \
  --push_to_hub
```

</hfoption>
</hfoptions>

è®­ç»ƒå®Œæˆåï¼Œå³å¯ä½¿ç”¨æ–°æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

<hfoptions id="training-inference">
<hfoption id="PyTorch">

```py
from diffusers import StableDiffusionPipeline
import torch

pipeline = StableDiffusionPipeline.from_pretrained("path/to/saved_model", torch_dtype=torch.float16, use_safetensors=True).to("cuda")

image = pipeline(prompt="yoda").images[0]
image.save("yoda-naruto.png")
```

</hfoption>
<hfoption id="Flax">

```py
import jax
import numpy as np
from flax.jax_utils import replicate
from flax.training.common_utils import shard
from diffusers import FlaxStableDiffusionPipeline

pipeline, params = FlaxStableDiffusionPipeline.from_pretrained("path/to/saved_model", dtype=jax.numpy.bfloat16)

prompt = "yoda naruto"
prng_seed = jax.random.PRNGKey(0)
num_inference_steps = 50

num_samples = jax.device_count()
prompt = num_samples * [prompt]
prompt_ids = pipeline.prepare_inputs(prompt)

# åˆ†ç‰‡è¾“å…¥å’Œéšæœºæ•°
params = replicate(params)
prng_seed = jax.random.split(prng_seed, jax.device_count())
prompt_ids = shard(prompt_ids)

images = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images
images = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))
image.save("yoda-naruto.png")
```

</hfoption>
</hfoptions>

## åç»­æ­¥éª¤

æ­å–œå®Œæˆæ–‡ç”Ÿå›¾æ¨¡å‹è®­ç»ƒï¼å¦‚éœ€è¿›ä¸€æ­¥ä½¿ç”¨æ¨¡å‹ï¼Œä»¥ä¸‹æŒ‡å—å¯èƒ½æœ‰æ‰€å¸®åŠ©ï¼š

- äº†è§£å¦‚ä½•åŠ è½½ [LoRAæƒé‡](../using-diffusers/loading_adapters#LoRA) è¿›è¡Œæ¨ç†ï¼ˆå¦‚æœè®­ç»ƒæ—¶ä½¿ç”¨äº†LoRAï¼‰
- åœ¨ [æ–‡ç”Ÿå›¾](../using-diffusers/conditional_image_generation) ä»»åŠ¡æŒ‡å—ä¸­ï¼Œäº†è§£å¼•å¯¼å°ºåº¦ç­‰å‚æ•°æˆ–æç¤ºè¯åŠ æƒç­‰æŠ€æœ¯å¦‚ä½•æ§åˆ¶ç”Ÿæˆæ•ˆæœ