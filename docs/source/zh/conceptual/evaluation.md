<!--Copyright 2025 The HuggingFace Team. All rights reserved.

æ ¹æ® Apache License 2.0 ç‰ˆæœ¬ï¼ˆ"è®¸å¯è¯"ï¼‰æˆæƒï¼Œé™¤éç¬¦åˆè®¸å¯è¯è¦æ±‚ï¼Œå¦åˆ™ä¸å¾—ä½¿ç”¨æœ¬æ–‡ä»¶ã€‚
æ‚¨å¯ä»¥åœ¨ä»¥ä¸‹ç½‘å€è·å–è®¸å¯è¯å‰¯æœ¬ï¼š

http://www.apache.org/licenses/LICENSE-2.0

é™¤éé€‚ç”¨æ³•å¾‹è¦æ±‚æˆ–ä¹¦é¢åŒæ„ï¼Œæœ¬è½¯ä»¶æŒ‰"åŸæ ·"åˆ†å‘ï¼Œä¸é™„å¸¦ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„æ‹…ä¿æˆ–æ¡ä»¶ã€‚è¯¦è§è®¸å¯è¯ä¸­è§„å®šçš„ç‰¹å®šè¯­è¨€æƒé™å’Œé™åˆ¶ã€‚
-->

# Diffusionæ¨¡å‹è¯„ä¼°æŒ‡å—

<a target="_blank" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/evaluation.ipynb">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="åœ¨ Colab ä¸­æ‰“å¼€"/>
</a>

> [!TIP]
> é‰´äºå½“å‰å·²å‡ºç°é’ˆå¯¹å›¾åƒç”ŸæˆDiffusionæ¨¡å‹çš„æˆç†Ÿè¯„ä¼°æ¡†æ¶ï¼ˆå¦‚[HEIM](https://crfm.stanford.edu/helm/heim/latest/)ã€[T2I-Compbench](https://huggingface.co/papers/2307.06350)ã€[GenEval](https://huggingface.co/papers/2310.11513)ï¼‰ï¼Œæœ¬æ–‡æ¡£éƒ¨åˆ†å†…å®¹å·²è¿‡æ—¶ã€‚

åƒ [Stable Diffusion](https://huggingface.co/docs/diffusers/stable_diffusion) è¿™ç±»ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°æœ¬è´¨ä¸Šæ˜¯ä¸»è§‚çš„ã€‚ä½†ä½œä¸ºå¼€å‘è€…å’Œç ”ç©¶è€…ï¼Œæˆ‘ä»¬ç»å¸¸éœ€è¦åœ¨ä¼—å¤šå¯èƒ½æ€§ä¸­åšå‡ºå®¡æ…é€‰æ‹©ã€‚é‚£ä¹ˆå½“é¢å¯¹ä¸åŒç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ GANsã€Diffusion ç­‰ï¼‰æ—¶ï¼Œè¯¥å¦‚ä½•å†³ç­–ï¼Ÿ

å®šæ€§è¯„ä¼°å®¹æ˜“äº§ç”Ÿåå·®ï¼Œå¯èƒ½å¯¼è‡´é”™è¯¯ç»“è®ºï¼›è€Œå®šé‡æŒ‡æ ‡åˆæœªå¿…èƒ½å‡†ç¡®åæ˜ å›¾åƒè´¨é‡ã€‚å› æ­¤ï¼Œé€šå¸¸éœ€è¦ç»“åˆå®šæ€§ä¸å®šé‡è¯„ä¼°æ¥è·å¾—æ›´å¯é çš„æ¨¡å‹é€‰æ‹©ä¾æ®ã€‚

æœ¬æ–‡æ¡£å°†ç³»ç»Ÿä»‹ç»æ‰©æ•£æ¨¡å‹çš„å®šæ€§ä¸å®šé‡è¯„ä¼°æ–¹æ³•ï¼ˆéç©·å°½åˆ—ä¸¾ï¼‰ã€‚å¯¹äºå®šé‡æ–¹æ³•ï¼Œæˆ‘ä»¬å°†é‡ç‚¹æ¼”ç¤ºå¦‚ä½•ç»“åˆ `diffusers` åº“å®ç°è¿™äº›è¯„ä¼°ã€‚

æ–‡æ¡£æ‰€ç¤ºæ–¹æ³•åŒæ ·é€‚ç”¨äºè¯„ä¼°ä¸åŒ[å™ªå£°è°ƒåº¦å™¨](https://huggingface.co/docs/diffusers/main/en/api/schedulers/overview)åœ¨å›ºå®šç”Ÿæˆæ¨¡å‹ä¸‹çš„è¡¨ç°å·®å¼‚ã€‚

## è¯„ä¼°åœºæ™¯

æˆ‘ä»¬æ¶µç›–ä»¥ä¸‹Diffusionæ¨¡å‹ç®¡çº¿çš„è¯„ä¼°ï¼š

- æ–‡æœ¬å¼•å¯¼å›¾åƒç”Ÿæˆï¼ˆå¦‚ [`StableDiffusionPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/text2img)ï¼‰
- åŸºäºæ–‡æœ¬å’Œè¾“å…¥å›¾åƒçš„å¼•å¯¼ç”Ÿæˆï¼ˆå¦‚ [`StableDiffusionImg2ImgPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img) å’Œ [`StableDiffusionInstructPix2PixPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix)ï¼‰
- ç±»åˆ«æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡å‹(å¦‚ [`DiTPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipe))

## å®šæ€§è¯„ä¼°

å®šæ€§è¯„ä¼°é€šå¸¸æ¶‰åŠå¯¹ç”Ÿæˆå›¾åƒçš„äººå·¥è¯„åˆ¤ã€‚è¯„ä¼°ç»´åº¦åŒ…æ‹¬æ„å›¾è´¨é‡ã€å›¾æ–‡å¯¹é½åº¦å’Œç©ºé—´å…³ç³»ç­‰æ–¹é¢ã€‚æ ‡å‡†åŒ–çš„æç¤ºè¯èƒ½ä¸ºè¿™äº›ä¸»è§‚æŒ‡æ ‡æä¾›ç»Ÿä¸€åŸºå‡†ã€‚DrawBenchå’ŒPartiPromptsæ˜¯å¸¸ç”¨çš„å®šæ€§è¯„ä¼°æç¤ºè¯æ•°æ®é›†ï¼Œåˆ†åˆ«ç”±[Imagen](https://imagen.research.google/)å’Œ[Parti](https://parti.research.google/)å›¢é˜Ÿæå‡ºã€‚

æ ¹æ®[Partiå®˜æ–¹ç½‘ç«™](https://parti.research.google/)è¯´æ˜ï¼š

> PartiPrompts (P2)æ˜¯æˆ‘ä»¬å‘å¸ƒçš„åŒ…å«1600å¤šä¸ªè‹±æ–‡æç¤ºè¯çš„ä¸°å¯Œé›†åˆï¼Œå¯ç”¨äºæµ‹é‡æ¨¡å‹åœ¨ä¸åŒç±»åˆ«å’ŒæŒ‘æˆ˜ç»´åº¦ä¸Šçš„èƒ½åŠ›ã€‚

![parti-prompts](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/parti-prompts.png)

PartiPromptsåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- Promptï¼ˆæç¤ºè¯ï¼‰
- Categoryï¼ˆç±»åˆ«ï¼Œå¦‚"æŠ½è±¡"ã€"ä¸–ç•ŒçŸ¥è¯†"ç­‰ï¼‰
- Challengeï¼ˆéš¾åº¦ç­‰çº§ï¼Œå¦‚"åŸºç¡€"ã€"å¤æ‚"ã€"æ–‡å­—ä¸ç¬¦å·"ç­‰ï¼‰

è¿™äº›åŸºå‡†æµ‹è¯•æ”¯æŒå¯¹ä¸åŒå›¾åƒç”Ÿæˆæ¨¡å‹è¿›è¡Œå¹¶æ’äººå·¥å¯¹æ¯”è¯„ä¼°ã€‚ä¸ºæ­¤ï¼ŒğŸ§¨ Diffuserså›¢é˜Ÿæ„å»ºäº†**Open Parti Prompts**â€”â€”ä¸€ä¸ªåŸºäºParti Promptsçš„ç¤¾åŒºé©±åŠ¨å‹å®šæ€§è¯„ä¼°åŸºå‡†ï¼Œç”¨äºæ¯”è¾ƒé¡¶å°–å¼€æºdiffusionæ¨¡å‹ï¼š
- [Open Parti Promptsæ¸¸æˆ](https://huggingface.co/spaces/OpenGenAI/open-parti-prompts)ï¼šå±•ç¤º10ä¸ªpartiæç¤ºè¯å¯¹åº”çš„4å¼ ç”Ÿæˆå›¾åƒï¼Œç”¨æˆ·é€‰æ‹©æœ€ç¬¦åˆæç¤ºçš„å›¾ç‰‡
- [Open Parti Promptsæ’è¡Œæ¦œ](https://huggingface.co/spaces/OpenGenAI/parti-prompts-leaderboard)ï¼šå¯¹æ¯”å½“å‰æœ€ä¼˜å¼€æºdiffusionæ¨¡å‹çš„æ€§èƒ½æ¦œå•

ä¸ºè¿›è¡Œæ‰‹åŠ¨å›¾åƒå¯¹æ¯”ï¼Œæˆ‘ä»¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨`diffusers`å¤„ç†éƒ¨åˆ†PartiPromptsæç¤ºè¯ã€‚

ä»¥ä¸‹æ˜¯ä»ä¸åŒæŒ‘æˆ˜ç»´åº¦ï¼ˆåŸºç¡€ã€å¤æ‚ã€è¯­è¨€ç»“æ„ã€æƒ³è±¡åŠ›ã€æ–‡å­—ä¸ç¬¦å·ï¼‰é‡‡æ ·çš„æç¤ºè¯ç¤ºä¾‹ï¼ˆä½¿ç”¨[PartiPromptsä½œä¸ºæ•°æ®é›†](https://huggingface.co/datasets/nateraw/parti-prompts)ï¼‰ï¼š

```python
from datasets import load_dataset

# prompts = load_dataset("nateraw/parti-prompts", split="train")
# prompts = prompts.shuffle()
# sample_prompts = [prompts[i]["Prompt"] for i in range(5)]

# Fixing these sample prompts in the interest of reproducibility.
sample_prompts = [
    "a corgi",
    "a hot air balloon with a yin-yang symbol, with the moon visible in the daytime sky",
    "a car with no windows",
    "a cube made of porcupine",
    'The saying "BE EXCELLENT TO EACH OTHER" written on a red brick wall with a graffiti image of a green alien wearing a tuxedo. A yellow fire hydrant is on a sidewalk in the foreground.',
]
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Stable Diffusionï¼ˆ[v1-4 checkpoint](https://huggingface.co/CompVis/stable-diffusion-v1-4)ï¼‰ç”Ÿæˆè¿™äº›æç¤ºè¯å¯¹åº”çš„å›¾åƒï¼š

```python
import torch

seed = 0
generator = torch.manual_seed(seed)

images = sd_pipeline(sample_prompts, num_images_per_prompt=1, generator=generator).images
```

![parti-prompts-14](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/parti-prompts-14.png)

æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®`num_images_per_prompt`å‚æ•°æ¥æ¯”è¾ƒåŒä¸€æç¤ºè¯ç”Ÿæˆçš„ä¸åŒå›¾åƒã€‚ä½¿ç”¨ä¸åŒæ£€æŸ¥ç‚¹([v1-5](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5))è¿è¡Œç›¸åŒæµç¨‹åï¼Œç»“æœå¦‚ä¸‹ï¼š

![parti-prompts-15](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/parti-prompts-15.png)

å½“ä½¿ç”¨å¤šä¸ªå¾…è¯„ä¼°æ¨¡å‹ä¸ºæ‰€æœ‰æç¤ºè¯ç”Ÿæˆè‹¥å¹²å›¾åƒåï¼Œè¿™äº›ç»“æœå°†æäº¤ç»™äººç±»è¯„ä¼°å‘˜è¿›è¡Œæ‰“åˆ†ã€‚æœ‰å…³DrawBenchå’ŒPartiPromptsåŸºå‡†æµ‹è¯•çš„æ›´å¤šç»†èŠ‚ï¼Œè¯·å‚é˜…å„è‡ªçš„è®ºæ–‡ã€‚

> [!TIP]
> åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æŸ¥çœ‹æ¨ç†æ ·æœ¬æœ‰åŠ©äºè¯„ä¼°è®­ç»ƒè¿›åº¦ã€‚æˆ‘ä»¬çš„[è®­ç»ƒè„šæœ¬](https://github.com/huggingface/diffusers/tree/main/examples/)æ”¯æŒæ­¤åŠŸèƒ½ï¼Œå¹¶é¢å¤–æä¾›TensorBoardå’ŒWeights & Biasesæ—¥å¿—è®°å½•åŠŸèƒ½ã€‚

## å®šé‡è¯„ä¼°

æœ¬èŠ‚å°†æŒ‡å¯¼æ‚¨å¦‚ä½•è¯„ä¼°ä¸‰ç§ä¸åŒçš„æ‰©æ•£æµç¨‹ï¼Œä½¿ç”¨ä»¥ä¸‹æŒ‡æ ‡ï¼š
- CLIPåˆ†æ•°
- CLIPæ–¹å‘ç›¸ä¼¼åº¦
- FIDï¼ˆå¼—é›·æ­‡èµ·å§‹è·ç¦»ï¼‰

### æ–‡æœ¬å¼•å¯¼å›¾åƒç”Ÿæˆ

[CLIPåˆ†æ•°](https://huggingface.co/papers/2104.08718)ç”¨äºè¡¡é‡å›¾åƒ-æ ‡é¢˜å¯¹çš„åŒ¹é…ç¨‹åº¦ã€‚CLIPåˆ†æ•°è¶Šé«˜è¡¨æ˜åŒ¹é…åº¦è¶Šé«˜ğŸ”¼ã€‚è¯¥åˆ†æ•°æ˜¯å¯¹"åŒ¹é…åº¦"è¿™ä¸€å®šæ€§æ¦‚å¿µçš„é‡åŒ–æµ‹é‡ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºå›¾åƒä¸æ ‡é¢˜ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚ç ”ç©¶å‘ç°CLIPåˆ†æ•°ä¸äººç±»åˆ¤æ–­å…·æœ‰é«˜åº¦ç›¸å…³æ€§ã€‚

é¦–å…ˆåŠ è½½[`StableDiffusionPipeline`]ï¼š

```python
from diffusers import StableDiffusionPipeline
import torch

model_ckpt = "CompVis/stable-diffusion-v1-4"
sd_pipeline = StableDiffusionPipeline.from_pretrained(model_ckpt, torch_dtype=torch.float16).to("cuda")
```

ä½¿ç”¨å¤šä¸ªæç¤ºè¯ç”Ÿæˆå›¾åƒï¼š

```python
prompts = [
    "a photo of an astronaut riding a horse on mars",
    "A high tech solarpunk utopia in the Amazon rainforest",
    "A pikachu fine dining with a view to the Eiffel Tower",
    "A mecha robot in a favela in expressionist style",
    "an insect robot preparing a delicious meal",
    "A small cabin on top of a snowy mountain in the style of Disney, artstation",
]

images = sd_pipeline(prompts, num_images_per_prompt=1, output_type="np").images

print(images.shape)
# (6, 512, 512, 3)
```

ç„¶åè®¡ç®—CLIPåˆ†æ•°ï¼š

```python
from torchmetrics.functional.multimodal import clip_score
from functools import partial

clip_score_fn = partial(clip_score, model_name_or_path="openai/clip-vit-base-patch16")

def calculate_clip_score(images, prompts):
    images_int = (images * 255).astype("uint8")
    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()
    return round(float(clip_score), 4)

sd_clip_score = calculate_clip_score(images, prompts)
print(f"CLIPåˆ†æ•°: {sd_clip_score}")
# CLIPåˆ†æ•°: 35.7038
```

ä¸Šè¿°ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆä¸€å¼ å›¾åƒã€‚å¦‚æœä¸ºæ¯ä¸ªæç¤ºç”Ÿæˆå¤šå¼ å›¾åƒï¼Œåˆ™éœ€è¦è®¡ç®—æ¯ä¸ªæç¤ºç”Ÿæˆå›¾åƒçš„å¹³å‡åˆ†æ•°ã€‚

å½“éœ€è¦æ¯”è¾ƒä¸¤ä¸ªå…¼å®¹[`StableDiffusionPipeline`]çš„æ£€æŸ¥ç‚¹æ—¶ï¼Œåº”åœ¨è°ƒç”¨ç®¡é“æ—¶ä¼ å…¥ç”Ÿæˆå™¨ã€‚é¦–å…ˆä½¿ç”¨[v1-4 Stable Diffusionæ£€æŸ¥ç‚¹](https://huggingface.co/CompVis/stable-diffusion-v1-4)ä»¥å›ºå®šç§å­ç”Ÿæˆå›¾åƒï¼š

```python
seed = 0
generator = torch.manual_seed(seed)

images = sd_pipeline(prompts, num_images_per_prompt=1, generator=generator, output_type="np").images
```

ç„¶ååŠ è½½[v1-5æ£€æŸ¥ç‚¹](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5)ç”Ÿæˆå›¾åƒï¼š

```python
model_ckpt_1_5 = "stable-diffusion-v1-5/stable-diffusion-v1-5"
sd_pipeline_1_5 = StableDiffusionPipeline.from_pretrained(model_ckpt_1_5, torch_dtype=torch.float16).to("cuda")

images_1_5 = sd_pipeline_1_5(prompts, num_images_per_prompt=1, generator=generator, output_type="np").images
```

æœ€åæ¯”è¾ƒä¸¤è€…çš„CLIPåˆ†æ•°ï¼š

```python
sd_clip_score_1_4 = calculate_clip_score(images, prompts)
print(f"v-1-4ç‰ˆæœ¬çš„CLIPåˆ†æ•°: {sd_clip_score_1_4}")
# v-1-4ç‰ˆæœ¬çš„CLIPåˆ†æ•°: 34.9102

sd_clip_score_1_5 = calculate_clip_score(images_1_5, prompts)
print(f"v-1-5ç‰ˆæœ¬çš„CLIPåˆ†æ•°: {sd_clip_score_1_5}")
# v-1-5ç‰ˆæœ¬çš„CLIPåˆ†æ•°: 36.2137
```

ç»“æœè¡¨æ˜[v1-5](https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5)æ£€æŸ¥ç‚¹æ€§èƒ½ä¼˜äºå‰ä»£ã€‚ä½†éœ€æ³¨æ„ï¼Œæˆ‘ä»¬ç”¨äºè®¡ç®—CLIPåˆ†æ•°çš„æç¤ºè¯æ•°é‡è¾ƒå°‘ã€‚å®é™…è¯„ä¼°æ—¶åº”ä½¿ç”¨æ›´å¤šæ ·åŒ–ä¸”æ•°é‡æ›´å¤§çš„æç¤ºè¯é›†ã€‚

> [!WARNING]
> è¯¥åˆ†æ•°å­˜åœ¨å›ºæœ‰å±€é™æ€§ï¼šè®­ç»ƒæ•°æ®ä¸­çš„æ ‡é¢˜æ˜¯ä»ç½‘ç»œçˆ¬å–ï¼Œå¹¶æå–è‡ªå›¾ç‰‡å…³è”çš„`alt`ç­‰æ ‡ç­¾ã€‚è¿™äº›æè¿°æœªå¿…ç¬¦åˆäººç±»æè¿°å›¾åƒçš„æ–¹å¼ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦äººå·¥"è®¾è®¡"éƒ¨åˆ†æç¤ºè¯ã€‚

### å›¾åƒæ¡ä»¶å¼æ–‡æœ¬ç”Ÿæˆå›¾åƒ

è¿™ç§æƒ…å†µä¸‹ï¼Œç”Ÿæˆç®¡é“åŒæ—¶æ¥å—è¾“å…¥å›¾åƒå’Œæ–‡æœ¬æç¤ºä½œä¸ºæ¡ä»¶ã€‚ä»¥[`StableDiffusionInstructPix2PixPipeline`]ä¸ºä¾‹ï¼Œè¯¥ç®¡é“æ¥æ”¶ç¼–è¾‘æŒ‡ä»¤ä½œä¸ºè¾“å…¥æç¤ºï¼Œå¹¶æ¥å—å¾…ç¼–è¾‘çš„è¾“å…¥å›¾åƒã€‚

ç¤ºä¾‹å›¾ç¤ºï¼š

![ç¼–è¾‘æŒ‡ä»¤](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/edit-instruction.png)

è¯„ä¼°æ­¤ç±»æ¨¡å‹çš„ç­–ç•¥ä¹‹ä¸€æ˜¯æµ‹é‡ä¸¤å¹…å›¾åƒé—´å˜åŒ–çš„è¿è´¯æ€§ï¼ˆé€šè¿‡[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)å®šä¹‰ï¼‰ä¸­ä¸¤ä¸ªå›¾åƒä¹‹é—´çš„å˜åŒ–ä¸ä¸¤ä¸ªå›¾åƒæè¿°ä¹‹é—´çš„å˜åŒ–çš„ä¸€è‡´æ€§ï¼ˆå¦‚è®ºæ–‡[ã€ŠCLIP-Guided Domain Adaptation of Image Generatorsã€‹](https://huggingface.co/papers/2108.00946)æ‰€ç¤ºï¼‰ã€‚è¿™è¢«ç§°ä¸ºâ€œ**CLIPæ–¹å‘ç›¸ä¼¼åº¦**â€ã€‚  

- **æè¿°1**å¯¹åº”è¾“å…¥å›¾åƒï¼ˆå›¾åƒ1ï¼‰ï¼Œå³å¾…ç¼–è¾‘çš„å›¾åƒã€‚  
- **æè¿°2**å¯¹åº”ç¼–è¾‘åçš„å›¾åƒï¼ˆå›¾åƒ2ï¼‰ï¼Œåº”åæ˜ ç¼–è¾‘æŒ‡ä»¤ã€‚  

ä»¥ä¸‹æ˜¯ç¤ºæ„å›¾ï¼š  

![edit-consistency](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/edit-consistency.png)  

æˆ‘ä»¬å‡†å¤‡äº†ä¸€ä¸ªå°å‹æ•°æ®é›†æ¥å®ç°è¯¥æŒ‡æ ‡ã€‚é¦–å…ˆåŠ è½½æ•°æ®é›†ï¼š  

```python
from datasets import load_dataset

dataset = load_dataset("sayakpaul/instructpix2pix-demo", split="train")
dataset.features
```  

```bash
{'input': Value(dtype='string', id=None),
 'edit': Value(dtype='string', id=None),
 'output': Value(dtype='string', id=None),
 'image': Image(decode=True, id=None)}
```  

æ•°æ®å­—æ®µè¯´æ˜ï¼š  

- `input`ï¼šä¸`image`å¯¹åº”çš„åŸå§‹æè¿°ã€‚  
- `edit`ï¼šç¼–è¾‘æŒ‡ä»¤ã€‚  
- `output`ï¼šåæ˜ `edit`æŒ‡ä»¤çš„ä¿®æ”¹åæè¿°ã€‚  

æŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬ï¼š  

```python
idx = 0
print(f"Original caption: {dataset[idx]['input']}")
print(f"Edit instruction: {dataset[idx]['edit']}")
print(f"Modified caption: {dataset[idx]['output']}")
```  

```bash
Original caption: 2. FAROE ISLANDS: An archipelago of 18 mountainous isles in the North Atlantic Ocean between Norway and Iceland, the Faroe Islands has 'everything you could hope for', according to Big 7 Travel. It boasts 'crystal clear waterfalls, rocky cliffs that seem to jut out of nowhere and velvety green hills'
Edit instruction: make the isles all white marble
Modified caption: 2. WHITE MARBLE ISLANDS: An archipelago of 18 mountainous white marble isles in the North Atlantic Ocean between Norway and Iceland, the White Marble Islands has 'everything you could hope for', according to Big 7 Travel. It boasts 'crystal clear waterfalls, rocky cliffs that seem to jut out of nowhere and velvety green hills'
```  

å¯¹åº”çš„å›¾åƒï¼š  

```python
dataset[idx]["image"]
```  

![edit-dataset](https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/edit-dataset.png)  

æˆ‘ä»¬å°†æ ¹æ®ç¼–è¾‘æŒ‡ä»¤ä¿®æ”¹æ•°æ®é›†ä¸­çš„å›¾åƒï¼Œå¹¶è®¡ç®—æ–¹å‘ç›¸ä¼¼åº¦ã€‚  

é¦–å…ˆåŠ è½½[`StableDiffusionInstructPix2PixPipeline`]ï¼š  

```python
from diffusers import StableDiffusionInstructPix2PixPipeline

instruct_pix2pix_pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(
    "timbrooks/instruct-pix2pix", torch_dtype=torch.float16
).to("cuda")
```  

æ‰§è¡Œç¼–è¾‘æ“ä½œï¼š  

```python
import numpy as np


def edit_image(input_image, instruction):
    image = instruct_pix2pix_pipeline(
        instruction,
        image=input_image,
        output_type="np",
        generator=generator,
    ).images[0]
    return image

input_images = []
original_captions = []
modified_captions = []
edited_images = []

for idx in range(len(dataset)):
    input_image = dataset[idx]["image"]
    edit_instruction = dataset[idx]["edit"]
    edited_image = edit_image(input_image, edit_instruction)

    input_images.append(np.array(input_image))
    original_captions.append(dataset[idx]["input"])
    modified_captions.append(dataset[idx]["output"])
    edited_images.append(edited_image)
```

ä¸ºæµ‹é‡æ–¹å‘ç›¸ä¼¼åº¦ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½CLIPçš„å›¾åƒå’Œæ–‡æœ¬ç¼–ç å™¨ï¼š

```python
from transformers import (
    CLIPTokenizer,
    CLIPTextModelWithProjection,
    CLIPVisionModelWithProjection,
    CLIPImageProcessor,
)

clip_id = "openai/clip-vit-large-patch14"
tokenizer = CLIPTokenizer.from_pretrained(clip_id)
text_encoder = CLIPTextModelWithProjection.from_pretrained(clip_id).to("cuda")
image_processor = CLIPImageProcessor.from_pretrained(clip_id)
image_encoder = CLIPVisionModelWithProjection.from_pretrained(clip_id).to("cuda")
```

æ³¨æ„æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ç‰¹å®šCLIPæ£€æŸ¥ç‚¹â€”â€”`openai/clip-vit-large-patch14`ï¼Œå› ä¸ºStable Diffusioné¢„è®­ç»ƒæ­£æ˜¯åŸºäºæ­¤CLIPå˜ä½“ã€‚è¯¦è§[æ–‡æ¡£](https://huggingface.co/docs/transformers/model_doc/clip)ã€‚

æ¥ç€å‡†å¤‡è®¡ç®—æ–¹å‘ç›¸ä¼¼åº¦çš„PyTorch `nn.Module`ï¼š

```python
import torch.nn as nn
import torch.nn.functional as F


class DirectionalSimilarity(nn.Module):
    def __init__(self, tokenizer, text_encoder, image_processor, image_encoder):
        super().__init__()
        self.tokenizer = tokenizer
        self.text_encoder = text_encoder
        self.image_processor = image_processor
        self.image_encoder = image_encoder

    def preprocess_image(self, image):
        image = self.image_processor(image, return_tensors="pt")["pixel_values"]
        return {"pixel_values": image.to("cuda")}

    def tokenize_text(self, text):
        inputs = self.tokenizer(
            text,
            max_length=self.tokenizer.model_max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        return {"input_ids": inputs.input_ids.to("cuda")}

    def encode_image(self, image):
        preprocessed_image = self.preprocess_image(image)
        image_features = self.image_encoder(**preprocessed_image).image_embeds
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        return image_features

    def encode_text(self, text):
        tokenized_text = self.tokenize_text(text)
        text_features = self.text_encoder(**tokenized_text).text_embeds
        text_features = text_features / text_features.norm(dim=1, keepdim=True)
        return text_features

    def compute_directional_similarity(self, img_feat_one, img_feat_two, text_feat_one, text_feat_two):
        sim_direction = F.cosine_similarity(img_feat_two - img_feat_one, text_feat_two - text_feat_one)
        return sim_direction

    def forward(self, image_one, image_two, caption_one, caption_two):
        img_feat_one = self.encode_image(image_one)
        img_feat_two = self.encode_image(image_two)
        text_feat_one = self.encode_text(caption_one)
        text_feat_two = self.encode_text(caption_two)
        directional_similarity = self.compute_directional_similarity(
            img_feat_one, img_feat_two, text_feat_one, text_feat_two
        )
        return directional_similarity
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨`DirectionalSimilarity`æ¨¡å—ï¼š

```python
dir_similarity = DirectionalSimilarity(tokenizer, text_encoder, image_processor, image_encoder)
scores = []

for i in range(len(input_images)):
    original_image = input_images[i]
    original_caption = original_captions[i]
    edited_image = edited_images[i]
    modified_caption = modified_captions[i]

    similarity_score = dir_similarity(original_image, edited_image, original_caption, modified_caption)
    scores.append(float(similarity_score.detach().cpu()))

print(f"CLIPæ–¹å‘ç›¸ä¼¼åº¦: {np.mean(scores)}")
# CLIPæ–¹å‘ç›¸ä¼¼åº¦: 0.0797976553440094
```

ä¸CLIPåˆ†æ•°ç±»ä¼¼ï¼ŒCLIPæ–¹å‘ç›¸ä¼¼åº¦æ•°å€¼è¶Šé«˜è¶Šå¥½ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`StableDiffusionInstructPix2PixPipeline`æä¾›äº†ä¸¤ä¸ªæ§åˆ¶å‚æ•°`image_guidance_scale`å’Œ`guidance_scale`æ¥è°ƒèŠ‚æœ€ç»ˆç¼–è¾‘å›¾åƒçš„è´¨é‡ã€‚å»ºè®®æ‚¨å°è¯•è°ƒæ•´è¿™ä¸¤ä¸ªå‚æ•°ï¼Œè§‚å¯Ÿå®ƒä»¬å¯¹æ–¹å‘ç›¸ä¼¼åº¦çš„å½±å“ã€‚

æˆ‘ä»¬å¯ä»¥æ‰©å±•è¿™ä¸ªåº¦é‡æ ‡å‡†æ¥è¯„ä¼°åŸå§‹å›¾åƒä¸ç¼–è¾‘ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦ï¼Œåªéœ€è®¡ç®—`F.cosine_similarity(img_feat_two, img_feat_one)`ã€‚å¯¹äºè¿™ç±»ç¼–è¾‘ä»»åŠ¡ï¼Œæˆ‘ä»¬ä»å¸Œæœ›å°½å¯èƒ½ä¿ç•™å›¾åƒçš„ä¸»è¦è¯­ä¹‰ç‰¹å¾ï¼ˆå³ä¿æŒè¾ƒé«˜çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰ã€‚

è¯¥åº¦é‡æ–¹æ³•åŒæ ·é€‚ç”¨äºç±»ä¼¼æµç¨‹ï¼Œä¾‹å¦‚[`StableDiffusionPix2PixZeroPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/pix2pix_zero#diffusers.StableDiffusionPix2PixZeroPipeline)ã€‚

> [!TIP]
> CLIPåˆ†æ•°å’ŒCLIPæ–¹å‘ç›¸ä¼¼åº¦éƒ½ä¾èµ–CLIPæ¨¡å‹ï¼Œå¯èƒ½å¯¼è‡´è¯„ä¼°ç»“æœå­˜åœ¨åå·®ã€‚

***æ‰©å±•ISã€FIDï¼ˆåæ–‡è®¨è®ºï¼‰æˆ–KIDç­‰æŒ‡æ ‡å­˜åœ¨å›°éš¾***ï¼Œå½“è¢«è¯„ä¼°æ¨¡å‹æ˜¯åœ¨å¤§å‹å›¾æ–‡æ•°æ®é›†ï¼ˆå¦‚[LAION-5Bæ•°æ®é›†](https://laion.ai/blog/laion-5b/)ï¼‰ä¸Šé¢„è®­ç»ƒæ—¶ã€‚å› ä¸ºè¿™äº›æŒ‡æ ‡çš„åº•å±‚éƒ½ä½¿ç”¨äº†åœ¨ImageNet-1kæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„InceptionNetæ¥æå–å›¾åƒç‰¹å¾ã€‚Stable Diffusionçš„é¢„è®­ç»ƒæ•°æ®é›†ä¸InceptionNetçš„é¢„è®­ç»ƒæ•°æ®é›†å¯èƒ½é‡å æœ‰é™ï¼Œå› æ­¤ä¸é€‚åˆä½œä¸ºç‰¹å¾æå–å™¨ã€‚

***ä¸Šè¿°æŒ‡æ ‡æ›´é€‚åˆè¯„ä¼°ç±»åˆ«æ¡ä»¶æ¨¡å‹***ï¼Œä¾‹å¦‚[DiT](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit)ã€‚è¯¥æ¨¡å‹æ˜¯åœ¨ImageNet-1kç±»åˆ«æ¡ä»¶ä¸‹é¢„è®­ç»ƒçš„ã€‚
è¿™æ˜¯9ç¯‡æ–‡æ¡£ä¸­çš„ç¬¬8éƒ¨åˆ†ã€‚

### åŸºäºç±»åˆ«çš„å›¾åƒç”Ÿæˆ

åŸºäºç±»åˆ«çš„ç”Ÿæˆæ¨¡å‹é€šå¸¸æ˜¯åœ¨å¸¦æœ‰ç±»åˆ«æ ‡ç­¾çš„æ•°æ®é›†ï¼ˆå¦‚[ImageNet-1k](https://huggingface.co/datasets/imagenet-1k)ï¼‰ä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ã€‚è¯„ä¼°è¿™äº›æ¨¡å‹çš„å¸¸ç”¨æŒ‡æ ‡åŒ…æ‹¬FrÃ©chet Inception Distanceï¼ˆFIDï¼‰ã€Kernel Inception Distanceï¼ˆKIDï¼‰å’ŒInception Scoreï¼ˆISï¼‰ã€‚æœ¬æ–‡æ¡£é‡ç‚¹ä»‹ç»FIDï¼ˆ[Heuselç­‰äºº](https://huggingface.co/papers/1706.08500)ï¼‰ï¼Œå¹¶å±•ç¤ºå¦‚ä½•ä½¿ç”¨[`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)è®¡ç®—è¯¥æŒ‡æ ‡ï¼Œè¯¥ç®¡é“åº•å±‚ä½¿ç”¨äº†[DiTæ¨¡å‹](https://huggingface.co/papers/2212.09748)ã€‚

FIDæ—¨åœ¨è¡¡é‡ä¸¤ç»„å›¾åƒæ•°æ®é›†çš„ç›¸ä¼¼ç¨‹åº¦ã€‚æ ¹æ®[æ­¤èµ„æº](https://mmgeneration.readthedocs.io/en/latest/quick_run.html#fid)ï¼š

> FrÃ©chet Inception Distanceæ˜¯è¡¡é‡ä¸¤ç»„å›¾åƒæ•°æ®é›†ç›¸ä¼¼åº¦çš„æŒ‡æ ‡ã€‚ç ”ç©¶è¡¨æ˜å…¶ä¸äººç±»å¯¹è§†è§‰è´¨é‡çš„ä¸»è§‚åˆ¤æ–­é«˜åº¦ç›¸å…³ï¼Œå› æ­¤æœ€å¸¸ç”¨äºè¯„ä¼°ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”Ÿæˆæ ·æœ¬çš„è´¨é‡ã€‚FIDé€šè¿‡è®¡ç®—Inceptionç½‘ç»œç‰¹å¾è¡¨ç¤ºæ‰€æ‹Ÿåˆçš„ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„FrÃ©chetè·ç¦»æ¥å®ç°ã€‚

è¿™ä¸¤ä¸ªæ•°æ®é›†æœ¬è´¨ä¸Šæ˜¯çœŸå®å›¾åƒæ•°æ®é›†å’Œç”Ÿæˆå›¾åƒæ•°æ®é›†ï¼ˆæœ¬ä¾‹ä¸­ä¸ºäººå·¥ç”Ÿæˆçš„å›¾åƒï¼‰ã€‚FIDé€šå¸¸åŸºäºä¸¤ä¸ªå¤§å‹æ•°æ®é›†è®¡ç®—ï¼Œä½†æœ¬æ–‡æ¡£å°†ä½¿ç”¨ä¸¤ä¸ªå°å‹æ•°æ®é›†è¿›è¡Œæ¼”ç¤ºã€‚

é¦–å…ˆä¸‹è½½ImageNet-1kè®­ç»ƒé›†ä¸­çš„éƒ¨åˆ†å›¾åƒï¼š

```python
from zipfile import ZipFile
import requests


def download(url, local_filepath):
    r = requests.get(url)
    with open(local_filepath, "wb") as f:
        f.write(r.content)
    return local_filepath

dummy_dataset_url = "https://hf.co/datasets/sayakpaul/sample-datasets/resolve/main/sample-imagenet-images.zip"
local_filepath = download(dummy_dataset_url, dummy_dataset_url.split("/")[-1])

with ZipFile(local_filepath, "r") as zipper:
    zipper.extractall(".")
```

```python
from PIL import Image
import os
import numpy as np

dataset_path = "sample-imagenet-images"
image_paths = sorted([os.path.join(dataset_path, x) for x in os.listdir(dataset_path)])

real_images = [np.array(Image.open(path).convert("RGB")) for path in image_paths]
```

è¿™äº›æ˜¯æ¥è‡ªä»¥ä¸‹ImageNet-1kç±»åˆ«çš„10å¼ å›¾åƒï¼š"cassette_player"ã€"chain_saw"ï¼ˆ2å¼ ï¼‰ã€"church"ã€"gas_pump"ï¼ˆ3å¼ ï¼‰ã€"parachute"ï¼ˆ2å¼ ï¼‰å’Œ"tench"ã€‚

<p align="center">
    <img src="https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/real-images.png" alt="çœŸå®å›¾åƒ"><br>
    <em>çœŸå®å›¾åƒ</em>
</p>

åŠ è½½å›¾åƒåï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œè½»é‡çº§é¢„å¤„ç†ä»¥ä¾¿ç”¨äºFIDè®¡ç®—ï¼š

```python
from torchvision.transforms import functional as F
import torch


def preprocess_image(image):
    image = torch.tensor(image).unsqueeze(0)
    image = image.permute(0, 3, 1, 2) / 255.0
    return F.center_crop(image, (256, 256))

real_images = torch.stack([dit_pipeline.preprocess_image(image) for image in real_images])
print(real_images.shape)
# torch.Size([10, 3, 256, 256])
```

æˆ‘ä»¬ç°åœ¨åŠ è½½[`DiTPipeline`](https://huggingface.co/docs/diffusers/api/pipelines/dit)æ¥ç”ŸæˆåŸºäºä¸Šè¿°ç±»åˆ«çš„æ¡ä»¶å›¾åƒã€‚

```python
from diffusers import DiTPipeline, DPMSolverMultistepScheduler

dit_pipeline = DiTPipeline.from_pretrained("facebook/DiT-XL-2-256", torch_dtype=torch.float16)
dit_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(dit_pipeline.scheduler.config)
dit_pipeline = dit_pipeline.to("cuda")

seed = 0
generator = torch.manual_seed(seed)


words = [
    "cassette player",
    "chainsaw",
    "chainsaw",
    "church",
    "gas pump",
    "gas pump",
    "gas pump",
    "parachute",
    "parachute",
    "tench",
]

class_ids = dit_pipeline.get_label_ids(words)
output = dit_pipeline(class_labels=class_ids, generator=generator, output_type="np")

fake_images = output.images
fake_images = torch.tensor(fake_images)
fake_images = fake_images.permute(0, 3, 1, 2)
print(fake_images.shape)
# torch.Size([10, 3, 256, 256])
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨[`torchmetrics`](https://torchmetrics.readthedocs.io/)è®¡ç®—FIDåˆ†æ•°ã€‚

```python
from torchmetrics.image.fid import FrechetInceptionDistance

fid = FrechetInceptionDistance(normalize=True)
fid.update(real_images, real=True)
fid.update(fake_images, real=False)

print(f"FIDåˆ†æ•°: {float(fid.compute())}")
# FIDåˆ†æ•°: 177.7147216796875
```

FIDåˆ†æ•°è¶Šä½è¶Šå¥½ã€‚ä»¥ä¸‹å› ç´ ä¼šå½±å“FIDç»“æœï¼š

- å›¾åƒæ•°é‡ï¼ˆåŒ…æ‹¬çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒï¼‰
- æ‰©æ•£è¿‡ç¨‹ä¸­å¼•å…¥çš„éšæœºæ€§
- æ‰©æ•£è¿‡ç¨‹çš„æ¨ç†æ­¥æ•°
- æ‰©æ•£è¿‡ç¨‹ä¸­ä½¿ç”¨çš„è°ƒåº¦å™¨

å¯¹äºæœ€åä¸¤ç‚¹ï¼Œæœ€ä½³å®è·µæ˜¯ä½¿ç”¨ä¸åŒçš„éšæœºç§å­å’Œæ¨ç†æ­¥æ•°è¿›è¡Œå¤šæ¬¡è¯„ä¼°ï¼Œç„¶åæŠ¥å‘Šå¹³å‡ç»“æœã€‚

> [!WARNING]
> FIDç»“æœå¾€å¾€å…·æœ‰è„†å¼±æ€§ï¼Œå› ä¸ºå®ƒä¾èµ–äºè®¸å¤šå› ç´ ï¼š
>
> * è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨çš„ç‰¹å®šInceptionæ¨¡å‹
> * è®¡ç®—å®ç°çš„å‡†ç¡®æ€§
> * å›¾åƒæ ¼å¼ï¼ˆPNGå’ŒJPGçš„èµ·ç‚¹ä¸åŒï¼‰
>
> éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒFIDé€šå¸¸åœ¨æ¯”è¾ƒç›¸ä¼¼å®éªŒæ—¶æœ€æœ‰ç”¨ï¼Œä½†é™¤éä½œè€…ä»”ç»†å…¬å¼€FIDæµ‹é‡ä»£ç ï¼Œå¦åˆ™å¾ˆéš¾å¤ç°è®ºæ–‡ç»“æœã€‚
>
> è¿™äº›æ³¨æ„äº‹é¡¹åŒæ ·é€‚ç”¨äºå…¶ä»–ç›¸å…³æŒ‡æ ‡ï¼Œå¦‚KIDå’ŒISã€‚

æœ€åï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–æ£€æŸ¥è¿™äº›`fake_images`ã€‚

<p align="center">
    <img src="https://huggingface.co/datasets/diffusers/docs-images/resolve/main/evaluation_diffusion_models/fake-images.png" alt="ç”Ÿæˆå›¾åƒ"><br>
    <em>ç”Ÿæˆå›¾åƒç¤ºä¾‹</em>
</p>
