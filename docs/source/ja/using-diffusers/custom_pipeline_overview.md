<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®èª­ã¿è¾¼ã¿

[[open-in-colab]]

## ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€ä»»æ„ã® [`DiffusionPipeline`] ã‚¯ãƒ©ã‚¹ã§ã‚ã‚Šã€è«–æ–‡ã§ææ¡ˆã•ã‚ŒãŸã‚ªãƒªã‚¸ãƒŠãƒ«ã®å®Ÿè£…ã¨ã¯ç•°ãªã‚Šã¾ã™ã€‚
(ãŸã¨ãˆã°ã€ [`StableDiffusionControlNetPipeline`] ã¯ [Text-to-Image Generation with ControlNet Conditioning](https://arxiv.org/abs/2302.05543) ã«å¯¾å¿œã—ã¾ã™ã€‚)
ãã‚Œã‚‰ã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å…ƒã®å®Ÿè£…ã‚’æ‹¡å¼µã—ãŸã‚Šã€è¿½åŠ æ©Ÿèƒ½ã‚’æä¾›ã—ãŸã‚Šã—ã¾ã™ã€‚

[Speech to Image](https://github.com/huggingface/diffusers/tree/main/examples/community#speech-to-image) ã‚„ [Composable Stable Diffusion](https://github.com/huggingface/diffusers/tree/main/examples/community#composable-stable-diffusion) ã®ã‚ˆã†ãªå¤šãã®ç´ æ™´ã‚‰ã—ã„ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚
ã™ã¹ã¦ã®å…¬å¼ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯[ã“ã“](https://github.com/huggingface/diffusers/tree/main/examples/community)ã‹ã‚‰ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ Hub ã«èª­ã¿è¾¼ã‚€ãŸã‚ã«ã¯ã€`custom_pipeline` å¼•æ•°ã«ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒªãƒã‚¸ãƒˆãƒª ID ã‚’æ¸¡ã—ã€
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é‡ã¿ã¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€ãƒ¢ãƒ‡ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã‚’æŒ‡å®šã—ã¾ã™ã€‚
ãŸã¨ãˆã°ã€ä»¥ä¸‹ã®ä¾‹ã§ã¯ [`hf-internal-testing/diffusers-dummy-pipeline`](https://huggingface.co/hf-internal-testing/diffusers-dummy-pipeline/blob/main/pipeline.py) ã‹ã‚‰ãƒ€ãƒŸãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’èª­ã¿è¾¼ã¿ã€
[`google/ddpm-cifar10-32`](https://huggingface.co/google/ddpm-cifar10-32) 


<Tip warning={true}>

ğŸ”’ Hugging Face Hub ã‹ã‚‰ã‚³ãƒŸãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã§ã€èª­ã¿è¾¼ã‚€ã‚³ãƒ¼ãƒ‰ãŒå®‰å…¨ã§ã‚ã‚‹ã“ã¨ã‚’ä¿¡é ¼ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å®Ÿè¡Œã™ã‚‹å‰ã«å¿…ãšã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼

</Tip>

```py
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained(
    "google/ddpm-cifar10-32", custom_pipeline="hf-internal-testing/diffusers-dummy-pipeline", use_safetensors=True
)
```

å…¬å¼ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã‚‚åŒæ§˜ã§ã™ãŒã€
å…¬å¼ãƒªãƒã‚¸ãƒˆãƒª ID ã‹ã‚‰é‡ã¿ã‚’èª­ã¿è¾¼ã‚€ã“ã¨ã¨ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç›´æ¥æ¸¡ã™ã“ã¨ã¯ä½µç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã® [CLIP Guided Stable Diffusion](https://github.com/huggingface/diffusers/tree/main/examples/community#clip-guided-stable-diffusion) ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’èª­ã¿è¾¼ã¿ã€
CLIP ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç›´æ¥æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™:

```py
from diffusers import DiffusionPipeline
from transformers import CLIPImageProcessor, CLIPModel

clip_model_id = "laion/CLIP-ViT-B-32-laion2B-s34B-b79K"

feature_extractor = CLIPImageProcessor.from_pretrained(clip_model_id)
clip_model = CLIPModel.from_pretrained(clip_model_id)

pipeline = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    custom_pipeline="clip_guided_stable_diffusion",
    clip_model=clip_model,
    feature_extractor=feature_extractor,
    use_safetensors=True,
)
```

ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](custom_pipeline_examples) ã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€ã‚³ãƒŸãƒ¥ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¿½åŠ ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€[ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¿½åŠ æ–¹æ³•](contribute_pipeline) ã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼

## ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚ˆã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ã¯ Diffusers ã«ãªã„ã€ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ã‚‚ã—ã€ã‚ãªãŸã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒ Diffusers ãŒã¾ã ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æŒã¤å ´åˆã¯ã€ãã‚Œã‚‰ã®å®Ÿè£…ã‚’ Python ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦æä¾›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã‚Œã‚‰ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã€VAEã€UNetã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒã‚ã‚Šã¾ã™ã€‚
ã»ã¨ã‚“ã©ã®å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚
ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªä½“ã‚‚ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ã§ã™ã€‚

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚

ä¾‹ã¨ã—ã¦ã€[showlab/show-1-base](https://huggingface.co/showlab/show-1-base) ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ã„ã¾ã™ã€‚
ãã‚Œã§ã¯ã€ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã‚’å§‹ã‚ã¾ã—ã‚‡ã†:

1. Transformers ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦èª­ã¿è¾¼ã‚€:

```python
from transformers import T5Tokenizer, T5EncoderModel

pipe_id = "showlab/show-1-base"
tokenizer = T5Tokenizer.from_pretrained(pipe_id, subfolder="tokenizer")
text_encoder = T5EncoderModel.from_pretrained(pipe_id, subfolder="text_encoder")
```

2. ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’èª­ã¿è¾¼ã‚€:

```python
from diffusers import DPMSolverMultistepScheduler

scheduler = DPMSolverMultistepScheduler.from_pretrained(pipe_id, subfolder="scheduler")
```

3. ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’èª­ã¿è¾¼ã‚€:

```python
from transformers import CLIPFeatureExtractor

feature_extractor = CLIPFeatureExtractor.from_pretrained(pipe_id, subfolder="feature_extractor")
```

<Tip warning={true}>

ã‚¹ãƒ†ãƒƒãƒ—4ã¨5ã«ãŠã„ã¦ã€ã‚«ã‚¹ã‚¿ãƒ  [UNet](https://github.com/showlab/Show-1/blob/main/showone/models/unet_3d_condition.py) ã¨[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)ã®å®Ÿè£…ã¯ã€ã“ã®ä¾‹ãŒå‹•ä½œã™ã‚‹ãŸã‚ã«ã€ãã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ä¸€è‡´ã—ã¦ã„ãªã‘ã‚Œã°ã„ã‘ã¾ã›ã‚“ã€‚

</Tip>

4. ã“ã®ä¾‹ã§ã¯ä¾¿å®œä¸Š `showone_unet_3d_condition.py` [ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)ã§æ—¢ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹[ã‚«ã‚¹ã‚¿ãƒ  UNet]((https://github.com/showlab/Show-1/blob/main/showone/models/unet_3d_condition.py)ã‚’èª­ã¿è¾¼ã‚“ã§ã„ãã¾ã—ã‚‡ã†ã€‚`UNet3DConditionModel` ã‚¯ãƒ©ã‚¹ãŒ `ShowOneUNet3DConditionModel` ã‚¯ãƒ©ã‚¹ã«å¤‰æ›´ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ°—ã¥ãã¨æ€ã„ã¾ã™ã€‚ã€€`ShowOneUNet3DConditionModel` ã‚¯ãƒ©ã‚¹ã«å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ `showone_unet_3d_condition.py` ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«é…ç½®ã—ã¦ãã ã•ã„ã€‚ 

ã“ã‚ŒãŒå®Œäº†ã—ãŸã‚‰ã€UNetã‚’åˆæœŸåŒ–ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™:

```python
from showone_unet_3d_condition import ShowOneUNet3DConditionModel

unet = ShowOneUNet3DConditionModel.from_pretrained(pipe_id, subfolder="unet")
```

5. æœ€å¾Œã«ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã®ä¾‹ã§ã¯ã€æ—¢ã« `pipeline_t2v_base_pixel.py`  [ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/pipeline_t2v_base_pixel.py)ã«ä½œã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã¯ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ `TextToVideoIFPipeline` ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã‚«ã‚¹ã‚¿ãƒ  Unet ã®ã‚ˆã†ã«ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«å¿…è¦ãªã‚³ãƒ¼ãƒ‰ã¯ `pipeline_t2v_base_pixel.py` ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

ã™ã¹ã¦ã®æº–å‚™ãŒæ•´ã£ãŸã‚‰ã€`TextToVideoIFPipeline` ã‚’ `ShowOneUNet3DConditionModel` ã§åˆæœŸåŒ–ã—ã¾ã™:

```python
from pipeline_t2v_base_pixel import TextToVideoIFPipeline
import torch

pipeline = TextToVideoIFPipeline(
    unet=unet,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    scheduler=scheduler,
    feature_extractor=feature_extractor
)
pipeline = pipeline.to(device="cuda")
pipeline.torch_dtype = torch.float16
```

ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ Hub ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã—ã¾ã—ã‚‡ã†ï¼

```python
pipeline.push_to_hub("custom-t2v-pipeline")
```

ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ—ãƒƒã‚·ãƒ¥ãŒæˆåŠŸã—ãŸã‚‰ã€ã„ãã¤ã‹ã®å¤‰æ›´ãŒå¿…è¦ã§ã™:

1. [`model_index.json`](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/model_index.json#L2) ã® `_class_name` å±æ€§ã‚’ `"pipeline_t2v_base_pixel"` ã¨ `"TextToVideoIFPipeline"` ã«å¤‰æ›´
2. `showone_unet_3d_condition.py` ã‚’ `unet` [ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py) ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
3. `pipeline_t2v_base_pixel.py` ã‚’ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹[ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª](https://huggingface.co/sayakpaul/show-1-base-with-code/blob/main/unet/showone_unet_3d_condition.py)ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–æ™‚ã« `trust_remote_code` å¼•æ•°ã‚’è¿½åŠ ã™ã‚‹ã ã‘ã§ã€èˆå°è£ã§å…¨ã¦ã®ã€Œãƒã‚¸ãƒƒã‚¯ã€ã‚’å‡¦ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python
from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained(
    "<change-username>/<change-id>", trust_remote_code=True, torch_dtype=torch.float16
).to("cuda")

prompt = "hello"

# ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿
prompt_embeds, negative_embeds = pipeline.encode_prompt(prompt)

# ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã®ç”Ÿæˆ (8x64x40, 2fps)
video_frames = pipeline(
    prompt_embeds=prompt_embeds,
    negative_prompt_embeds=negative_embeds,
    num_frames=8,
    height=40,
    width=64,
    num_inference_steps=2,
    guidance_scale=9.0,
    output_type="pt"
).frames
```

ã•ã‚‰ãªã‚‹å‚è€ƒä¾‹ã¨ã—ã¦ã€`trust_remote_code` ã‚’åˆ©ç”¨ã—ãŸ [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl/) ã®ãƒªãƒã‚¸ãƒˆãƒªæ§‹é€ ã‚’å‚ç…§ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

```python

from diffusers import DiffusionPipeline
import torch

pipeline = DiffusionPipeline.from_pretrained(
    "stabilityai/japanese-stable-diffusion-xl", trust_remote_code=True
)
pipeline.to("cuda")

# if using torch < 2.0
# pipeline.enable_xformers_memory_efficient_attention()

prompt = "æŸ´çŠ¬ã€ã‚«ãƒ©ãƒ•ãƒ«ã‚¢ãƒ¼ãƒˆ"

image = pipeline(prompt=prompt).images[0]

```