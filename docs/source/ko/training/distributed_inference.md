# ì—¬ëŸ¬ GPUë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° ì¶”ë¡ 

ë¶„ì‚° ì„¤ì •ì—ì„œëŠ” ì—¬ëŸ¬ ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì‹œì— ìƒì„±í•  ë•Œ ìœ ìš©í•œ ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index) ë˜ëŠ” [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ GPUì—ì„œ ì¶”ë¡ ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” ë¶„ì‚° ì¶”ë¡ ì„ ìœ„í•´ ğŸ¤— Accelerateì™€ PyTorch Distributedë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.

## ğŸ¤— Accelerate

ğŸ¤— [Accelerate](https://huggingface.co/docs/accelerate/index)ëŠ” ë¶„ì‚° ì„¤ì •ì—ì„œ ì¶”ë¡ ì„ ì‰½ê²Œ í›ˆë ¨í•˜ê±°ë‚˜ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ë¶„ì‚° í™˜ê²½ ì„¤ì • í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ì—¬ PyTorch ì½”ë“œì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤.

ì‹œì‘í•˜ë ¤ë©´ Python íŒŒì¼ì„ ìƒì„±í•˜ê³  [`accelerate.PartialState`]ë¥¼ ì´ˆê¸°í™”í•˜ì—¬ ë¶„ì‚° í™˜ê²½ì„ ìƒì„±í•˜ë©´, ì„¤ì •ì´ ìë™ìœ¼ë¡œ ê°ì§€ë˜ë¯€ë¡œ `rank` ë˜ëŠ” `world_size`ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ['DiffusionPipeline`]ì„ `distributed_state.device`ë¡œ ì´ë™í•˜ì—¬ ê° í”„ë¡œì„¸ìŠ¤ì— GPUë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.

ì´ì œ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ìë¡œ [`~accelerate.PartialState.split_between_processes`] ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ìˆ˜ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¶„ë°°í•©ë‹ˆë‹¤.


```py
from accelerate import PartialState
from diffusers import DiffusionPipeline

pipeline = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
distributed_state = PartialState()
pipeline.to(distributed_state.device)

with distributed_state.split_between_processes(["a dog", "a cat"]) as prompt:
    result = pipeline(prompt).images[0]
    result.save(f"result_{distributed_state.process_index}.png")
```

Use the `--num_processes` argument to specify the number of GPUs to use, and call `accelerate launch` to run the script:

```bash
accelerate launch run_distributed.py --num_processes=2
```

<Tip>ìì„¸í•œ ë‚´ìš©ì€ [ğŸ¤— Accelerateë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° ì¶”ë¡ ](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate) ê°€ì´ë“œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

</Tip>

## Pytoerch ë¶„ì‚°

PyTorchëŠ” ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì‹œì‘í•˜ë ¤ë©´ Python íŒŒì¼ì„ ìƒì„±í•˜ê³  `torch.distributed` ë° `torch.multiprocessing`ì„ ì„í¬íŠ¸í•˜ì—¬ ë¶„ì‚° í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ì„¤ì •í•˜ê³  ê° GPUì—ì„œ ì¶”ë¡ ìš© í”„ë¡œì„¸ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  [`DiffusionPipeline`]ë„ ì´ˆê¸°í™”í•´ì•¼ í•©ë‹ˆë‹¤:

í™•ì‚° íŒŒì´í”„ë¼ì¸ì„ `rank`ë¡œ ì´ë™í•˜ê³  `get_rank`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í”„ë¡œì„¸ìŠ¤ì— GPUë¥¼ í• ë‹¹í•˜ë©´ ê° í”„ë¡œì„¸ìŠ¤ê°€ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:

```py
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

from diffusers import DiffusionPipeline

sd = DiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16)
```

ì‚¬ìš©í•  ë°±ì—”ë“œ ìœ í˜•, í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ `rank`, `world_size` ë˜ëŠ” ì°¸ì—¬í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ìˆ˜ë¡œ ë¶„ì‚° í™˜ê²½ ìƒì„±ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜[`init_process_group`]ë¥¼ ë§Œë“¤ì–´ ì¶”ë¡ ì„ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

2ê°œì˜ GPUì—ì„œ ì¶”ë¡ ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” ê²½ìš° `world_size`ëŠ” 2ì…ë‹ˆë‹¤.

```py
def run_inference(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

    sd.to(rank)

    if torch.distributed.get_rank() == 0:
        prompt = "a dog"
    elif torch.distributed.get_rank() == 1:
        prompt = "a cat"

    image = sd(prompt).images[0]
    image.save(f"./{'_'.join(prompt)}.png")
```

ë¶„ì‚° ì¶”ë¡ ì„ ì‹¤í–‰í•˜ë ¤ë©´ [`mp.spawn`](https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn)ì„ í˜¸ì¶œí•˜ì—¬ `world_size`ì— ì •ì˜ëœ GPU ìˆ˜ì— ëŒ€í•´ `run_inference` í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

```py
def main():
    world_size = 2
    mp.spawn(run_inference, args=(world_size,), nprocs=world_size, join=True)


if __name__ == "__main__":
    main()
```

ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì™„ë£Œí–ˆìœ¼ë©´ `--nproc_per_node` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©í•  GPU ìˆ˜ë¥¼ ì§€ì •í•˜ê³  `torchrun`ì„ í˜¸ì¶œí•˜ì—¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤:

```bash
torchrun run_distributed.py --nproc_per_node=2
```