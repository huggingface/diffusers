<!--Copyright 2025 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# ControlNet

[Adding Conditional Control to Text-to-Image Diffusion Models](https://huggingface.co/papers/2302.05543) (ControlNet)ì€ Lvmin Zhangê³¼ Maneesh Agrawalaì— ì˜í•´ ì“°ì—¬ì¡ŒìŠµë‹ˆë‹¤.

ì´ ì˜ˆì‹œëŠ” [ì›ë³¸ ControlNet ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ì˜ˆì‹œ í•™ìŠµí•˜ê¸°](https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md)ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ControlNetì€ ì›ë“¤ì„ ì±„ìš°ê¸° ìœ„í•´ [small synthetic dataset](https://huggingface.co/datasets/fusing/fill50k)ì„ ì‚¬ìš©í•´ì„œ í•™ìŠµë©ë‹ˆë‹¤.

## ì˜ì¡´ì„± ì„¤ì¹˜í•˜ê¸°

ì•„ë˜ì˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì—, ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í•™ìŠµ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

> [!WARNING]
> ê°€ì¥ ìµœì‹  ë²„ì „ì˜ ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ”, ì†ŒìŠ¤ì—ì„œ ì„¤ì¹˜í•˜ê³  ìµœì‹  ë²„ì „ì˜ ì„¤ì¹˜ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì„ ê°•ë ¥í•˜ê²Œ ì¶”ì²œí•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì˜ˆì‹œ ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ìì£¼ ì—…ë°ì´íŠ¸í•˜ê³  ì˜ˆì‹œì— ë§ì¶˜ íŠ¹ì •í•œ ìš”êµ¬ì‚¬í•­ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.

ìœ„ ì‚¬í•­ì„ ë§Œì¡±ì‹œí‚¤ê¸° ìœ„í•´ì„œ, ìƒˆë¡œìš´ ê°€ìƒí™˜ê²½ì—ì„œ ë‹¤ìŒ ì¼ë ¨ì˜ ìŠ¤í…ì„ ì‹¤í–‰í•˜ì„¸ìš”:

```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install -e .
```

ê·¸ ë‹¤ìŒì—ëŠ” [ì˜ˆì‹œ í´ë”](https://github.com/huggingface/diffusers/tree/main/examples/controlnet)ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.

```bash
cd examples/controlnet
```

ì´ì œ ì‹¤í–‰í•˜ì„¸ìš”:

```bash
pip install -r requirements.txt
```

[ğŸ¤—Accelerate](https://github.com/huggingface/accelerate/) í™˜ê²½ì„ ì´ˆê¸°í™” í•©ë‹ˆë‹¤:

```bash
accelerate config
```

í˜¹ì€ ì—¬ëŸ¬ë¶„ì˜ í™˜ê²½ì´ ë¬´ì—‡ì¸ì§€ ëª°ë¼ë„ ê¸°ë³¸ì ì¸ ğŸ¤—Accelerate êµ¬ì„±ìœ¼ë¡œ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
accelerate config default
```

í˜¹ì€ ë‹¹ì‹ ì˜ í™˜ê²½ì´ ë…¸íŠ¸ë¶ ê°™ì€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì‰˜ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì•„ë˜ì˜ ì½”ë“œë¡œ ì´ˆê¸°í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from accelerate.utils import write_basic_config

write_basic_config()
```

ìì²´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” [í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„±í•˜ê¸°](create_dataset) ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.

## í•™ìŠµ

ì´ í•™ìŠµì— ì‚¬ìš©ë  ë‹¤ìŒ ì´ë¯¸ì§€ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:

```sh
wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png

wget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png
```

`MODEL_NAME` í™˜ê²½ ë³€ìˆ˜ (Hub ëª¨ë¸ ë¦¬í¬ì§€í† ë¦¬ ì•„ì´ë”” í˜¹ì€ ëª¨ë¸ ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ë””ë ‰í† ë¦¬ë¡œ ê°€ëŠ” ì£¼ì†Œ)ë¥¼ ëª…ì‹œí•˜ê³  [`pretrained_model_name_or_path`](https://huggingface.co/docs/diffusers/en/api/diffusion_pipeline#diffusers.DiffusionPipeline.from_pretrained.pretrained_model_name_or_path) ì¸ìë¡œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¹ì‹ ì˜ ë¦¬í¬ì§€í† ë¦¬ì— `diffusion_pytorch_model.bin` íŒŒì¼ì„ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.

```bash
export MODEL_DIR="stable-diffusion-v1-5/stable-diffusion-v1-5"
export OUTPUT_DIR="path to save model"

accelerate launch train_controlnet.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --resolution=512 \
 --learning_rate=1e-5 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --train_batch_size=4 \
 --push_to_hub
```

ì´ ê¸°ë³¸ì ì¸ ì„¤ì •ìœ¼ë¡œëŠ” ~38GB VRAMì´ í•„ìš”í•©ë‹ˆë‹¤.

ê¸°ë³¸ì ìœ¼ë¡œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ëŠ” ê²°ê³¼ë¥¼ í…ì„œë³´ë“œì— ê¸°ë¡í•©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜(weight)ì™€ í¸í–¥(bias)ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ `--report_to wandb` ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.

ë” ì‘ì€ batch(ë°°ì¹˜) í¬ê¸°ë¡œ gradient accumulation(ê¸°ìš¸ê¸° ëˆ„ì )ì„ í•˜ë©´ í•™ìŠµ ìš”êµ¬ì‚¬í•­ì„ ~20 GB VRAMìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
export MODEL_DIR="stable-diffusion-v1-5/stable-diffusion-v1-5"
export OUTPUT_DIR="path to save model"

accelerate launch train_controlnet.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --resolution=512 \
 --learning_rate=1e-5 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --train_batch_size=1 \
 --gradient_accumulation_steps=4 \
  --push_to_hub
```

## ì—¬ëŸ¬ê°œ GPUë¡œ í•™ìŠµí•˜ê¸°

`accelerate` ì€ seamless multi-GPU í•™ìŠµì„ ê³ ë ¤í•©ë‹ˆë‹¤. `accelerate`ê³¼ í•¨ê»˜ ë¶„ì‚°ëœ í•™ìŠµì„ ì‹¤í–‰í•˜ê¸° ìœ„í•´ [ì—¬ê¸°](https://huggingface.co/docs/accelerate/basic_tutorials/launch)
ì˜ ì„¤ëª…ì„ í™•ì¸í•˜ì„¸ìš”. ì•„ë˜ëŠ” ì˜ˆì‹œ ëª…ë ¹ì–´ì…ë‹ˆë‹¤:

```bash
export MODEL_DIR="stable-diffusion-v1-5/stable-diffusion-v1-5"
export OUTPUT_DIR="path to save model"

accelerate launch --mixed_precision="fp16" --multi_gpu train_controlnet.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --resolution=512 \
 --learning_rate=1e-5 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --train_batch_size=4 \
 --mixed_precision="fp16" \
 --tracker_project_name="controlnet-demo" \
 --report_to=wandb \
  --push_to_hub
```

## ì˜ˆì‹œ ê²°ê³¼

#### ë°°ì¹˜ ì‚¬ì´ì¦ˆ 8ë¡œ 300 ìŠ¤í… ì´í›„:

| |  |
|-------------------|:-------------------------:|
| | í‘¸ë¥¸ ë°°ê²½ê³¼ ë¹¨ê°„ ì›  |
![conditioning image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png) | ![í‘¸ë¥¸ ë°°ê²½ê³¼ ë¹¨ê°„ ì›](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/red_circle_with_blue_background_300_steps.png) |
| | ê°ˆìƒ‰ ê½ƒ ë°°ê²½ê³¼ ì²­ë¡ìƒ‰ ì› |
![conditioning image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png) | ![ê°ˆìƒ‰ ê½ƒ ë°°ê²½ê³¼ ì²­ë¡ìƒ‰ ì›](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/cyan_circle_with_brown_floral_background_300_steps.png) |

#### ë°°ì¹˜ ì‚¬ì´ì¦ˆ 8ë¡œ 6000 ìŠ¤í… ì´í›„:

| |  |
|-------------------|:-------------------------:|
| | í‘¸ë¥¸ ë°°ê²½ê³¼ ë¹¨ê°„ ì›  |
![conditioning image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_1.png) | ![í‘¸ë¥¸ ë°°ê²½ê³¼ ë¹¨ê°„ ì›](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/red_circle_with_blue_background_6000_steps.png) |
| | ê°ˆìƒ‰ ê½ƒ ë°°ê²½ê³¼ ì²­ë¡ìƒ‰ ì› |
![conditioning image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/conditioning_image_2.png) | ![ê°ˆìƒ‰ ê½ƒ ë°°ê²½ê³¼ ì²­ë¡ìƒ‰ ì›](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/controlnet_training/cyan_circle_with_brown_floral_background_6000_steps.png) |

## 16GB GPUì—ì„œ í•™ìŠµí•˜ê¸°

16GB GPUì—ì„œ í•™ìŠµí•˜ê¸° ìœ„í•´ ë‹¤ìŒì˜ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì„¸ìš”:

- ê¸°ìš¸ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥í•˜ê¸°
- bitsandbyteì˜ [8-bit optimizer](https://github.com/TimDettmers/bitsandbytes#requirements--installation)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë§í¬ì— ì—°ê²°ëœ ì„¤ëª…ì„œë¥¼ ë³´ì„¸ìš”.

ì´ì œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
export MODEL_DIR="stable-diffusion-v1-5/stable-diffusion-v1-5"
export OUTPUT_DIR="path to save model"

accelerate launch train_controlnet.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --resolution=512 \
 --learning_rate=1e-5 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --train_batch_size=1 \
 --gradient_accumulation_steps=4 \
 --gradient_checkpointing \
 --use_8bit_adam \
  --push_to_hub
```

## 12GB GPUì—ì„œ í•™ìŠµí•˜ê¸°

12GB GPUì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ë‹¤ìŒì˜ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì„¸ìš”:

- ê¸°ìš¸ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥í•˜ê¸°
- bitsandbyteì˜ 8-bit [optimizer](https://github.com/TimDettmers/bitsandbytes#requirements--installation)(ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë§í¬ì— ì—°ê²°ëœ ì„¤ëª…ì„œë¥¼ ë³´ì„¸ìš”)
- [xFormers](https://huggingface.co/docs/diffusers/training/optimization/xformers)(ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë§í¬ì— ì—°ê²°ëœ ì„¤ëª…ì„œë¥¼ ë³´ì„¸ìš”)
- ê¸°ìš¸ê¸°ë¥¼ `None`ìœ¼ë¡œ ì„¤ì •

```bash
export MODEL_DIR="stable-diffusion-v1-5/stable-diffusion-v1-5"
export OUTPUT_DIR="path to save model"

accelerate launch train_controlnet.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --resolution=512 \
 --learning_rate=1e-5 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --train_batch_size=1 \
 --gradient_accumulation_steps=4 \
 --gradient_checkpointing \
 --use_8bit_adam \
 --enable_xformers_memory_efficient_attention \
 --set_grads_to_none \
  --push_to_hub
```

`pip install xformers`ìœ¼ë¡œ `xformers`ì„ í™•ì‹¤íˆ ì„¤ì¹˜í•˜ê³  `enable_xformers_memory_efficient_attention`ì„ ì‚¬ìš©í•˜ì„¸ìš”.

## 8GB GPUì—ì„œ í•™ìŠµí•˜ê¸°

ìš°ë¦¬ëŠ” ControlNetì„ ì§€ì›í•˜ê¸° ìœ„í•œ DeepSpeedë¥¼ ì² ì €í•˜ê²Œ í…ŒìŠ¤íŠ¸í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ì„¤ì •ì´ ë©”ëª¨ë¦¬ë¥¼ ì €ì¥í•  ë•Œ,
ê·¸ í™˜ê²½ì´ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµí–ˆëŠ”ì§€ë¥¼ í™•ì •í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„±ê³µí•œ í•™ìŠµ ì‹¤í–‰ì„ ìœ„í•´ ì„¤ì •ì„ ë³€ê²½í•´ì•¼ í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

8GB GPUì—ì„œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ë‹¤ìŒì˜ ìµœì í™”ë¥¼ ì§„í–‰í•˜ì„¸ìš”:

- ê¸°ìš¸ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥í•˜ê¸°
- bitsandbyteì˜ 8-bit [optimizer](https://github.com/TimDettmers/bitsandbytes#requirements--installation)(ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë§í¬ì— ì—°ê²°ëœ ì„¤ëª…ì„œë¥¼ ë³´ì„¸ìš”)
- [xFormers](https://huggingface.co/docs/diffusers/training/optimization/xformers)(ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë§í¬ì— ì—°ê²°ëœ ì„¤ëª…ì„œë¥¼ ë³´ì„¸ìš”)
- ê¸°ìš¸ê¸°ë¥¼ `None`ìœ¼ë¡œ ì„¤ì •
- DeepSpeed stage 2 ë³€ìˆ˜ì™€ optimizer ì—†ì—ê¸°
- fp16 í˜¼í•© ì •ë°€ë„(precision)

[DeepSpeed](https://www.deepspeed.ai/)ëŠ” CPU ë˜ëŠ” NVMEë¡œ í…ì„œë¥¼ VRAMì—ì„œ ì˜¤í”„ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë¥¼ ìœ„í•´ì„œ í›¨ì”¬ ë” ë§ì€ RAM(ì•½ 25 GB)ê°€ í•„ìš”í•©ë‹ˆë‹¤.

DeepSpeed stage 2ë¥¼ í™œì„±í™”í•˜ê¸° ìœ„í•´ì„œ `accelerate config`ë¡œ í™˜ê²½ì„ êµ¬ì„±í•´ì•¼í•©ë‹ˆë‹¤.

êµ¬ì„±(configuration) íŒŒì¼ì€ ì´ëŸ° ëª¨ìŠµì´ì–´ì•¼ í•©ë‹ˆë‹¤:

```yaml
compute_environment: LOCAL_MACHINE
deepspeed_config:
  gradient_accumulation_steps: 4
  offload_optimizer_device: cpu
  offload_param_device: cpu
  zero3_init_flag: false
  zero_stage: 2
distributed_type: DEEPSPEED
```

<íŒ>

[ë¬¸ì„œ](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)ë¥¼ ë” ë§ì€ DeepSpeed ì„¤ì • ì˜µì…˜ì„ ìœ„í•´ ë³´ì„¸ìš”.

<íŒ>

ê¸°ë³¸ Adam optimizerë¥¼ DeepSpeed'ì˜ Adam
`deepspeed.ops.adam.DeepSpeedCPUAdam` ìœ¼ë¡œ ë°”ê¾¸ë©´ ìƒë‹¹í•œ ì†ë„ í–¥ìƒì„ ì´ë£°ìˆ˜ ìˆì§€ë§Œ,
Pytorchì™€ ê°™ì€ ë²„ì „ì˜ CUDA toolchainì´ í•„ìš”í•©ë‹ˆë‹¤. 8-ë¹„íŠ¸ optimizerëŠ” í˜„ì¬ DeepSpeedì™€
í˜¸í™˜ë˜ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

```bash
export MODEL_DIR="stable-diffusion-v1-5/stable-diffusion-v1-5"
export OUTPUT_DIR="path to save model"

accelerate launch train_controlnet.py \
 --pretrained_model_name_or_path=$MODEL_DIR \
 --output_dir=$OUTPUT_DIR \
 --dataset_name=fusing/fill50k \
 --resolution=512 \
 --validation_image "./conditioning_image_1.png" "./conditioning_image_2.png" \
 --validation_prompt "red circle with blue background" "cyan circle with brown floral background" \
 --train_batch_size=1 \
 --gradient_accumulation_steps=4 \
 --gradient_checkpointing \
 --enable_xformers_memory_efficient_attention \
 --set_grads_to_none \
 --mixed_precision fp16 \
 --push_to_hub
```

## ì¶”ë¡ 

í•™ìŠµëœ ëª¨ë¸ì€ [`StableDiffusionControlNetPipeline`]ê³¼ í•¨ê»˜ ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
`base_model_path`ì™€ `controlnet_path` ì— ê°’ì„ ì§€ì •í•˜ì„¸ìš” `--pretrained_model_name_or_path` ì™€
`--output_dir` ëŠ” í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì— ê°œë³„ì ìœ¼ë¡œ ì§€ì •ë©ë‹ˆë‹¤.

```py
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
import torch

base_model_path = "path to model"
controlnet_path = "path to controlnet"

controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    base_model_path, controlnet=controlnet, torch_dtype=torch.float16
)

# ë” ë¹ ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ë©”ëª¨ë¦¬ ìµœì í™”ë¡œ diffusion í”„ë¡œì„¸ìŠ¤ ì†ë„ ì˜¬ë¦¬ê¸°
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
# xformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šìœ¼ë©´ ì•„ë˜ ì¤„ì„ ì‚­ì œí•˜ê¸°
pipe.enable_xformers_memory_efficient_attention()

pipe.enable_model_cpu_offload()

control_image = load_image("./conditioning_image_1.png")
prompt = "pale golden rod circle with old lace background"

# ì´ë¯¸ì§€ ìƒì„±í•˜ê¸°
generator = torch.manual_seed(0)
image = pipe(prompt, num_inference_steps=20, generator=generator, image=control_image).images[0]

image.save("./output.png")
```
