<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# ë©”ëª¨ë¦¬ì™€ ì†ë„

ë©”ëª¨ë¦¬ ë˜ëŠ” ì†ë„ì— ëŒ€í•´ ğŸ¤— Diffusers *ì¶”ë¡ *ì„ ìµœì í™”í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ê¸°ìˆ ê³¼ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•©ë‹ˆë‹¤. 
ì¼ë°˜ì ìœ¼ë¡œ, memory-efficient attentionì„ ìœ„í•´ [xFormers](https://github.com/facebookresearch/xformers) ì‚¬ìš©ì„ ì¶”ì²œí•˜ê¸° ë•Œë¬¸ì—, ì¶”ì²œí•˜ëŠ” [ì„¤ì¹˜ ë°©ë²•](xformers)ì„ ë³´ê³  ì„¤ì¹˜í•´ ë³´ì„¸ìš”.

ë‹¤ìŒ ì„¤ì •ì´ ì„±ëŠ¥ê³¼ ë©”ëª¨ë¦¬ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.

|                  | ì§€ì—°ì‹œê°„  | ì†ë„ í–¥ìƒ |
| ---------------- | ------- | ------- |
| ë³„ë„ ì„¤ì • ì—†ìŒ      | 9.50s   | x1      |
| cuDNN auto-tuner | 9.37s   | x1.01   |
| fp16             | 3.61s   | x2.63   |
| Channels Last ë©”ëª¨ë¦¬ í˜•ì‹     | 3.30s   | x2.88   |
| traced UNet      | 3.21s   | x2.96   |
| memory-efficient attention | 2.63s  | x3.61   |

<em>
   NVIDIA TITAN RTXì—ì„œ 50 DDIM ìŠ¤í…ì˜ "a photo of an astronaut riding a horse on mars" í”„ë¡¬í”„íŠ¸ë¡œ 512x512 í¬ê¸°ì˜ ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. 
</em>

## cuDNN auto-tuner í™œì„±í™”í•˜ê¸°

[NVIDIA cuDNN](https://developer.nvidia.com/cudnn)ì€ ì»¨ë³¼ë£¨ì…˜ì„ ê³„ì‚°í•˜ëŠ” ë§ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì§€ì›í•©ë‹ˆë‹¤. AutotunerëŠ” ì§§ì€ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•˜ê³  ì£¼ì–´ì§„ ì…ë ¥ í¬ê¸°ì— ëŒ€í•´ ì£¼ì–´ì§„ í•˜ë“œì›¨ì–´ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ê°€ì§„ ì»¤ë„ì„ ì„ íƒí•©ë‹ˆë‹¤.

**ì»¨ë³¼ë£¨ì…˜ ë„¤íŠ¸ì›Œí¬**ë¥¼ í™œìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— (ë‹¤ë¥¸ ìœ í˜•ë“¤ì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŒ), ë‹¤ìŒ ì„¤ì •ì„ í†µí•´ ì¶”ë¡  ì „ì— cuDNN autotunerë¥¼ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
import torch

torch.backends.cudnn.benchmark = True
```

### fp32 ëŒ€ì‹  tf32 ì‚¬ìš©í•˜ê¸°  (Ampere ë° ì´í›„ CUDA ì¥ì¹˜ë“¤ì—ì„œ)

Ampere ë° ì´í›„ CUDA ì¥ì¹˜ì—ì„œ í–‰ë ¬ê³± ë° ì»¨ë³¼ë£¨ì…˜ì€ TensorFloat32(TF32) ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ë¹ ë¥´ì§€ë§Œ ì•½ê°„ ëœ ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ê¸°ë³¸ì ìœ¼ë¡œ PyTorchëŠ” ì»¨ë³¼ë£¨ì…˜ì— ëŒ€í•´ TF32 ëª¨ë“œë¥¼ í™œì„±í™”í•˜ì§€ë§Œ í–‰ë ¬ ê³±ì…ˆì€ í™œì„±í™”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. 
ë„¤íŠ¸ì›Œí¬ì— ì™„ì „í•œ float32 ì •ë°€ë„ê°€ í•„ìš”í•œ ê²½ìš°ê°€ ì•„ë‹ˆë©´ í–‰ë ¬ ê³±ì…ˆì— ëŒ€í•´ì„œë„ ì´ ì„¤ì •ì„ í™œì„±í™”í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 
ì´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜ì˜ ì •í™•ë„ ì†ì‹¤ì´ ìˆì§€ë§Œ, ê³„ì‚° ì†ë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ê·¸ê²ƒì— ëŒ€í•´ [ì—¬ê¸°](https://huggingface.co/docs/transformers/v4.18.0/en/performance#tf32)ì„œ ë” ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ì¶”ë¡ í•˜ê¸° ì „ì— ë‹¤ìŒì„ ì¶”ê°€í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤:

```python
import torch

torch.backends.cuda.matmul.allow_tf32 = True
```

## ë°˜ì •ë°€ë„ ê°€ì¤‘ì¹˜

ë” ë§ì€ GPU ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³  ë” ë¹ ë¥¸ ì†ë„ë¥¼ ì–»ê¸° ìœ„í•´ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì •ë°€ë„(half precision)ë¡œ ì§ì ‘ ë¡œë“œí•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ì—¬ê¸°ì—ëŠ” `fp16`ì´ë¼ëŠ” ë¸Œëœì¹˜ì— ì €ì¥ëœ float16 ë²„ì „ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ê·¸ ë•Œ `float16` ìœ í˜•ì„ ì‚¬ìš©í•˜ë„ë¡ PyTorchì— ì§€ì‹œí•˜ëŠ” ì‘ì—…ì´ í¬í•¨ë©ë‹ˆë‹¤.

```Python
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    
    torch_dtype=torch.float16,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
image = pipe(prompt).images[0]
```

<Tip warning={true}>
  ì–´ë–¤ íŒŒì´í”„ë¼ì¸ì—ì„œë„ [`torch.autocast`](https://pytorch.org/docs/stable/amp.html#torch.autocast) ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ê²€ì€ìƒ‰ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê³ , ìˆœìˆ˜í•œ float16 ì •ë°€ë„ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ í•­ìƒ ëŠë¦¬ê¸° ë•Œë¬¸ì— ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. 
</Tip>

## ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ìŠ¬ë¼ì´ìŠ¤ ì–´í…ì…˜

ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´, í•œ ë²ˆì— ëª¨ë‘ ê³„ì‚°í•˜ëŠ” ëŒ€ì‹  ë‹¨ê³„ì ìœ¼ë¡œ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ìŠ¬ë¼ì´ìŠ¤ ë²„ì „ì˜ ì–´í…ì…˜(attention)ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<Tip>
  Attention slicingì€ ëª¨ë¸ì´ í•˜ë‚˜ ì´ìƒì˜ ì–´í…ì…˜ í—¤ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” í•œ, ë°°ì¹˜ í¬ê¸°ê°€ 1ì¸ ê²½ìš°ì—ë„ ìœ ìš©í•©ë‹ˆë‹¤.
  í•˜ë‚˜ ì´ìƒì˜ ì–´í…ì…˜ í—¤ë“œê°€ ìˆëŠ” ê²½ìš° *QK^T* ì–´í…ì…˜ ë§¤íŠ¸ë¦­ìŠ¤ëŠ” ìƒë‹¹í•œ ì–‘ì˜ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆëŠ” ê° í—¤ë“œì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</Tip>

ê° í—¤ë“œì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ ì–´í…ì…˜ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ë ¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ ì¶”ë¡  ì „ì— íŒŒì´í”„ë¼ì¸ì—ì„œ [`~StableDiffusionPipeline.enable_attention_slicing`]ë¥¼ í˜¸ì¶œí•˜ë©´ ë©ë‹ˆë‹¤:

```Python
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    
    torch_dtype=torch.float16,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_attention_slicing()
image = pipe(prompt).images[0]
```

ì¶”ë¡  ì‹œê°„ì´ ì•½ 10% ëŠë ¤ì§€ëŠ” ì•½ê°„ì˜ ì„±ëŠ¥ ì €í•˜ê°€ ìˆì§€ë§Œ ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ 3.2GB ì •ë„ì˜ ì‘ì€ VRAMìœ¼ë¡œë„ Stable Diffusionì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!


## ë” í° ë°°ì¹˜ë¥¼ ìœ„í•œ sliced VAE ë””ì½”ë“œ

ì œí•œëœ VRAMì—ì„œ ëŒ€ê·œëª¨ ì´ë¯¸ì§€ ë°°ì¹˜ë¥¼ ë””ì½”ë”©í•˜ê±°ë‚˜ 32ê°œ ì´ìƒì˜ ì´ë¯¸ì§€ê°€ í¬í•¨ëœ ë°°ì¹˜ë¥¼ í™œì„±í™”í•˜ê¸° ìœ„í•´, ë°°ì¹˜ì˜ latent ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆì— í•˜ë‚˜ì”© ë””ì½”ë”©í•˜ëŠ” ìŠ¬ë¼ì´ìŠ¤ VAE ë””ì½”ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë¥¼ [`~StableDiffusionPipeline.enable_attention_slicing`] ë˜ëŠ” [`~StableDiffusionPipeline.enable_xformers_memory_efficient_attention`]ê³¼ ê²°í•©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¶”ê°€ë¡œ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

VAE ë””ì½”ë“œë¥¼ í•œ ë²ˆì— í•˜ë‚˜ì”© ìˆ˜í–‰í•˜ë ¤ë©´ ì¶”ë¡  ì „ì— íŒŒì´í”„ë¼ì¸ì—ì„œ [`~StableDiffusionPipeline.enable_vae_slicing`]ì„ í˜¸ì¶œí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:

```Python
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    
    torch_dtype=torch.float16,
)
pipe = pipe.to("cuda")

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_vae_slicing()
images = pipe([prompt] * 32).images
```

ë‹¤ì¤‘ ì´ë¯¸ì§€ ë°°ì¹˜ì—ì„œ VAE ë””ì½”ë“œê°€ ì•½ê°„ì˜ ì„±ëŠ¥ í–¥ìƒì´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ë‹¨ì¼ ì´ë¯¸ì§€ ë°°ì¹˜ì—ì„œëŠ” ì„±ëŠ¥ ì˜í–¥ì€ ì—†ìŠµë‹ˆë‹¤.


## ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ê°€ì† ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ CPUë¡œ ì˜¤í”„ë¡œë”©

ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ CPUë¡œ ì˜¤í”„ë¡œë“œí•˜ê³  ìˆœë°©í–¥ ì „ë‹¬ì„ ìˆ˜í–‰í•  ë•Œë§Œ GPUë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

CPU ì˜¤í”„ë¡œë”©ì„ ìˆ˜í–‰í•˜ë ¤ë©´ [`~StableDiffusionPipeline.enable_sequential_cpu_offload`]ë¥¼ í˜¸ì¶œí•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤:

```Python
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    
    torch_dtype=torch.float16,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_sequential_cpu_offload()
image = pipe(prompt).images[0]
```

ê·¸ëŸ¬ë©´ ë©”ëª¨ë¦¬ ì†Œë¹„ë¥¼ 3GB ë¯¸ë§Œìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Note that this method works at the submodule level, not on whole models. This is the best way to minimize memory consumption, but inference is much slower due to the iterative nature of the process. The UNet component of the pipeline runs several times (as many as `num_inference_steps`); each time, the different submodules of the UNet are sequentially onloaded and then offloaded as they are needed, so the number of memory transfers is large.

<Tip>
Consider using <a href="#model_offloading">model offloading</a> as another point in the optimization space: it will be much faster, but memory savings won't be as large.
</Tip>

It is also possible to chain offloading with attention slicing for minimal memory consumption (< 2GB).


```Python
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    
    torch_dtype=torch.float16,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_sequential_cpu_offload()
pipe.enable_attention_slicing(1)

image = pipe(prompt).images[0]
```

**ì°¸ê³ **: 'enable_sequential_cpu_offload()'ë¥¼ ì‚¬ìš©í•  ë•Œ, ë¯¸ë¦¬ íŒŒì´í”„ë¼ì¸ì„ CUDAë¡œ ì´ë™í•˜ì§€ **ì•ŠëŠ”** ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë©”ëª¨ë¦¬ ì†Œë¹„ì˜ ì´ë“ì´ ìµœì†Œí™”ë©ë‹ˆë‹¤. ë” ë§ì€ ì •ë³´ë¥¼ ìœ„í•´ [ì´ ì´ìŠˆ](https://github.com/huggingface/diffusers/issues/1934)ë¥¼ ë³´ì„¸ìš”.

<a name="model_offloading"></a>
## Model offloading for fast inference and memory savings

[Sequential CPU offloading](#sequential_offloading), as discussed in the previous section, preserves a lot of memory but makes inference slower, because submodules are moved to GPU as needed, and immediately returned to CPU when a new module runs.

Full-model offloading is an alternative that moves whole models to the GPU, instead of handling each model's constituent _modules_. This results in a negligible impact on inference time (compared with moving the pipeline to `cuda`), while still providing some memory savings.

In this scenario, only one of the main components of the pipeline (typically: text encoder, unet and vae)
will be in the GPU while the others wait in the CPU. Components like the UNet that run for multiple iterations will stay on GPU until they are no longer needed.

This feature can be enabled by invoking `enable_model_cpu_offload()` on the pipeline, as shown below.

```Python
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",  
    torch_dtype=torch.float16,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_model_cpu_offload()
image = pipe(prompt).images[0]
```

This is also compatible with attention slicing for additional memory savings.

```Python
import torch
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
)

prompt = "a photo of an astronaut riding a horse on mars"
pipe.enable_model_cpu_offload()
pipe.enable_attention_slicing(1)

image = pipe(prompt).images[0]
```

<Tip>
This feature requires `accelerate` version 0.17.0 or larger.
</Tip>

## Channels Last ë©”ëª¨ë¦¬ í˜•ì‹ ì‚¬ìš©í•˜ê¸°

Channels Last ë©”ëª¨ë¦¬ í˜•ì‹ì€ ì°¨ì› ìˆœì„œë¥¼ ë³´ì¡´í•˜ëŠ” ë©”ëª¨ë¦¬ì—ì„œ NCHW í…ì„œ ë°°ì—´ì„ ëŒ€ì²´í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
Channels Last í…ì„œëŠ” ì±„ë„ì´ ê°€ì¥ ì¡°ë°€í•œ ì°¨ì›ì´ ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì •ë ¬ë©ë‹ˆë‹¤(ì¼ëª… í”½ì…€ë‹¹ ì´ë¯¸ì§€ë¥¼ ì €ì¥).
í˜„ì¬ ëª¨ë“  ì—°ì‚°ì Channels Last í˜•ì‹ì„ ì§€ì›í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¼ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì‚¬ìš©í•´ë³´ê³  ëª¨ë¸ì— ì˜ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.


ì˜ˆë¥¼ ë“¤ì–´ íŒŒì´í”„ë¼ì¸ì˜ UNet ëª¨ë¸ì´ channels Last í˜•ì‹ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•˜ë ¤ë©´ ë‹¤ìŒì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
print(pipe.unet.conv_out.state_dict()["weight"].stride())  # (2880, 9, 3, 1)
pipe.unet.to(memory_format=torch.channels_last)  # in-place ì—°ì‚°
# 2ë²ˆì§¸ ì°¨ì›ì—ì„œ ìŠ¤íŠ¸ë¼ì´ë“œ 1ì„ ê°€ì§€ëŠ” (2880, 1, 960, 320)ë¡œ, ì—°ì‚°ì´ ì‘ë™í•¨ì„ ì¦ëª…í•©ë‹ˆë‹¤.
print(pipe.unet.conv_out.state_dict()["weight"].stride())
```

## ì¶”ì (tracing)

ì¶”ì ì€ ëª¨ë¸ì„ í†µí•´ ì˜ˆì œ ì…ë ¥ í…ì„œë¥¼ í†µí•´ ì‹¤í–‰ë˜ëŠ”ë°, í•´ë‹¹ ì…ë ¥ì´ ëª¨ë¸ì˜ ë ˆì´ì–´ë¥¼ í†µê³¼í•  ë•Œ í˜¸ì¶œë˜ëŠ” ì‘ì—…ì„ ìº¡ì²˜í•˜ì—¬ ì‹¤í–‰ íŒŒì¼ ë˜ëŠ” 'ScriptFunction'ì´ ë°˜í™˜ë˜ë„ë¡ í•˜ê³ , ì´ëŠ” just-in-time ì»´íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ë©ë‹ˆë‹¤.

UNet ëª¨ë¸ì„ ì¶”ì í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
import time
import torch
from diffusers import StableDiffusionPipeline
import functools

# torch ê¸°ìš¸ê¸° ë¹„í™œì„±í™”
torch.set_grad_enabled(False)

# ë³€ìˆ˜ ì„¤ì •
n_experiments = 2
unet_runs_per_experiment = 50

# ì…ë ¥ ë¶ˆëŸ¬ì˜¤ê¸°
def generate_inputs():
    sample = torch.randn(2, 4, 64, 64).half().cuda()
    timestep = torch.rand(1).half().cuda() * 999
    encoder_hidden_states = torch.randn(2, 77, 768).half().cuda()
    return sample, timestep, encoder_hidden_states


pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
).to("cuda")
unet = pipe.unet
unet.eval()
unet.to(memory_format=torch.channels_last)  # Channels Last ë©”ëª¨ë¦¬ í˜•ì‹ ì‚¬ìš©
unet.forward = functools.partial(unet.forward, return_dict=False)  # return_dict=Falseì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •

# ì›Œë°ì—…
for _ in range(3):
    with torch.inference_mode():
        inputs = generate_inputs()
        orig_output = unet(*inputs)

# ì¶”ì 
print("tracing..")
unet_traced = torch.jit.trace(unet, inputs)
unet_traced.eval()
print("done tracing")


# ì›Œë°ì—… ë° ê·¸ë˜í”„ ìµœì í™”
for _ in range(5):
    with torch.inference_mode():
        inputs = generate_inputs()
        orig_output = unet_traced(*inputs)


# ë²¤ì¹˜ë§ˆí‚¹
with torch.inference_mode():
    for _ in range(n_experiments):
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(unet_runs_per_experiment):
            orig_output = unet_traced(*inputs)
        torch.cuda.synchronize()
        print(f"unet traced inference took {time.time() - start_time:.2f} seconds")
    for _ in range(n_experiments):
        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(unet_runs_per_experiment):
            orig_output = unet(*inputs)
        torch.cuda.synchronize()
        print(f"unet inference took {time.time() - start_time:.2f} seconds")

# ëª¨ë¸ ì €ì¥
unet_traced.save("unet_traced.pt")
```

ê·¸ ë‹¤ìŒ, íŒŒì´í”„ë¼ì¸ì˜ `unet` íŠ¹ì„±ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¶”ì ëœ ëª¨ë¸ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from diffusers import StableDiffusionPipeline
import torch
from dataclasses import dataclass


@dataclass
class UNet2DConditionOutput:
    sample: torch.FloatTensor


pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
).to("cuda")

# jitted unet ì‚¬ìš©
unet_traced = torch.jit.load("unet_traced.pt")


# pipe.unet ì‚­ì œ
class TracedUNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.in_channels = pipe.unet.in_channels
        self.device = pipe.unet.device

    def forward(self, latent_model_input, t, encoder_hidden_states):
        sample = unet_traced(latent_model_input, t, encoder_hidden_states)[0]
        return UNet2DConditionOutput(sample=sample)


pipe.unet = TracedUNet()

with torch.inference_mode():
    image = pipe([prompt] * 1, num_inference_steps=50).images[0]
```


## Memory-efficient attention

ì–´í…ì…˜ ë¸”ë¡ì˜ ëŒ€ì—­í­ì„ ìµœì í™”í•˜ëŠ” ìµœê·¼ ì‘ì—…ìœ¼ë¡œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ê²Œ í–¥ìƒë˜ê³  í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.
@tridaoì˜ ê°€ì¥ ìµœê·¼ì˜ í”Œë˜ì‹œ ì–´í…ì…˜: [code](https://github.com/HazyResearch/flash-attention), [paper](https://arxiv.org/pdf/2205.14135.pdf).

ë°°ì¹˜ í¬ê¸° 1(í”„ë¡¬í”„íŠ¸ 1ê°œ)ì˜ 512x512 í¬ê¸°ë¡œ ì¶”ë¡ ì„ ì‹¤í–‰í•  ë•Œ ëª‡ ê°€ì§€ Nvidia GPUì—ì„œ ì–»ì€ ì†ë„ í–¥ìƒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

| GPU              	| ê¸°ì¤€ ì–´í…ì…˜ FP16 	       | ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì–´í…ì…˜ FP16 	|
|------------------	|---------------------	|---------------------------------	|
| NVIDIA Tesla T4  	| 3.5it/s             	| 5.5it/s                         	|
| NVIDIA 3060 RTX  	| 4.6it/s             	| 7.8it/s                         	|
| NVIDIA A10G      	| 8.88it/s            	| 15.6it/s                        	|
| NVIDIA RTX A6000 	| 11.7it/s            	| 21.09it/s                       	|
| NVIDIA TITAN RTX  | 12.51it/s         	| 18.22it/s                       	|
| A100-SXM4-40GB    	| 18.6it/s            	| 29.it/s                        	|
| A100-SXM-80GB    	| 18.7it/s            	| 29.5it/s                        	|

ì´ë¥¼ í™œìš©í•˜ë ¤ë©´ ë‹¤ìŒì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤: 
 - PyTorch > 1.12
 - Cuda ì‚¬ìš© ê°€ëŠ¥
 - [xformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•¨](xformers)
```python
from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
).to("cuda")

pipe.enable_xformers_memory_efficient_attention()

with torch.inference_mode():
    sample = pipe("a small cat")

# ì„ íƒ: ì´ë¥¼ ë¹„í™œì„±í™” í•˜ê¸° ìœ„í•´ ë‹¤ìŒì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# pipe.disable_xformers_memory_efficient_attention()
```
