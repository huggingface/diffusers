<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Kandinsky

[[open-in-colab]]

Kandinsky ëª¨ë¸ì€ ì¼ë ¨ì˜ ë‹¤êµ­ì–´ text-to-image ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤. Kandinsky 2.0 ëª¨ë¸ì€ ë‘ ê°œì˜ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ ì—°ê²°í•´ UNetì— ì‚¬ìš©ë©ë‹ˆë‹¤.

[Kandinsky 2.1](../api/pipelines/kandinsky)ì€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„ë² ë”© ê°„ì˜ ë§¤í•‘ì„ ìƒì„±í•˜ëŠ” image prior ëª¨ë¸([`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip))ì„ í¬í•¨í•˜ë„ë¡ ì•„í‚¤í…ì²˜ë¥¼ ë³€ê²½í–ˆìŠµë‹ˆë‹¤. ì´ ë§¤í•‘ì€ ë” ë‚˜ì€ text-image alignmentë¥¼ ì œê³µí•˜ë©°, í•™ìŠµ ì¤‘ì— í…ìŠ¤íŠ¸ ì„ë² ë”©ê³¼ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ ë” ë†’ì€ í’ˆì§ˆì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, Kandinsky 2.1ì€ spatial conditional ì •ê·œí™” ë ˆì´ì–´ë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ì‹¤ê°ì„ ë†’ì—¬ì£¼ëŠ” [Modulating Quantized Vectors (MoVQ)](https://huggingface.co/papers/2209.09002) ë””ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ latentsë¥¼ ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤.

[Kandinsky 2.2](../api/pipelines/kandinsky_v22)ëŠ” image prior ëª¨ë¸ì˜ ì´ë¯¸ì§€ ì¸ì½”ë”ë¥¼ ë” í° CLIP-ViT-G ëª¨ë¸ë¡œ êµì²´í•˜ì—¬ í’ˆì§ˆì„ ê°œì„ í•¨ìœ¼ë¡œì¨ ì´ì „ ëª¨ë¸ì„ ê°œì„ í–ˆìŠµë‹ˆë‹¤. ë˜í•œ image prior ëª¨ë¸ì€ í•´ìƒë„ì™€ ì¢…íš¡ë¹„ê°€ ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œ ì¬í›ˆë ¨ë˜ì–´ ë” ë†’ì€ í•´ìƒë„ì˜ ì´ë¯¸ì§€ì™€ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

[Kandinsky 3](../api/pipelines/kandinsky3)ëŠ” ì•„í‚¤í…ì²˜ë¥¼ ë‹¨ìˆœí™”í•˜ê³  prior ëª¨ë¸ê³¼ diffusion ëª¨ë¸ì„ í¬í•¨í•˜ëŠ” 2ë‹¨ê³„ ìƒì„± í”„ë¡œì„¸ìŠ¤ì—ì„œ ë²—ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤. ëŒ€ì‹ , Kandinsky 3ëŠ” [Flan-UL2](https://huggingface.co/google/flan-ul2)ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ê³ , [BigGan-deep](https://hf.co/papers/1809.11096) ë¸”ë¡ì´ í¬í•¨ëœ UNetì„ ì‚¬ìš©í•˜ë©°, [Sber-MoVQGAN](https://github.com/ai-forever/MoVQGAN)ì„ ì‚¬ìš©í•˜ì—¬ latentsë¥¼ ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì´í•´ì™€ ìƒì„±ëœ ì´ë¯¸ì§€ í’ˆì§ˆì€ ì£¼ë¡œ ë” í° í…ìŠ¤íŠ¸ ì¸ì½”ë”ì™€ UNetì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë‹¬ì„±ë©ë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” text-to-image, image-to-image, ì¸í˜ì¸íŒ…, ë³´ê°„ ë“±ì„ ìœ„í•´ Kandinsky ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```py
# Colabì—ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê¸° ìœ„í•´ ì£¼ì„ì„ ì œì™¸í•˜ì„¸ìš”
#!pip install -q diffusers transformers accelerate
```

> [!WARNING]
> Kandinsky 2.1ê³¼ 2.2ì˜ ì‚¬ìš©ë²•ì€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤! ìœ ì¼í•œ ì°¨ì´ì ì€ Kandinsky 2.2ëŠ” latentsë¥¼ ë””ì½”ë”©í•  ë•Œ `í”„ë¡¬í”„íŠ¸`ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ëŒ€ì‹ , Kandinsky 2.2ëŠ” ë””ì½”ë”© ì¤‘ì—ëŠ” `image_embeds`ë§Œ ë°›ì•„ë“¤ì…ë‹ˆë‹¤.
>
> <br>
>
> Kandinsky 3ëŠ” ë” ê°„ê²°í•œ ì•„í‚¤í…ì²˜ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©° prior ëª¨ë¸ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¦‰, [Stable Diffusion XL](sdxl)ê³¼ ê°™ì€ ë‹¤ë¥¸ diffusion ëª¨ë¸ê³¼ ì‚¬ìš©ë²•ì´ ë™ì¼í•©ë‹ˆë‹¤.

## Text-to-image

ëª¨ë“  ì‘ì—…ì— Kandinsky ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ í•­ìƒ í”„ë¡¬í”„íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ê³  ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” prior íŒŒì´í”„ë¼ì¸ì„ ì„¤ì •í•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. ì´ì „ íŒŒì´í”„ë¼ì¸ì€ negative í”„ë¡¬í”„íŠ¸ `""`ì— í•´ë‹¹í•˜ëŠ” `negative_image_embeds`ë„ ìƒì„±í•©ë‹ˆë‹¤. ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ ì´ì „ íŒŒì´í”„ë¼ì¸ì— ì‹¤ì œ `negative_prompt`ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆì§€ë§Œ, ì´ë ‡ê²Œ í•˜ë©´ prior íŒŒì´í”„ë¼ì¸ì˜ ìœ íš¨ ë°°ì¹˜ í¬ê¸°ê°€ 2ë°°ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.

<hfoptions id="text-to-image">
<hfoption id="Kandinsky 2.1">

```py
from diffusers import KandinskyPriorPipeline, KandinskyPipeline
import torch

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16).to("cuda")
pipeline = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16).to("cuda")

prompt = "A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting"
negative_prompt = "low quality, bad quality" # negative í”„ë¡¬í”„íŠ¸ í¬í•¨ì€ ì„ íƒì ì´ì§€ë§Œ, ë³´í†µ ê²°ê³¼ëŠ” ë” ì¢‹ìŠµë‹ˆë‹¤
image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt, guidance_scale=1.0).to_tuple()
```

ì´ì œ ëª¨ë“  í”„ë¡¬í”„íŠ¸ì™€ ì„ë² ë”©ì„ [`KandinskyPipeline`]ì— ì „ë‹¬í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```py
image = pipeline(prompt, image_embeds=image_embeds, negative_prompt=negative_prompt, negative_image_embeds=negative_image_embeds, height=768, width=768).images[0]
image
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/kandinsky-docs/cheeseburger.png"/>
</div>

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline
import torch

prior_pipeline = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16).to("cuda")
pipeline = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16).to("cuda")

prompt = "A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting"
negative_prompt = "low quality, bad quality" # negative í”„ë¡¬í”„íŠ¸ í¬í•¨ì€ ì„ íƒì ì´ì§€ë§Œ, ë³´í†µ ê²°ê³¼ëŠ” ë” ì¢‹ìŠµë‹ˆë‹¤
image_embeds, negative_image_embeds = prior_pipeline(prompt, guidance_scale=1.0).to_tuple()
```

ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ `image_embeds`ì™€ `negative_image_embeds`ë¥¼ [`KandinskyV22Pipeline`]ì— ì „ë‹¬í•©ë‹ˆë‹¤:

```py
image = pipeline(image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768).images[0]
image
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/kandinsky-text-to-image.png"/>
</div>

</hfoption>
<hfoption id="Kandinsky 3">

Kandinsky 3ëŠ” prior ëª¨ë¸ì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ [`Kandinsky3Pipeline`]ì„ ì§ì ‘ ë¶ˆëŸ¬ì˜¤ê³  ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
from diffusers import Kandinsky3Pipeline
import torch

pipeline = Kandinsky3Pipeline.from_pretrained("kandinsky-community/kandinsky-3", variant="fp16", torch_dtype=torch.float16)
pipeline.enable_model_cpu_offload()

prompt = "A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting"
image = pipeline(prompt).images[0]
image
```

</hfoption>
</hfoptions>

ğŸ¤— DiffusersëŠ” ë˜í•œ [`KandinskyCombinedPipeline`] ë° [`KandinskyV22CombinedPipeline`]ì´ í¬í•¨ëœ end-to-end APIë¥¼ ì œê³µí•˜ë¯€ë¡œ prior íŒŒì´í”„ë¼ì¸ê³¼ text-to-image ë³€í™˜ íŒŒì´í”„ë¼ì¸ì„ ë³„ë„ë¡œ ë¶ˆëŸ¬ì˜¬ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°í•©ëœ íŒŒì´í”„ë¼ì¸ì€ prior ëª¨ë¸ê³¼ ë””ì½”ë”ë¥¼ ëª¨ë‘ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì›í•˜ëŠ” ê²½ìš° `prior_guidance_scale` ë° `prior_num_inference_steps` ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ prior íŒŒì´í”„ë¼ì¸ì— ëŒ€í•´ ë‹¤ë¥¸ ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‚´ë¶€ì—ì„œ ê²°í•©ëœ íŒŒì´í”„ë¼ì¸ì„ ìë™ìœ¼ë¡œ í˜¸ì¶œí•˜ë ¤ë©´ [`AutoPipelineForText2Image`]ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

<hfoptions id="text-to-image">
<hfoption id="Kandinsky 2.1">

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16)
pipeline.enable_model_cpu_offload()

prompt = "A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting"
negative_prompt = "low quality, bad quality"

image = pipeline(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, guidance_scale=4.0, height=768, width=768).images[0]
image
```

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16)
pipeline.enable_model_cpu_offload()

prompt = "A alien cheeseburger creature eating itself, claymation, cinematic, moody lighting"
negative_prompt = "low quality, bad quality"

image = pipeline(prompt=prompt, negative_prompt=negative_prompt, prior_guidance_scale=1.0, guidance_scale=4.0, height=768, width=768).images[0]
image
```

</hfoption>
</hfoptions>

## Image-to-image

Image-to-image ê²½ìš°, ì´ˆê¸° ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì— ì´ë¯¸ì§€ë¥¼ conditioningí•©ë‹ˆë‹¤. Prior íŒŒì´í”„ë¼ì¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤:

<hfoptions id="image-to-image">
<hfoption id="Kandinsky 2.1">

```py
import torch
from diffusers import KandinskyImg2ImgPipeline, KandinskyPriorPipeline

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
pipeline = KandinskyImg2ImgPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
```

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
import torch
from diffusers import KandinskyV22Img2ImgPipeline, KandinskyPriorPipeline

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
pipeline = KandinskyV22Img2ImgPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
```

</hfoption>
<hfoption id="Kandinsky 3">

Kandinsky 3ëŠ” prior ëª¨ë¸ì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ image-to-image íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
from diffusers import Kandinsky3Img2ImgPipeline
from diffusers.utils import load_image
import torch

pipeline = Kandinsky3Img2ImgPipeline.from_pretrained("kandinsky-community/kandinsky-3", variant="fp16", torch_dtype=torch.float16)
pipeline.enable_model_cpu_offload()
```

</hfoption>
</hfoptions>

Conditioningí•  ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤:

```py
from diffusers.utils import load_image

# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
original_image = load_image(url)
original_image = original_image.resize((768, 512))
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"/>
</div>

Prior íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ `image_embeds`ì™€ `negative_image_embeds`ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```py
prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt).to_tuple()
```

ì´ì œ ì›ë³¸ ì´ë¯¸ì§€ì™€ ëª¨ë“  í”„ë¡¬í”„íŠ¸ ë° ì„ë² ë”©ì„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

<hfoptions id="image-to-image">
<hfoption id="Kandinsky 2.1">

```py
from diffusers.utils import make_image_grid

image = pipeline(prompt, negative_prompt=negative_prompt, image=original_image, image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768, strength=0.3).images[0]
make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/kandinsky-docs/img2img_fantasyland.png"/>
</div>

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
from diffusers.utils import make_image_grid

image = pipeline(image=original_image, image_embeds=image_embeds, negative_image_embeds=negative_image_embeds, height=768, width=768, strength=0.3).images[0]
make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/kandinsky-image-to-image.png"/>
</div>

</hfoption>
<hfoption id="Kandinsky 3">

```py
image = pipeline(prompt, negative_prompt=negative_prompt, image=image, strength=0.75, num_inference_steps=25).images[0]
image
```

</hfoption>
</hfoptions>

ë˜í•œ ğŸ¤— Diffusersì—ì„œëŠ” [`KandinskyImg2ImgCombinedPipeline`] ë° [`KandinskyV22Img2ImgCombinedPipeline`]ì´ í¬í•¨ëœ end-to-end APIë¥¼ ì œê³µí•˜ë¯€ë¡œ prior íŒŒì´í”„ë¼ì¸ê³¼ image-to-image íŒŒì´í”„ë¼ì¸ì„ ë³„ë„ë¡œ ë¶ˆëŸ¬ì˜¬ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°í•©ëœ íŒŒì´í”„ë¼ì¸ì€ prior ëª¨ë¸ê³¼ ë””ì½”ë”ë¥¼ ëª¨ë‘ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì›í•˜ëŠ” ê²½ìš° `prior_guidance_scale` ë° `prior_num_inference_steps` ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ íŒŒì´í”„ë¼ì¸ì— ëŒ€í•´ ë‹¤ë¥¸ ê°’ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‚´ë¶€ì—ì„œ ê²°í•©ëœ íŒŒì´í”„ë¼ì¸ì„ ìë™ìœ¼ë¡œ í˜¸ì¶œí•˜ë ¤ë©´ [`AutoPipelineForImage2Image`]ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

<hfoptions id="image-to-image">
<hfoption id="Kandinsky 2.1">

```py
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image
import torch

pipeline = AutoPipelineForImage2Image.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16, use_safetensors=True)
pipeline.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
original_image = load_image(url)

original_image.thumbnail((768, 768))

image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=original_image, strength=0.3).images[0]
make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)
```

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image
import torch

pipeline = AutoPipelineForImage2Image.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16)
pipeline.enable_model_cpu_offload()

prompt = "A fantasy landscape, Cinematic lighting"
negative_prompt = "low quality, bad quality"

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
original_image = load_image(url)

original_image.thumbnail((768, 768))

image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=original_image, strength=0.3).images[0]
make_image_grid([original_image.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)
```

</hfoption>
</hfoptions>

## Inpainting

> [!WARNING]
> âš ï¸ Kandinsky ëª¨ë¸ì€ ì´ì œ ê²€ì€ìƒ‰ í”½ì…€ ëŒ€ì‹  â¬œï¸ **í°ìƒ‰ í”½ì…€**ì„ ì‚¬ìš©í•˜ì—¬ ë§ˆìŠ¤í¬ ì˜ì—­ì„ í‘œí˜„í•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ì—ì„œ [`KandinskyInpaintPipeline`]ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° í°ìƒ‰ í”½ì…€ì„ ì‚¬ìš©í•˜ë„ë¡ ë§ˆìŠ¤í¬ë¥¼ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤:
>
> ```py
> # PIL ì…ë ¥ì— ëŒ€í•´
> import PIL.ImageOps
> mask = PIL.ImageOps.invert(mask)
>
> # PyTorchì™€ NumPy ì…ë ¥ì— ëŒ€í•´
> mask = 1 - mask
> ```

ì¸í˜ì¸íŒ…ì—ì„œëŠ” ì›ë³¸ ì´ë¯¸ì§€, ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ëŒ€ì²´í•  ì˜ì—­ì˜ ë§ˆìŠ¤í¬, ì¸í˜ì¸íŒ…í•  ë‚´ìš©ì— ëŒ€í•œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. Prior íŒŒì´í”„ë¼ì¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:

<hfoptions id="inpaint">
<hfoption id="Kandinsky 2.1">

```py
from diffusers import KandinskyInpaintPipeline, KandinskyPriorPipeline
from diffusers.utils import load_image, make_image_grid
import torch
import numpy as np
from PIL import Image

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
pipeline = KandinskyInpaintPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
```

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
from diffusers import KandinskyV22InpaintPipeline, KandinskyV22PriorPipeline
from diffusers.utils import load_image, make_image_grid
import torch
import numpy as np
from PIL import Image

prior_pipeline = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
pipeline = KandinskyV22InpaintPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
```

</hfoption>
</hfoptions>

ì´ˆê¸° ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```py
init_image = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png")
mask = np.zeros((768, 768), dtype=np.float32)
# mask area above cat's head
mask[:250, 250:-250] = 1
```

Prior íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤:

```py
prompt = "a hat"
prior_output = prior_pipeline(prompt)
```

ì´ì œ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•´ ì´ˆê¸° ì´ë¯¸ì§€, ë§ˆìŠ¤í¬, í”„ë¡¬í”„íŠ¸ì™€ ì„ë² ë”©ì„ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬í•©ë‹ˆë‹¤:

<hfoptions id="inpaint">
<hfoption id="Kandinsky 2.1">

```py
output_image = pipeline(prompt, image=init_image, mask_image=mask, **prior_output, height=768, width=768, num_inference_steps=150).images[0]
mask = Image.fromarray((mask*255).astype('uint8'), 'L')
make_image_grid([init_image, mask, output_image], rows=1, cols=3)
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/kandinsky-docs/inpaint_cat_hat.png"/>
</div>

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
output_image = pipeline(image=init_image, mask_image=mask, **prior_output, height=768, width=768, num_inference_steps=150).images[0]
mask = Image.fromarray((mask*255).astype('uint8'), 'L')
make_image_grid([init_image, mask, output_image], rows=1, cols=3)
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/kandinskyv22-inpaint.png"/>
</div>

</hfoption>
</hfoptions>

[`KandinskyInpaintCombinedPipeline`] ë° [`KandinskyV22InpaintCombinedPipeline`]ì„ ì‚¬ìš©í•˜ì—¬ ë‚´ë¶€ì—ì„œ prior ë° ë””ì½”ë” íŒŒì´í”„ë¼ì¸ì„ í•¨ê»˜ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ [`AutoPipelineForInpainting`]ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

<hfoptions id="inpaint">
<hfoption id="Kandinsky 2.1">

```py
import torch
import numpy as np
from PIL import Image
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipe = AutoPipelineForInpainting.from_pretrained("kandinsky-community/kandinsky-2-1-inpaint", torch_dtype=torch.float16)
pipe.enable_model_cpu_offload()

init_image = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png")
mask = np.zeros((768, 768), dtype=np.float32)
# ê³ ì–‘ì´ ë¨¸ë¦¬ ìœ„ ë§ˆìŠ¤í¬ ì§€ì—­
mask[:250, 250:-250] = 1
prompt = "a hat"

output_image = pipe(prompt=prompt, image=init_image, mask_image=mask).images[0]
mask = Image.fromarray((mask*255).astype('uint8'), 'L')
make_image_grid([init_image, mask, output_image], rows=1, cols=3)
```

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
import torch
import numpy as np
from PIL import Image
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipe = AutoPipelineForInpainting.from_pretrained("kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16)
pipe.enable_model_cpu_offload()

init_image = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png")
mask = np.zeros((768, 768), dtype=np.float32)
# ê³ ì–‘ì´ ë¨¸ë¦¬ ìœ„ ë§ˆìŠ¤í¬ ì˜ì—­
mask[:250, 250:-250] = 1
prompt = "a hat"

output_image = pipe(prompt=prompt, image=original_image, mask_image=mask).images[0]
mask = Image.fromarray((mask*255).astype('uint8'), 'L')
make_image_grid([init_image, mask, output_image], rows=1, cols=3)
```

</hfoption>
</hfoptions>

## Interpolation (ë³´ê°„)

Interpolation(ë³´ê°„)ì„ ì‚¬ìš©í•˜ë©´ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ì´ì˜ latent spaceë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆì–´ prior ëª¨ë¸ì˜ ì¤‘ê°„ ê²°ê³¼ë¬¼ì„ ë³¼ ìˆ˜ ìˆëŠ” ë©‹ì§„ ë°©ë²•ì…ë‹ˆë‹¤. Prior íŒŒì´í”„ë¼ì¸ê³¼ ë³´ê°„í•˜ë ¤ëŠ” ë‘ ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:

<hfoptions id="interpolate">
<hfoption id="Kandinsky 2.1">

```py
from diffusers import KandinskyPriorPipeline, KandinskyPipeline
from diffusers.utils import load_image, make_image_grid
import torch

prior_pipeline = KandinskyPriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-1-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
img_1 = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png")
img_2 = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg")
make_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)
```

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
from diffusers import KandinskyV22PriorPipeline, KandinskyV22Pipeline
from diffusers.utils import load_image, make_image_grid
import torch

prior_pipeline = KandinskyV22PriorPipeline.from_pretrained("kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16, use_safetensors=True).to("cuda")
img_1 = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png")
img_2 = load_image("https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg")
make_image_grid([img_1.resize((512,512)), img_2.resize((512,512))], rows=1, cols=2)
```

</hfoption>
</hfoptions>

<div class="flex gap-4">
  <div>
    <img class="rounded-xl" src="https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/cat.png"/>
    <figcaption class="mt-2 text-center text-sm text-gray-500">a cat</figcaption>
  </div>
  <div>
    <img class="rounded-xl" src="https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky/starry_night.jpeg"/>
    <figcaption class="mt-2 text-center text-sm text-gray-500">Van Gogh's Starry Night painting</figcaption>
  </div>
</div>

ë³´ê°„í•  í…ìŠ¤íŠ¸ ë˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì§€ì •í•˜ê³  ê° í…ìŠ¤íŠ¸ ë˜ëŠ” ì´ë¯¸ì§€ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ì‹¤í—˜í•˜ì—¬ ë³´ê°„ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!

```py
images_texts = ["a cat", img_1, img_2]
weights = [0.3, 0.3, 0.4]
```

`interpolate` í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•œ ë‹¤ìŒ, íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

<hfoptions id="interpolate">
<hfoption id="Kandinsky 2.1">

```py
# í”„ë¡¬í”„íŠ¸ëŠ” ë¹ˆì¹¸ìœ¼ë¡œ ë‚¨ê²¨ë„ ë©ë‹ˆë‹¤
prompt = ""
prior_out = prior_pipeline.interpolate(images_texts, weights)

pipeline = KandinskyPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16, use_safetensors=True).to("cuda")

image = pipeline(prompt, **prior_out, height=768, width=768).images[0]
image
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/kandinsky-docs/starry_cat.png"/>
</div>

</hfoption>
<hfoption id="Kandinsky 2.2">

```py
# í”„ë¡¬í”„íŠ¸ëŠ” ë¹ˆì¹¸ìœ¼ë¡œ ë‚¨ê²¨ë„ ë©ë‹ˆë‹¤
prompt = ""
prior_out = prior_pipeline.interpolate(images_texts, weights)

pipeline = KandinskyV22Pipeline.from_pretrained("kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True).to("cuda")

image = pipeline(prompt, **prior_out, height=768, width=768).images[0]
image
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/kandinskyv22-interpolate.png"/>
</div>

</hfoption>
</hfoptions>

## ControlNet

> [!WARNING]
> âš ï¸ ControlNetì€ Kandinsky 2.2ì—ì„œë§Œ ì§€ì›ë©ë‹ˆë‹¤!

ControlNetì„ ì‚¬ìš©í•˜ë©´ depth mapì´ë‚˜ edge detectionì™€ ê°™ì€ ì¶”ê°€ ì…ë ¥ì„ í†µí•´ ì‚¬ì „í•™ìŠµëœ large diffusion ëª¨ë¸ì„ conditioningí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëª¨ë¸ì´ depth mapì˜ êµ¬ì¡°ë¥¼ ì´í•´í•˜ê³  ë³´ì¡´í•  ìˆ˜ ìˆë„ë¡ ê¹Šì´ ë§µìœ¼ë¡œ Kandinsky 2.2ë¥¼ conditioningí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  depth mapì„ ì¶”ì¶œí•´ ë³´ê² ìŠµë‹ˆë‹¤:

```py
from diffusers.utils import load_image

img = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png"
).resize((768, 768))
img
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png"/>
</div>

ê·¸ëŸ° ë‹¤ìŒ ğŸ¤— Transformersì˜ `depth-estimation` [`~transformers.Pipeline`]ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•´ depth mapì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
import torch
import numpy as np

from transformers import pipeline

def make_hint(image, depth_estimator):
    image = depth_estimator(image)["depth"]
    image = np.array(image)
    image = image[:, :, None]
    image = np.concatenate([image, image, image], axis=2)
    detected_map = torch.from_numpy(image).float() / 255.0
    hint = detected_map.permute(2, 0, 1)
    return hint

depth_estimator = pipeline("depth-estimation")
hint = make_hint(img, depth_estimator).unsqueeze(0).half().to("cuda")
```

### Text-to-image [[controlnet-text-to-image]]

Prior íŒŒì´í”„ë¼ì¸ê³¼ [`KandinskyV22ControlnetPipeline`]ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:

```py
from diffusers import KandinskyV22PriorPipeline, KandinskyV22ControlnetPipeline

prior_pipeline = KandinskyV22PriorPipeline.from_pretrained(
    "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

pipeline = KandinskyV22ControlnetPipeline.from_pretrained(
    "kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16
).to("cuda")
```

í”„ë¡¬í”„íŠ¸ì™€ negative í”„ë¡¬í”„íŠ¸ë¡œ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤:

```py
prompt = "A robot, 4k photo"
negative_prior_prompt = "lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature"

generator = torch.Generator(device="cuda").manual_seed(43)

image_emb, zero_image_emb = prior_pipeline(
    prompt=prompt, negative_prompt=negative_prior_prompt, generator=generator
).to_tuple()
```

ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ depth ì´ë¯¸ì§€ë¥¼ [`KandinskyV22ControlnetPipeline`]ì— ì „ë‹¬í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```py
image = pipeline(image_embeds=image_emb, negative_image_embeds=zero_image_emb, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]
image
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/robot_cat_text2img.png"/>
</div>

### Image-to-image [[controlnet-image-to-image]]

ControlNetì„ ì‚¬ìš©í•œ image-to-imageì˜ ê²½ìš°, ë‹¤ìŒì„ ì‚¬ìš©í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤:

- [`KandinskyV22PriorEmb2EmbPipeline`]ë¡œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì™€ ì´ë¯¸ì§€ì—ì„œ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
- [`KandinskyV22ControlnetImg2ImgPipeline`]ë¡œ ì´ˆê¸° ì´ë¯¸ì§€ì™€ ì´ë¯¸ì§€ ì„ë² ë”©ì—ì„œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ğŸ¤— Transformersì—ì„œ `depth-estimation` [`~transformers.Pipeline`]ì„ ì‚¬ìš©í•˜ì—¬ ê³ ì–‘ì´ì˜ ì´ˆê¸° ì´ë¯¸ì§€ì˜ depth mapì„ ì²˜ë¦¬í•´ ì¶”ì¶œí•©ë‹ˆë‹¤:

```py
import torch
import numpy as np

from diffusers import KandinskyV22PriorEmb2EmbPipeline, KandinskyV22ControlnetImg2ImgPipeline
from diffusers.utils import load_image
from transformers import pipeline

img = load_image(
    "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/cat.png"
).resize((768, 768))

def make_hint(image, depth_estimator):
    image = depth_estimator(image)["depth"]
    image = np.array(image)
    image = image[:, :, None]
    image = np.concatenate([image, image, image], axis=2)
    detected_map = torch.from_numpy(image).float() / 255.0
    hint = detected_map.permute(2, 0, 1)
    return hint

depth_estimator = pipeline("depth-estimation")
hint = make_hint(img, depth_estimator).unsqueeze(0).half().to("cuda")
```

Prior íŒŒì´í”„ë¼ì¸ê³¼ [`KandinskyV22ControlnetImg2ImgPipeline`]ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:

```py
prior_pipeline = KandinskyV22PriorEmb2EmbPipeline.from_pretrained(
    "kandinsky-community/kandinsky-2-2-prior", torch_dtype=torch.float16, use_safetensors=True
).to("cuda")

pipeline = KandinskyV22ControlnetImg2ImgPipeline.from_pretrained(
    "kandinsky-community/kandinsky-2-2-controlnet-depth", torch_dtype=torch.float16
).to("cuda")
```

í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì™€ ì´ˆê¸° ì´ë¯¸ì§€ë¥¼ ì´ì „ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬í•˜ì—¬ ì´ë¯¸ì§€ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤:

```py
prompt = "A robot, 4k photo"
negative_prior_prompt = "lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature"

generator = torch.Generator(device="cuda").manual_seed(43)

img_emb = prior_pipeline(prompt=prompt, image=img, strength=0.85, generator=generator)
negative_emb = prior_pipeline(prompt=negative_prior_prompt, image=img, strength=1, generator=generator)
```

ì´ì œ [`KandinskyV22ControlnetImg2ImgPipeline`]ì„ ì‹¤í–‰í•˜ì—¬ ì´ˆê¸° ì´ë¯¸ì§€ì™€ ì´ë¯¸ì§€ ì„ë² ë”©ìœ¼ë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
image = pipeline(image=img, strength=0.5, image_embeds=img_emb.image_embeds, negative_image_embeds=negative_emb.image_embeds, hint=hint, num_inference_steps=50, generator=generator, height=768, width=768).images[0]
make_image_grid([img.resize((512, 512)), image.resize((512, 512))], rows=1, cols=2)
```

<div class="flex justify-center">
    <img class="rounded-xl" src="https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinskyv22/robot_cat.png"/>
</div>

## ìµœì í™”

KandinskyëŠ” mappingì„ ìƒì„±í•˜ê¸° ìœ„í•œ prior íŒŒì´í”„ë¼ì¸ê³¼ latentsë¥¼ ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©í•˜ê¸° ìœ„í•œ ë‘ ë²ˆì§¸ íŒŒì´í”„ë¼ì¸ì´ í•„ìš”í•˜ë‹¤ëŠ” ì ì—ì„œ ë…íŠ¹í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê³„ì‚°ì´ ë‘ ë²ˆì§¸ íŒŒì´í”„ë¼ì¸ì—ì„œ ì´ë£¨ì–´ì§€ë¯€ë¡œ ìµœì í™”ì˜ ë…¸ë ¥ì€ ë‘ ë²ˆì§¸ íŒŒì´í”„ë¼ì¸ì— ì§‘ì¤‘ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ì¶”ë¡  ì¤‘ Kandinskyí‚¤ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ íŒì…ë‹ˆë‹¤.

1. PyTorch < 2.0ì„ ì‚¬ìš©í•  ê²½ìš° [xFormers](../optimization/xformers)ì„ í™œì„±í™”í•©ë‹ˆë‹¤.

```diff
  from diffusers import DiffusionPipeline
  import torch

  pipe = DiffusionPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16)
+ pipe.enable_xformers_memory_efficient_attention()
```

2. PyTorch >= 2.0ì„ ì‚¬ìš©í•  ê²½ìš° `torch.compile`ì„ í™œì„±í™”í•˜ì—¬ scaled dot-product attention (SDPA)ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©í•˜ë„ë¡ í•©ë‹ˆë‹¤:

```diff
  pipe.unet.to(memory_format=torch.channels_last)
+ pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead", fullgraph=True)
```

ì´ëŠ” attention processorë¥¼ ëª…ì‹œì ìœ¼ë¡œ [`~models.attention_processor.AttnAddedKVProcessor2_0`]ì„ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•©ë‹ˆë‹¤:

```py
from diffusers.models.attention_processor import AttnAddedKVProcessor2_0

pipe.unet.set_attn_processor(AttnAddedKVProcessor2_0())
```

3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ [`~KandinskyPriorPipeline.enable_model_cpu_offload`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ CPUë¡œ ì˜¤í”„ë¡œë“œí•©ë‹ˆë‹¤:

```diff
  from diffusers import DiffusionPipeline
  import torch

  pipe = DiffusionPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", torch_dtype=torch.float16)
+ pipe.enable_model_cpu_offload()
```

4. ê¸°ë³¸ì ìœ¼ë¡œ text-to-image íŒŒì´í”„ë¼ì¸ì€ [`DDIMScheduler`]ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, [`DDPMScheduler`]ì™€ ê°™ì€ ë‹¤ë¥¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¡œ ëŒ€ì²´í•˜ì—¬ ì¶”ë¡  ì†ë„ì™€ ì´ë¯¸ì§€ í’ˆì§ˆ ê°„ì˜ ê· í˜•ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```py
from diffusers import DDPMScheduler
from diffusers import DiffusionPipeline

scheduler = DDPMScheduler.from_pretrained("kandinsky-community/kandinsky-2-1", subfolder="ddpm_scheduler")
pipe = DiffusionPipeline.from_pretrained("kandinsky-community/kandinsky-2-1", scheduler=scheduler, torch_dtype=torch.float16, use_safetensors=True).to("cuda")
```
