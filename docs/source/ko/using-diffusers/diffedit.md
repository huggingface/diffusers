<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# DiffEdit

[[open-in-colab]]

ì´ë¯¸ì§€ í¸ì§‘ì„ í•˜ë ¤ë©´ ì¼ë°˜ì ìœ¼ë¡œ í¸ì§‘í•  ì˜ì—­ì˜ ë§ˆìŠ¤í¬ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. DiffEditëŠ” í…ìŠ¤íŠ¸ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆìŠ¤í¬ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ë¯€ë¡œ ì´ë¯¸ì§€ í¸ì§‘ ì†Œí”„íŠ¸ì›¨ì–´ ì—†ì´ë„ ë§ˆìŠ¤í¬ë¥¼ ë§Œë“¤ê¸°ê°€ ì „ë°˜ì ìœ¼ë¡œ ë” ì‰¬ì›Œì§‘ë‹ˆë‹¤. DiffEdit ì•Œê³ ë¦¬ì¦˜ì€ ì„¸ ë‹¨ê³„ë¡œ ì‘ë™í•©ë‹ˆë‹¤:

1. Diffusion ëª¨ë¸ì´ ì¼ë¶€ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ì°¸ì¡° í…ìŠ¤íŠ¸ë¥¼ ì¡°ê±´ë¶€ë¡œ ì´ë¯¸ì§€ì˜ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ì—¬ ì´ë¯¸ì§€ì˜ ì—¬ëŸ¬ ì˜ì—­ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ë…¸ì´ì¦ˆ ì¶”ì •ì¹˜ë¥¼ ìƒì„±í•˜ê³ , ê·¸ ì°¨ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ë„ë¡ ì´ë¯¸ì§€ì˜ ì–´ëŠ ì˜ì—­ì„ ë³€ê²½í•´ì•¼ í•˜ëŠ”ì§€ ì‹ë³„í•˜ê¸° ìœ„í•œ ë§ˆìŠ¤í¬ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
2. ì…ë ¥ ì´ë¯¸ì§€ê°€ DDIMì„ ì‚¬ìš©í•˜ì—¬ ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©ë©ë‹ˆë‹¤.
3. ë§ˆìŠ¤í¬ ì™¸ë¶€ì˜ í”½ì…€ì´ ì…ë ¥ ì´ë¯¸ì§€ì™€ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ë„ë¡ ë§ˆìŠ¤í¬ë¥¼ ê°€ì´ë“œë¡œ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¿¼ë¦¬ì— ì¡°ê±´ì´ ì§€ì •ëœ diffusion ëª¨ë¸ë¡œ latentsë¥¼ ë””ì½”ë”©í•©ë‹ˆë‹¤.

ì´ ê°€ì´ë“œì—ì„œëŠ” ë§ˆìŠ¤í¬ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë§Œë“¤ì§€ ì•Šê³  DiffEditë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í¸ì§‘í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:

```py
# Colabì—ì„œ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê¸° ìœ„í•´ ì£¼ì„ì„ ì œì™¸í•˜ì„¸ìš”
#!pip install -q diffusers transformers accelerate
```

[`StableDiffusionDiffEditPipeline`]ì—ëŠ” ì´ë¯¸ì§€ ë§ˆìŠ¤í¬ì™€ ë¶€ë¶„ì ìœ¼ë¡œ ë°˜ì „ëœ latents ì§‘í•©ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë§ˆìŠ¤í¬ëŠ” [`~StableDiffusionDiffEditPipeline.generate_mask`] í•¨ìˆ˜ì—ì„œ ìƒì„±ë˜ë©°, ë‘ ê°œì˜ íŒŒë¼ë¯¸í„°ì¸ `source_prompt`ì™€ `target_prompt`ê°€ í¬í•¨ë©ë‹ˆë‹¤. ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ì´ë¯¸ì§€ì—ì„œ ë¬´ì—‡ì„ í¸ì§‘í• ì§€ ê²°ì •í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, *ê³¼ì¼* í•œ ê·¸ë¦‡ì„ *ë°°* í•œ ê·¸ë¦‡ìœ¼ë¡œ ë³€ê²½í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ì„¸ìš”:

```py
source_prompt = "a bowl of fruits"
target_prompt = "a bowl of pears"
```

ë¶€ë¶„ì ìœ¼ë¡œ ë°˜ì „ëœ latentsëŠ” [`~StableDiffusionDiffEditPipeline.invert`] í•¨ìˆ˜ì—ì„œ ìƒì„±ë˜ë©°, ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ” `prompt` ë˜ëŠ” *ìº¡ì…˜*ì„ í¬í•¨í•˜ëŠ” ê²ƒì´ inverse latent sampling í”„ë¡œì„¸ìŠ¤ë¥¼ ê°€ì´ë“œí•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ìº¡ì…˜ì€ ì¢…ì¢… `source_prompt`ê°€ ë  ìˆ˜ ìˆì§€ë§Œ, ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ ììœ ë¡­ê²Œ ì‹¤í—˜í•´ ë³´ì„¸ìš”!

íŒŒì´í”„ë¼ì¸, ìŠ¤ì¼€ì¤„ëŸ¬, ì—­ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ìµœì í™”ë¥¼ í™œì„±í™”í•´ ë³´ê² ìŠµë‹ˆë‹¤:

```py
import torch
from diffusers import DDIMScheduler, DDIMInverseScheduler, StableDiffusionDiffEditPipeline

pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1",
    torch_dtype=torch.float16,
    safety_checker=None,
    use_safetensors=True,
)
pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)
pipeline.enable_model_cpu_offload()
pipeline.enable_vae_slicing()
```

ìˆ˜ì •í•˜ê¸° ìœ„í•œ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:

```py
from diffusers.utils import load_image, make_image_grid

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))
raw_image
```

ì´ë¯¸ì§€ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ [`~StableDiffusionDiffEditPipeline.generate_mask`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì—ì„œ í¸ì§‘í•  ë‚´ìš©ì„ ì§€ì •í•˜ê¸° ìœ„í•´ `source_prompt`ì™€ `target_prompt`ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤:

```py
from PIL import Image

source_prompt = "a bowl of fruits"
target_prompt = "a basket of pears"
mask_image = pipeline.generate_mask(
    image=raw_image,
    source_prompt=source_prompt,
    target_prompt=target_prompt,
)
Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L").resize((768, 768))
```

ë‹¤ìŒìœ¼ë¡œ, ë°˜ì „ëœ latentsë¥¼ ìƒì„±í•˜ê³  ì´ë¯¸ì§€ë¥¼ ë¬˜ì‚¬í•˜ëŠ” ìº¡ì…˜ì— ì „ë‹¬í•©ë‹ˆë‹¤:

```py
inv_latents = pipeline.invert(prompt=source_prompt, image=raw_image).latents
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, ì´ë¯¸ì§€ ë§ˆìŠ¤í¬ì™€ ë°˜ì „ëœ latentsë¥¼ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬í•©ë‹ˆë‹¤. `target_prompt`ëŠ” ì´ì œ `prompt`ê°€ ë˜ë©°, `source_prompt`ëŠ” `negative_prompt`ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

```py
output_image = pipeline(
    prompt=target_prompt,
    mask_image=mask_image,
    image_latents=inv_latents,
    negative_prompt=source_prompt,
).images[0]
mask_image = Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L").resize((768, 768))
make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)
```

<div class="flex gap-4">
  <div>
    <img class="rounded-xl" src="https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"/>
    <figcaption class="mt-2 text-center text-sm text-gray-500">original image</figcaption>
  </div>
  <div>
    <img class="rounded-xl" src="https://github.com/Xiang-cd/DiffEdit-stable-diffusion/blob/main/assets/target.png?raw=true"/>
    <figcaption class="mt-2 text-center text-sm text-gray-500">edited image</figcaption>
  </div>
</div>

## Sourceì™€ target ì„ë² ë”© ìƒì„±í•˜ê¸°

Sourceì™€ target ì„ë² ë”©ì€ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëŒ€ì‹  [Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Flan-T5 ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:

```py
import torch
from transformers import AutoTokenizer, T5ForConditionalGeneration

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto", torch_dtype=torch.float16)
```

ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸í•  sourceì™€ target í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ì´ˆê¸° í…ìŠ¤íŠ¸ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.

```py
source_concept = "bowl"
target_concept = "basket"

source_text = f"Provide a caption for images containing a {source_concept}. "
"The captions should be in English and should be no longer than 150 characters."

target_text = f"Provide a caption for images containing a {target_concept}. "
"The captions should be in English and should be no longer than 150 characters."
```

ë‹¤ìŒìœ¼ë¡œ, í”„ë¡¬í”„íŠ¸ë“¤ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```py
@torch.no_grad()
def generate_prompts(input_prompt):
    input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids.to("cuda")

    outputs = model.generate(
        input_ids, temperature=0.8, num_return_sequences=16, do_sample=True, max_new_tokens=128, top_k=10
    )
    return tokenizer.batch_decode(outputs, skip_special_tokens=True)

source_prompts = generate_prompts(source_text)
target_prompts = generate_prompts(target_text)
print(source_prompts)
print(target_prompts)
```

> [!TIP]
> ë‹¤ì–‘í•œ í’ˆì§ˆì˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ì „ëµì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ìƒì„± ì „ëµ](https://huggingface.co/docs/transformers/main/en/generation_strategies) ê°€ì´ë“œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸ ì¸ì½”ë”©ì„ ìœ„í•´ [`StableDiffusionDiffEditPipeline`]ì—ì„œ ì‚¬ìš©í•˜ëŠ” í…ìŠ¤íŠ¸ ì¸ì½”ë” ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤:

```py
import torch
from diffusers import StableDiffusionDiffEditPipeline

pipeline = StableDiffusionDiffEditPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-1", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
pipeline.enable_vae_slicing()

@torch.no_grad()
def embed_prompts(sentences, tokenizer, text_encoder, device="cuda"):
    embeddings = []
    for sent in sentences:
        text_inputs = tokenizer(
            sent,
            padding="max_length",
            max_length=tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        )
        text_input_ids = text_inputs.input_ids
        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=None)[0]
        embeddings.append(prompt_embeds)
    return torch.concatenate(embeddings, dim=0).mean(dim=0).unsqueeze(0)

source_embeds = embed_prompts(source_prompts, pipeline.tokenizer, pipeline.text_encoder)
target_embeds = embed_prompts(target_prompts, pipeline.tokenizer, pipeline.text_encoder)
```

ë§ˆì§€ë§‰ìœ¼ë¡œ, ì„ë² ë”©ì„ [`~StableDiffusionDiffEditPipeline.generate_mask`] ë° [`~StableDiffusionDiffEditPipeline.invert`] í•¨ìˆ˜ì™€ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:

```diff
  from diffusers import DDIMInverseScheduler, DDIMScheduler
  from diffusers.utils import load_image, make_image_grid
  from PIL import Image

  pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config)
  pipeline.inverse_scheduler = DDIMInverseScheduler.from_config(pipeline.scheduler.config)

  img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
  raw_image = load_image(img_url).resize((768, 768))

  mask_image = pipeline.generate_mask(
      image=raw_image,
-     source_prompt=source_prompt,
-     target_prompt=target_prompt,
+     source_prompt_embeds=source_embeds,
+     target_prompt_embeds=target_embeds,
  )

  inv_latents = pipeline.invert(
-     prompt=source_prompt,
+     prompt_embeds=source_embeds,
      image=raw_image,
  ).latents

  output_image = pipeline(
      mask_image=mask_image,
      image_latents=inv_latents,
-     prompt=target_prompt,
-     negative_prompt=source_prompt,
+     prompt_embeds=target_embeds,
+     negative_prompt_embeds=source_embeds,
  ).images[0]
  mask_image = Image.fromarray((mask_image.squeeze()*255).astype("uint8"), "L")
  make_image_grid([raw_image, mask_image, output_image], rows=1, cols=3)
```

## ë°˜ì „ì„ ìœ„í•œ ìº¡ì…˜ ìƒì„±í•˜ê¸°

`source_prompt`ë¥¼ ìº¡ì…˜ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ë¶€ë¶„ì ìœ¼ë¡œ ë°˜ì „ëœ latentsë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, [BLIP](https://huggingface.co/docs/transformers/model_doc/blip) ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìº¡ì…˜ì„ ìë™ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ğŸ¤— Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ BLIP ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:

```py
import torch
from transformers import BlipForConditionalGeneration, BlipProcessor

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base", torch_dtype=torch.float16, low_cpu_mem_usage=True)
```

ì…ë ¥ ì´ë¯¸ì§€ì—ì„œ ìº¡ì…˜ì„ ìƒì„±í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤:

```py
@torch.no_grad()
def generate_caption(images, caption_generator, caption_processor):
    text = "a photograph of"

    inputs = caption_processor(images, text, return_tensors="pt").to(device="cuda", dtype=caption_generator.dtype)
    caption_generator.to("cuda")
    outputs = caption_generator.generate(**inputs, max_new_tokens=128)

    # ìº¡ì…˜ generator ì˜¤í”„ë¡œë“œ
    caption_generator.to("cpu")

    caption = caption_processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return caption
```

ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  `generate_caption` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ì´ë¯¸ì§€ì— ëŒ€í•œ ìº¡ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤:

```py
from diffusers.utils import load_image

img_url = "https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"
raw_image = load_image(img_url).resize((768, 768))
caption = generate_caption(raw_image, model, processor)
```

<div class="flex justify-center">
    <figure>
        <img class="rounded-xl" src="https://github.com/Xiang-cd/DiffEdit-stable-diffusion/raw/main/assets/origin.png"/>
        <figcaption class="text-center">generated caption: "a photograph of a bowl of fruit on a table"</figcaption>
    </figure>
</div>

ì´ì œ ìº¡ì…˜ì„ [`~StableDiffusionDiffEditPipeline.invert`] í•¨ìˆ˜ì— ë†“ì•„ ë¶€ë¶„ì ìœ¼ë¡œ ë°˜ì „ëœ latentsë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
