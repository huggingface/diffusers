# Copyright 2024 The SkyReels-V2 Authors and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from dataclasses import dataclass
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import numpy as np
import PIL.Image
import torch
from transformers import CLIPTextModel, CLIPTokenizer

from ...image_processor import VideoProcessor
from ...models import AutoencoderKLWan, WanTransformer3DModel
from ...schedulers import FlowUniPCMultistepScheduler
from ...utils import (
    BaseOutput,
    logging,
    replace_example_docstring,
)
from ...utils.torch_utils import randn_tensor
from ..pipeline_utils import DiffusionPipeline


logger = logging.get_logger(__name__)  # pylint: disable=invalid-name


EXAMPLE_DOC_STRING = """
    Examples:
        ```py
        >>> import torch
        >>> from diffusers import SkyReelsV2TextToVideoPipeline
        >>> from diffusers.utils import export_to_video

        >>> # Load the pipeline
        >>> pipe = SkyReelsV2TextToVideoPipeline.from_pretrained(
        ...     "HF_placeholder/SkyReels-V2-T2V-14B-540P", torch_dtype=torch.float16
        ... )
        >>> pipe = pipe.to("cuda")

        >>> prompt = "A panda eating bamboo on a rock, 4k, detailed"
        >>> video_frames = pipe(prompt, num_frames=97).frames  # Default num_frames is often higher for video
        >>> export_to_video(video_frames, "skyreels_v2_t2v.mp4")
        ```
"""


@dataclass
class SkyReelsV2PipelineOutput(BaseOutput):
    """
    Output class for SkyReels-V2 pipelines.

    Args:
        frames (`List[np.ndarray]` or `torch.Tensor`):
            List of video frames generated by the pipeline. Format depends on `output_type` argument. `np.ndarray` list
            is default. For `output_type="np"`: list of `np.ndarray` of shape `(num_frames, height, width,
            num_channels)` with values in [0, 255]. For `output_type="tensor"`: `torch.Tensor` of shape `(batch_size,
            num_frames, channels, height, width)` with values in [0, 1]. For `output_type="pil"`: list of
            `PIL.Image.Image`.
    """

    frames: Union[List[np.ndarray], torch.Tensor, List[PIL.Image.Image]]


class SkyReelsV2TextToVideoPipeline(DiffusionPipeline):
    """
    Pipeline for text-to-video generation using SkyReels-V2.

    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods
    implemented for all pipelines (downloading, saving, running on a specific device, etc.).

    The pipeline is based on the Wan 2.1 architecture (WanTransformer3DModel, AutoencoderKLWan). It expects checkpoints
    saved in the standard diffusers format, typically including subfolders: `vae`, `text_encoder`, `tokenizer`,
    `transformer`, `scheduler`.

    Args:
        vae ([`AutoencoderKLWan`]):
            Variational Auto-Encoder (VAE) model capable of encoding and decoding videos in latent space. Expected to
            handle 3D inputs (temporal dimension).
        text_encoder ([`~transformers.CLIPTextModel`]):
            Frozen text-encoder. SkyReels-V2 typically uses CLIP (e.g., `openai/clip-vit-large-patch14`).
        tokenizer ([`~transformers.CLIPTokenizer`]):
            Tokenizer corresponding to the `text_encoder`.
        transformer ([`WanTransformer3DModel`]):
            The core diffusion transformer model that denoises latents based on text conditioning.
        scheduler ([`FlowUniPCMultistepScheduler`]):
            A scheduler compatible with the Flow Matching framework used by SkyReels-V2.
        video_processor ([`VideoProcessor`]):
            Processor for converting VAE output latents to standard video formats (np, tensor, pil).
    """

    model_cpu_offload_seq = "text_encoder->transformer->vae"
    _callback_tensor_inputs = ["latents", "prompt_embeds", "negative_prompt_embeds"]

    def __init__(
        self,
        vae: AutoencoderKLWan,
        text_encoder: CLIPTextModel,
        tokenizer: CLIPTokenizer,
        transformer: WanTransformer3DModel,
        scheduler: FlowUniPCMultistepScheduler,
        video_processor: VideoProcessor,
    ):
        super().__init__()
        self.register_modules(
            vae=vae,
            text_encoder=text_encoder,
            tokenizer=tokenizer,
            transformer=transformer,
            scheduler=scheduler,
            video_processor=video_processor,
        )
        self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)

    def enable_vae_slicing(self):
        r"""
        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to
        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.
        """
        self.vae.enable_slicing()

    def disable_vae_slicing(self):
        r"""
        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to
        computing decoding in one step.
        """
        self.vae.disable_slicing()

    def enable_vae_tiling(self):
        r"""
        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to
        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow
        processing larger images.
        """
        self.vae.enable_tiling()

    def disable_vae_tiling(self):
        r"""
        Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to
        computing decoding in one step.
        """
        self.vae.disable_tiling()

    def encode_prompt(
        self,
        prompt: Union[str, List[str]],
        device: torch.device,
        num_videos_per_prompt: int,
        do_classifier_free_guidance: bool,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        max_sequence_length: Optional[int] = None,
        lora_scale: Optional[float] = None,
    ):
        r"""
        Encodes the prompt into text encoder hidden states.

        Args:
            prompt (`str` or `List[str]`):
                The prompt or prompts to guide video generation.
            device: (`torch.device`):
                torch device.
            num_videos_per_prompt (`int`):
                Number of videos to generate per prompt.
            do_classifier_free_guidance (`bool`):
                Whether to use classifier-free guidance.
            negative_prompt (`str` or `List[str]`, *optional*):
                The negative prompt or prompts. Ignored if `do_classifier_free_guidance` is `False`.
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Higher priority than `prompt`.
            negative_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative text embeddings. Higher priority than `negative_prompt`.
            max_sequence_length (`int`, *optional*):
                Maximum sequence length for tokenizer. Defaults to `self.tokenizer.model_max_length`.
            lora_scale (`float`, *optional*):
                A LoRA scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.
        """
        # Set LoRA scale
        lora_scale = lora_scale or self.lora_scale

        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]

        # Define tokenizer parameters
        if max_sequence_length is None:
            max_sequence_length = self.tokenizer.model_max_length

        # Get prompt text embeddings
        if prompt_embeds is None:
            text_inputs = self.tokenizer(
                prompt,
                padding="max_length",
                max_length=max_sequence_length,
                truncation=True,
                return_tensors="pt",
            )
            text_input_ids = text_inputs.input_ids
            prompt_embeds = self.text_encoder(
                text_input_ids.to(device),
                attention_mask=text_inputs.attention_mask.to(device),
                output_hidden_states=False,
            )[0]

        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)

        bs_embed, seq_len, _ = prompt_embeds.shape
        # duplicate text embeddings for each generation per prompt, using mps friendly method
        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)
        prompt_embeds = prompt_embeds.view(bs_embed * num_videos_per_prompt, seq_len, -1)

        # Get negative prompt embeddings
        if do_classifier_free_guidance:
            if negative_prompt_embeds is None:
                if negative_prompt is None:
                    negative_prompt = ""  # Use empty string
                # Do not repeat for multiple prompts if it is empty string
                if isinstance(negative_prompt, str) and negative_prompt == "":
                    negative_prompt = [negative_prompt] * batch_size
                elif isinstance(negative_prompt, str):
                    negative_prompt = [negative_prompt]  # Already handled single string case

                if isinstance(negative_prompt, list) and batch_size != len(negative_prompt):
                    raise ValueError(
                        f"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`: {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches batch size of `prompt`."
                    )

                uncond_input = self.tokenizer(
                    negative_prompt,
                    padding="max_length",
                    max_length=max_sequence_length,
                    truncation=True,
                    return_tensors="pt",
                )
                negative_prompt_embeds = self.text_encoder(
                    uncond_input.input_ids.to(device),
                    attention_mask=uncond_input.attention_mask.to(device),
                    output_hidden_states=False,
                )[0]
            negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)

            # duplicate negative embeddings for each generation per prompt, using mps friendly method
            bs_embed, seq_len, _ = negative_prompt_embeds.shape
            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_videos_per_prompt, 1)
            negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_videos_per_prompt, seq_len, -1)

            # For classifier free guidance, we need to do two forward passes.
            # Here we concatenate the unconditional and text embeddings into a single batch
            # to avoid doing two forward passes
            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])

        return prompt_embeds

    def decode_latents(self, latents: torch.Tensor) -> torch.Tensor:
        """
        Decode video latents using the VAE.

        Args:
            latents (`torch.Tensor`): Latents of shape (batch, channels, latent_frames, height, width).

        Returns:
            `torch.Tensor`: Decoded video frames of shape (batch, frames, channels, height, width) as float tensor [0,
            1].
        """
        # AutoencoderKLWan expects B, C, F, H, W latents directly
        video = self.vae.decode(latents).sample

        # Output is likely B, C, F, H, W in range [-1, 1]
        # Convert to B, F, C, H, W and range [0, 1]
        video = video.permute(0, 2, 1, 3, 4)  # B, F, C, H, W
        video = (video / 2 + 0.5).clamp(0, 1)
        return video

    def prepare_latents(
        self,
        batch_size: int,
        num_channels_latents: int,
        num_frames: int,
        height: int,
        width: int,
        dtype: torch.dtype,
        device: torch.device,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        """
        Prepare latent variables from noise for the diffusion process.

        Args:
            batch_size (`int`):
                Number of samples to generate.
            num_channels_latents (`int`):
                Number of channels in the latent space (e.g., `self.vae.config.latent_channels`).
            num_frames (`int`):
                Number of video frames *in the final output*.
            height (`int`):
                Height of the generated video in pixels.
            width (`int`):
                Width of the generated video in pixels.
            dtype (`torch.dtype`):
                Data type for the latents.
            device (`torch.device`):
                Device for the latents.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                PyTorch Generator object(s).
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents.

        Returns:
            `torch.Tensor`: Initial latent variables (noise) scaled by the scheduler's `init_noise_sigma`.
        """
        vae_scale_factor = self.vae_scale_factor
        shape_spatial = (
            batch_size,
            num_channels_latents,
            height // vae_scale_factor,
            width // vae_scale_factor,
        )

        # Calculate temporal downsampling factor from VAE config
        if hasattr(self.vae.config, "temperal_downsample") and self.vae.config.temperal_downsample is not None:
            num_true_temporal_downsamples = sum(1 for td in self.vae.config.temperal_downsample if td)
            temporal_downsample_factor = 2**num_true_temporal_downsamples
        else:
            temporal_downsample_factor = 4  # Default from original SkyReels
            logger.warning(
                "VAE config does not have 'temperal_downsample'. Using default temporal_downsample_factor=4."
            )

        # Calculate the number of latent frames
        num_latent_frames = (num_frames - 1) // temporal_downsample_factor + 1
        shape = (shape_spatial[0], shape_spatial[1], num_latent_frames, shape_spatial[2], shape_spatial[3])

        if isinstance(generator, list) and len(generator) != batch_size:
            raise ValueError(
                f"You have passed a list of generators of length {len(generator)}, but requested an effective batch"
                f" size of {batch_size}. Make sure the batch size matches the length of the generators."
            )

        if latents is None:
            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)
        else:
            if latents.shape != shape:
                raise ValueError(f"Unexpected latents shape: {latents.shape}. Expected {shape}.")
            latents = latents.to(device)

        # scale the initial noise by the standard deviation required by the scheduler
        latents = latents * self.scheduler.init_noise_sigma
        return latents

    @replace_example_docstring(EXAMPLE_DOC_STRING)
    def __call__(
        self,
        prompt: Union[str, List[str]] = None,
        num_frames: int = 97,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: int = 30,
        guidance_scale: float = 6.0,
        negative_prompt: Optional[Union[str, List[str]]] = None,
        num_videos_per_prompt: Optional[int] = 1,
        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,
        latents: Optional[torch.Tensor] = None,
        prompt_embeds: Optional[torch.Tensor] = None,
        negative_prompt_embeds: Optional[torch.Tensor] = None,
        max_sequence_length: Optional[int] = None,
        output_type: Optional[str] = "np",
        return_dict: bool = True,
        callback: Optional[Callable[[int, int, torch.Tensor], None]] = None,
        callback_steps: int = 1,
        cross_attention_kwargs: Optional[Dict[str, Any]] = None,
        custom_shift: Optional[float] = 8.0,
    ) -> Union[SkyReelsV2PipelineOutput, Tuple]:
        """
        The call function to the pipeline for text-to-video generation.

        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt(s) to guide video generation.
            num_frames (`int`, *optional*, defaults to 97):
                The number of frames to generate.
            height (`int`, *optional*):
                The height in pixels of the generated video. Defaults to VAE output size.
            width (`int`, *optional*):
                The width in pixels of the generated video. Defaults to VAE output size.
            num_inference_steps (`int`, *optional*, defaults to 30):
                The number of denoising steps.
            guidance_scale (`float`, *optional*, defaults to 6.0):
                Scale for classifier-free guidance. `guidance_scale <= 1` disables CFG.
            negative_prompt (`str` or `List[str]`, *optional*):
                The negative prompt(s) for CFG.
            num_videos_per_prompt (`int`, *optional*, defaults to 1):
                Number of videos to generate per prompt.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                Generator(s) for deterministic generation.
            latents (`torch.Tensor`, *optional*):
                Pre-generated noisy latents. Shape should match `(batch_size * num_videos_per_prompt, C, F_latent,
                H_latent, W_latent)`.
            prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated text embeddings. Shape `(batch_size * num_videos_per_prompt, seq_len, embed_dim)`.
            negative_prompt_embeds (`torch.Tensor`, *optional*):
                Pre-generated negative text embeddings. Shape `(batch_size * num_videos_per_prompt, seq_len,
                embed_dim)`.
            max_sequence_length (`int`, *optional*):
                Maximum sequence length for tokenizer. Defaults to `self.tokenizer.model_max_length`.
            output_type (`str`, *optional*, defaults to `"np"`):
                The output format of the generated video. Choose between `"np"` (list of np.ndarray), `"tensor"`
                (torch.Tensor), or `"pil"` (list of PIL.Image.Image).
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether to return a [`~pipelines.skyreels_v2.SkyReelsV2PipelineOutput`] or a tuple.
            callback (`Callable`, *optional*):
                A function called every `callback_steps` steps during inference.
            callback_steps (`int`, *optional*, defaults to 1):
                Frequency of callback calls.
            cross_attention_kwargs (`dict`, *optional*):
                Arguments passed to the attention processor.
            custom_shift (`float`, *optional*, defaults to 8.0):
                The "shift" parameter for the `FlowUniPCMultistepScheduler`. Controls emphasis on diffusion trajectory
                parts. Corresponds to `shift` in the original SkyReels repository.

        Examples:
            ```py
            >>> # Example usage is included in the EXAMPLE_DOC_STRING variable
            ```

        Returns:
            [`~pipelines.skyreels_v2.SkyReelsV2PipelineOutput`] or `tuple`:
                If `return_dict` is `True`, returns [`~pipelines.skyreels_v2.SkyReelsV2PipelineOutput`]. Otherwise
                returns a tuple `(video,)` where `video` format depends on `output_type`.
        """
        # 0. Default height and width to VAE constraints
        # Note: WanTransformer3DModel config doesn't have a standard 'sample_size'.
        # Relying on VAE scale factor might be sufficient if input H/W are provided or inferred.
        # Let's keep the defaults based on user input or raise error if not determinable.
        if height is None or width is None:
            # Height and width are required for this pipeline.
            raise ValueError("Please provide `height` and `width` for video generation.")

        # Ensure height and width are multiples of VAE scale factor
        height = height - height % self.vae_scale_factor
        width = width - width % self.vae_scale_factor
        if height == 0 or width == 0:
            raise ValueError("Provided height and width are too small.")

        # 1. Check inputs
        self.check_inputs(
            prompt, height, width, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds
        )

        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]

        device = self._execution_device
        # Implement LoRA scale handling - requires PeftAdapterMixin setup if LoRA is used
        lora_scale = cross_attention_kwargs.get("scale", None) if cross_attention_kwargs is not None else None
        do_classifier_free_guidance = guidance_scale > 1.0

        # 3. Encode input prompt
        prompt_embeds = self.encode_prompt(
            prompt,
            device,
            num_videos_per_prompt,
            do_classifier_free_guidance,
            negative_prompt,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
            max_sequence_length=max_sequence_length,
            lora_scale=lora_scale,
        )

        # 4. Prepare timesteps
        self.scheduler.set_timesteps(num_inference_steps, device=device, shift=custom_shift)
        timesteps = self.scheduler.timesteps

        # 5. Prepare latent variables
        num_channels_latents = self.vae.config.latent_channels
        latents = self.prepare_latents(
            batch_size * num_videos_per_prompt,
            num_channels_latents,
            num_frames,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            latents=latents,
        )

        # 6. Denoising loop
        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                # expand the latents if we are doing classifier free guidance
                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)

                # predict the noise residual
                model_pred = self.transformer(
                    latent_model_input,
                    timestep=t,
                    encoder_hidden_states=prompt_embeds,
                    cross_attention_kwargs=cross_attention_kwargs,
                ).sample

                # perform guidance
                if do_classifier_free_guidance:
                    model_pred_uncond, model_pred_text = model_pred.chunk(2)
                    model_pred = model_pred_uncond + guidance_scale * (model_pred_text - model_pred_uncond)

                # compute the previous noisy sample x_t -> x_t-1
                latents = self.scheduler.step(model_pred, t, latents).prev_sample

                # call the callback, if provided
                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):
                    progress_bar.update()
                    if callback is not None and i % callback_steps == 0:
                        step_idx = i // getattr(self.scheduler, "order", 1)
                        callback(step_idx, t, latents)

        # 7. Post-processing
        video_tensor = self.decode_latents(latents)  # B, F, C, H, W float [0,1]

        # 8. Process video output
        video = self.video_processor.postprocess_video(video_tensor, output_type=output_type)

        # Offload all models
        self.maybe_free_model_hooks()

        if not return_dict:
            return (video,)

        return SkyReelsV2PipelineOutput(frames=video)
