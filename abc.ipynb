{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.utils import load_image\n",
    "from diffusers.pipelines.pipeline_utils import numpy_to_pil\n",
    "\n",
    "input_image = load_image(\n",
    " \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/kandinsky3/t2i.png\"\n",
    ")\n",
    "midpoint_image = torch.randn(1, 4, 64, 64)\n",
    "image_processor = VaeImageProcessor(\n",
    "    vae_scale_factor=2**3,\n",
    "    vae_latent_channels=4,\n",
    "    resample=\"bicubic\",\n",
    "    reducing_gap=1\n",
    ")\n",
    "\n",
    "def preprocess(image, image_processor=None, branch=\"default\"):\n",
    "    if branch == \"main\":\n",
    "        arr = np.array(image.convert(\"RGB\"))\n",
    "        arr = arr.astype(np.float32) / 127.5 - 1\n",
    "        arr = np.transpose(arr, [2, 0, 1])\n",
    "        image = torch.from_numpy(arr).unsqueeze(0)\n",
    "        return image\n",
    "    \n",
    "    image = image_processor.preprocess(image)\n",
    "    return image\n",
    "\n",
    "def postprocess(image, image_processor=None, branch=\"default\"):\n",
    "    if branch==\"main\":\n",
    "        image = image * 0.5 + 0.5\n",
    "        image = image.clamp(0, 1)\n",
    "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "        image = numpy_to_pil(image)[0]\n",
    "        return image\n",
    "    \n",
    "    image = image_processor.postprocess(image)[0]\n",
    "    return image\n",
    "\n",
    "if torch.equal(preprocess(input_image, image_processor), preprocess(input_image, branch=\"main\")):\n",
    "    print(\"Preprocessed images are exactly the same.\")\n",
    "\n",
    "if list(postprocess(midpoint_image, image_processor).getdata()) == list(postprocess(midpoint_image, branch=\"main\").getdata()):\n",
    "    print(\"Postprocessed images are exactly the same.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "image = Image.open('/home/ishan/Downloads/test4.jpg')\n",
    "image1 = Image.open('/home/ishan/Downloads/test5.jpg')\n",
    "\n",
    "if list(image.getdata()) == list(image1.getdata()):\n",
    "    print(\"Images are exactly the same.\")\n",
    "\n",
    "ImageChops.difference(image, image1).save('/home/ishan/Downloads/diff.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.embeddings import PatchEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embed = PatchEmbed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "print(patch_embed(t).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPImageProcessor\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the processor\n",
    "image_processor = CLIPImageProcessor(\n",
    "    do_resize=True,\n",
    "    resample=Image.BICUBIC,\n",
    "    do_center_crop=True,\n",
    "    do_normalize=True,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5]\n",
    ")\n",
    "\n",
    "def process_image(control_signal, hw):\n",
    "    \"\"\"Process and return the image tensor.\"\"\"\n",
    "    height, width = int(hw[0]), int(hw[1])\n",
    "    control_signal = control_signal.convert(\"RGB\")\n",
    "    \n",
    "    processed = image_processor(\n",
    "        control_signal,\n",
    "        size={\"height\": height, \"width\": width},\n",
    "        crop_size={\"height\": height, \"width\": width},\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"pixel_values\"]  # Returns tensor (1, 3, H, W)\n",
    "\n",
    "    return processed\n",
    "\n",
    "def show_processed_image(processed_image):\n",
    "    \"\"\"Denormalize and show the processed image.\"\"\"\n",
    "    mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
    "\n",
    "    # Reverse normalization: x' = x * std + mean\n",
    "    image = processed_image[0] * std + mean  # (3, H, W)\n",
    "    image = image.clamp(0, 1)  # Ensure values are in [0,1]\n",
    "    \n",
    "    # Convert to NumPy and transpose (C, H, W) -> (H, W, C)\n",
    "    image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image_np)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image = Image.open(\"/home/ishan/Downloads/ishan.jpg\")  # Replace with actual image path\n",
    "print(image.size)\n",
    "hw = (224, 224)  # Desired height and width\n",
    "\n",
    "processed_image = process_image(image, hw)\n",
    "print(processed_image.shape)\n",
    "show_processed_image(processed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/richzhang/hed/releases/download/0.2.0/hed_pretrained_bsds.caffemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load the ControlNet model trained with HED edge detection\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_softedge\", torch_dtype=torch.float16)\n",
    "\n",
    "# Load the base model\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Prepare input image\n",
    "image_url = \"https://drive.google.com/file/d/1jqnWqB2z47UKSrAUgLhkpEDSodtQqrFs/view?usp=sharing\"\n",
    "image = Image.open(BytesIO(requests.get(image_url).content)).convert(\"RGB\")\n",
    "\n",
    "# Generate the HED soft edges\n",
    "output = pipe(image, num_inference_steps=50).images[0]\n",
    "\n",
    "# Save the result\n",
    "output.save(\"soft_edge_output.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "control_image = load_image(\n",
    "    \"https://huggingface.co/ishan24/Sana_600M_1024px_ControlNet_diffusers/resolve/main/hed_result.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img = np.array(control_image, dtype=np.float32)\n",
    "img = img.flatten()\n",
    "\n",
    "np.concatenate((img[:16], img[-16:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def prepare_image(pil_image, w=512, h=512):\n",
    "    pil_image = pil_image.resize((w, h), resample=Image.BICUBIC, reducing_gap=1)\n",
    "    arr = np.array(pil_image.convert(\"RGB\"))\n",
    "    arr = arr.astype(np.float32) / 127.5 - 1\n",
    "    arr = np.transpose(arr, [2, 0, 1])\n",
    "    image = torch.from_numpy(arr).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def prepare_image1(image, w=512, h=512):\n",
    "    image = image_processor.preprocess(image, h, w)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = prepare_image(control_image)\n",
    "img2 = prepare_image1(control_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = img1 == img2\n",
    "mask = mask.flatten()\n",
    "mask.sum() == len(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def _assign_components_to_devices(\n",
    "    module_sizes: Dict[str, float], device_memory: Dict[str, float], device_mapping_strategy: str = \"balanced\"\n",
    "):\n",
    "    device_ids = list(device_memory.keys())\n",
    "    device_cycle = device_ids + device_ids[::-1]\n",
    "    device_memory = device_memory.copy()\n",
    "\n",
    "    device_id_component_mapping = {}\n",
    "    current_device_index = 0\n",
    "    for component in module_sizes:\n",
    "        print(current_device_index % len(device_cycle))\n",
    "        device_id = device_cycle[current_device_index % len(device_cycle)]\n",
    "        component_memory = module_sizes[component]\n",
    "        curr_device_memory = device_memory[device_id]\n",
    "        print(\"Component Memory\", component_memory, flush=True)\n",
    "        print(\"Current Device Memory\", curr_device_memory, flush=True)\n",
    "        # If the GPU doesn't fit the current component offload to the CPU.\n",
    "        if component_memory > curr_device_memory:\n",
    "            device_id_component_mapping[\"cpu\"] = [component]\n",
    "        else:\n",
    "            if device_id not in device_id_component_mapping:\n",
    "                device_id_component_mapping[device_id] = [component]\n",
    "            else:\n",
    "                device_id_component_mapping[device_id].append(component)\n",
    "\n",
    "            # Update the device memory.\n",
    "            device_memory[device_id] -= component_memory\n",
    "            current_device_index += 1\n",
    "    print(\"Device ID Component Mapping\", device_id_component_mapping, flush=True)\n",
    "    return device_id_component_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_assign_components_to_devices({'text_encoder': 522869032, 'transformer': 13834957424, 'vae': 6222450053}, {0: 15540027392, 1: 15500181504})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install transformers==4.49.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
    "import torch\n",
    "\n",
    "model = CLIPTextModelWithProjection.from_pretrained(\"stabilityai/stable-diffusion-3.5-medium\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_projection.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_model.final_layer_norm.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stable-diffusion-3.5-medium\", subfolder=\"tokenizer\", torch_dtype=\"bfloat16\")\n",
    "text_inputs = tokenizer(\n",
    "    \"Hello world!\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=16,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(text_inputs.input_ids.dtype, text_inputs.input_ids.device)  # torch.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**text_inputs, output_hidden_states=True)\n",
    "outputs.text_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.randn(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.tensor([1,2], dtype=torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def unpack_weights(uint8tensor, bits):\n",
    "    num_values = uint8tensor.shape[0] * 8 // bits\n",
    "\n",
    "    num_steps = 8 // bits\n",
    "\n",
    "    unpacked_tensor = torch.zeros((num_values), dtype=torch.uint8)\n",
    "\n",
    "    unpacked_idx = 0\n",
    "\n",
    "    # 1 0 3 2 - 01 00 11 10\n",
    "\n",
    "    # [00000000 00000000 00000000 00000000]\n",
    "    # [10110001 00101100 00001011 00000010]\n",
    "    # [00000001 00000000 00000011 00000010]\n",
    "\n",
    "    # 10110001\n",
    "    # 00000011\n",
    "    \n",
    "    # 00000001\n",
    "\n",
    "    # 1: [10110001]\n",
    "    # 2: [00101100]\n",
    "    # 3: [00001011]\n",
    "\n",
    "    mask = 2 ** bits - 1\n",
    "\n",
    "    for i in range(uint8tensor.shape[0]):\n",
    "        for j in range(num_steps):\n",
    "            unpacked_tensor[unpacked_idx] |= uint8tensor[i] >> (bits * j)\n",
    "            unpacked_idx += 1\n",
    "    \n",
    "    print(mask)\n",
    "    print(unpacked_tensor)\n",
    "    unpacked_tensor &= mask\n",
    "    return unpacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_tensor = torch.tensor([177, 255], \n",
    "                               dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer should be: torch.tensor([1, 0, 3, 2, 3, 3, 3, 3]\n",
    "unpack_weights(unpacked_tensor, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "with init_empty_weights():\n",
    "    checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "    model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "inputs = tokenizer.encode(\"Gravity is\", return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelopt.torch.quantization as mtq\n",
    "import modelopt.torch.opt.conversion as mtc\n",
    "from copy import deepcopy\n",
    "\n",
    "mq = deepcopy(model)\n",
    "config = mtq.FP8_DEFAULT_CFG\n",
    "mtq.quantize(mq,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mq.model.layers[0].self_attn.q_proj.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_memory_footprint()/1e6, mq.get_memory_footprint()/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "hidden = torch.randn((2,32,32,32))\n",
    "encoder = torch.randn((2,10,300,2304))\n",
    "timestep = torch.Tensor([0,0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(hidden, encoder, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "timestep = torch.Tensor([0,1])\n",
    "\n",
    "scm_timestep = torch.sin(timestep) / (torch.cos(timestep) + torch.sin(timestep))\n",
    "\n",
    "scm_timestep_expanded = scm_timestep.view(-1, 1, 1, 1)\n",
    "\n",
    "print(scm_timestep_expanded.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.quantizers.quantization_config import NVIDIAModelOptConfig\n",
    "quant_config = NVIDIAModelOptConfig(quant_type=\"FP8_WO\", block)\n",
    "\n",
    "quant_config.get_config_from_quant_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"git+https://github.com/ishan-modi/diffusers.git@add-trtquant-backend\"\n",
    "# !pip install \"git+https://github.com/ishan-modi/TensorRT-Model-Optimizer.git@fixes-hfargs-order#egg=nvidia-modelopt[torch, onnx]\"\n",
    "\n",
    "import torch\n",
    "from diffusers import SanaTransformer2DModel\n",
    "from diffusers.quantizers.quantization_config import NVIDIAModelOptConfig\n",
    "\n",
    "checkpoint = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "\n",
    "# quant_config = {\"quant_type\": \"INT4_WO\", \"quant_method\": \"modelopt\", \"modules_to_not_convert\": [\"conv\"], \"block_quantize\": True}\n",
    "quant_config = {\"quant_type\": \"FP8_WO\", \"quant_method\": \"modelopt\", \"modules_to_not_convert\": [\"conv\"]}\n",
    "quant_config = NVIDIAModelOptConfig(quant_type=\"FP8_WO\")\n",
    "model = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\", quantization_config=quant_config, torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "hidden = torch.randn((2,32,32,32), dtype=torch.bfloat16).to('cuda')\n",
    "encoder = torch.randn((2,10,300,2304), dtype=torch.bfloat16).to('cuda')\n",
    "timestep = torch.Tensor([0,0]).to('cuda')\n",
    "with torch.no_grad():\n",
    "    output = model(hidden, encoder, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Conv2d(128, 128, 3)\n",
    "        self.l2 = nn.Conv2d(128, 256, 5)\n",
    "        self.l3 = nn.Conv2d(256, 128, 3)\n",
    "        \n",
    "        self._init_weights()  # Call weight initialization\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights for all layers\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l3(self.l2(self.l1(x)))\n",
    "\n",
    "model = DummyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.quantization import quantize, compress\n",
    "from modelopt.torch.quantization.config import FP8_DEFAULT_CFG\n",
    "\n",
    "FP8_DEFAULT_CFG['quant_cfg']['*weight_quantizer'].update({'fake_quant': False})\n",
    "FP8_DEFAULT_CFG['quant_cfg']['*input_quantizer'].update({'enable': False})\n",
    "model = quantize(model, FP8_DEFAULT_CFG)\n",
    "compress(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from modelopt.torch.quantization.utils import export_torch_mode\n",
    "\n",
    "x = torch.randn((1,128, 10, 10))\n",
    "with torch.no_grad():\n",
    "    with export_torch_mode():\n",
    "        out = model(x)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import SanaTransformer2DModel\n",
    "\n",
    "checkpoint = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "\n",
    "# quant_config = {\"quant_type\": \"INT4_WO\", \"quant_method\": \"modelopt\", \"modules_to_not_convert\": [\"conv\"], \"block_quantize\": True}\n",
    "quant_config = {\"quant_type\": \"FP8_WO\", \"quant_method\": \"modelopt\", \"modules_to_not_convert\": [\"conv\"]}\n",
    "model = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\", quantization_config=quant_config, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_hf_checkpoint\n",
    "\n",
    "with torch.inference_mode():\n",
    "    export_hf_checkpoint(\n",
    "        model,  # The quantized model.\n",
    "        \".\",  # The directory where the exported files will be stored.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.randn((2,32,32,32), dtype=torch.bfloat16)\n",
    "encoder = torch.randn((2,10,300,2304), dtype=torch.bfloat16)\n",
    "timestep = torch.Tensor([0,0])\n",
    "with torch.no_grad():\n",
    "    output = model(hidden, encoder, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import SanaTransformer2DModel\n",
    "from diffusers.quantizers.quantization_config import NVIDIAModelOptConfig\n",
    "\n",
    "checkpoint = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "\n",
    "quant_config = {\"quant_type\": \"FP8\", \"quant_method\": \"modelopt\", \"block_quantize\": 128, \"channel_quantize\": -1}\n",
    "\n",
    "quant_config = NVIDIAModelOptConfig(**quant_config)\n",
    "print(quant_config.get_config_from_quant_type())\n",
    "quant_model = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\", quantization_config=quant_config, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.get_memory_footprint() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import SanaTransformer2DModel\n",
    "from diffusers.quantizers.quantization_config import NVIDIAModelOptConfig\n",
    "\n",
    "checkpoint = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "\n",
    "quant_config_fp8 = {\"quant_type\": \"FP8\", \"quant_method\": \"modelopt\"}\n",
    "quant_config_int4 = {\"quant_type\": \"INT4\", \"quant_method\": \"modelopt\", \"block_quantize\": 128, \"channel_quantize\": -1, \"modules_to_not_convert\": [\"conv\"]}\n",
    "quant_config_nf4 = {\"quant_type\": \"NF4\", \"quant_method\": \"modelopt\", \"block_quantize\": 128, \"channel_quantize\": -1, \"modules_to_not_convert\": [\"conv\"]}\n",
    "quant_config_nvfp4 = {\"quant_type\": \"NVFP4\", \"quant_method\": \"modelopt\", \"block_quantize\": 128, \"channel_quantize\": -1, \"modules_to_not_convert\": [\"conv\"]}\n",
    "\n",
    "quant_config_fp8 = NVIDIAModelOptConfig(**quant_config_fp8)\n",
    "quant_config_int4 = NVIDIAModelOptConfig(**quant_config_int4)\n",
    "quant_config_nf4 = NVIDIAModelOptConfig(**quant_config_nf4)\n",
    "quant_config_nvfp4 = NVIDIAModelOptConfig(**quant_config_nvfp4)\n",
    "\n",
    "print(quant_config_fp8.get_config_from_quant_type())\n",
    "quant_model_fp8 = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\", quantization_config=quant_config_fp8, torch_dtype=torch.bfloat16)\n",
    "# print(quant_config_int4.get_config_from_quant_type())\n",
    "# quant_model_int4 = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\", quantization_config=quant_config_int4, torch_dtype=torch.bfloat16).to('cuda')\n",
    "# print(quant_config_nf4.get_config_from_quant_type())\n",
    "# quant_model_nf4 = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\", quantization_config=quant_config_nf4, torch_dtype=torch.bfloat16).to('cuda')\n",
    "# print(quant_config_nvfp4.get_config_from_quant_type())\n",
    "# quant_model_nvfp4 = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\", quantization_config=quant_config_nvfp4, torch_dtype=torch.bfloat16).to('cuda')\n",
    "# model = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\", torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "print(\"FP8 Model Memory Footprint: \", quant_model_fp8.get_memory_footprint() / 1e6)\n",
    "# print(\"INT4 Model Memory Footprint: \", quant_model_int4.get_memory_footprint() / 1e6)\n",
    "# print(\"NF4 Model Memory Footprint: \", quant_model_nf4.get_memory_footprint() / 1e6)\n",
    "# print(\"NVFP4 Model Memory Footprint: \", quant_model_nvfp4.get_memory_footprint() / 1e6)\n",
    "# print(\"FP8 Model Memory Footprint: \", quant_model_fp8.get_memory_footprint() / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "print(isinstance(quant_model_fp8.patch_embed.proj.weight, nn.Parameter))\n",
    "\n",
    "quant_model_fp8.patch_embed.proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model_fp8.push_to_hub(\"ishan24/test_modelopt_quant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.opt import enable_huggingface_checkpointing\n",
    "\n",
    "enable_huggingface_checkpointing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model_fp8.save_pretrained(\"test_modelopt_quant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import SanaTransformer2DModel\n",
    "import modelopt.torch.opt as mto\n",
    "mto.enable_huggingface_checkpointing()\n",
    "\n",
    "qmodel = SanaTransformer2DModel.from_pretrained(\"/home/ishan/vscode-workspace/diffusers/test_modelopt_quant\")\n",
    "qmodel.get_memory_footprint() / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from diffusers import SanaTransformer2DModel\n",
    "import modelopt.torch.quantization as mtq\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "# checkpoint = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "# model = SanaTransformer2DModel.from_pretrained(checkpoint, subfolder=\"transformer\")\n",
    "\n",
    "config = mtq.FP8_DEFAULT_CFG\n",
    "config['quant_cfg']['*weight_quantizer'].update({'fake_quant': False})\n",
    "mq = mtq.quantize(model, config)\n",
    "mtq.compress(mq)\n",
    "\n",
    "mq.save_pretrained(\"test_modelopt_quant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from diffusers import SanaTransformer2DModel\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "\n",
    "checkpoint = \"/home/ishan/vscode-workspace/diffusers/test_modelopt_quant\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "# model = SanaTransformer2DModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Union\n",
    "from modelopt.torch.quantization.utils import reduce_block_padding, reduce_amax, reduce_block_amax, convert_quantization_axis_to_reduce_axis\n",
    "\n",
    "def quantize(\n",
    "        input: torch.Tensor,\n",
    "        scales: torch.Tensor = None,\n",
    "        axis: Union[tuple, int, None] = None,\n",
    "        block_sizes: dict = None,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Converting a tensor to a quantized format based on FP8 quantization. Only E4M3 is supported.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): The input tensor to be quantized.\n",
    "            scales (torch.Tensor): The scales for quantization.\n",
    "            axis: The dimensions to reduce for quantization. None or int or tuple of ints.\n",
    "            block_sizes (dict): A dictionary specifying the block size for each dimension.\n",
    "\n",
    "        Note: One can only provide axis or block_sizes for FP8 quantization.\n",
    "\n",
    "        Returns:\n",
    "            tuple: FP8QTensor, scales\n",
    "        \"\"\"\n",
    "        original_input = input\n",
    "\n",
    "        # If block_sizes is provided, pad the input so that each dimension is divisible by the block size.\n",
    "        if block_sizes:\n",
    "            input = reduce_block_padding(input, block_sizes)\n",
    "\n",
    "        print(input.shape)\n",
    "        # Compute scales if not provided\n",
    "        if scales is None:\n",
    "            if block_sizes:\n",
    "                amax = reduce_block_amax(input, block_sizes)\n",
    "            else:\n",
    "                reduce_axis = convert_quantization_axis_to_reduce_axis(input, axis)\n",
    "                amax = reduce_amax(input, axis=reduce_axis)\n",
    "            scales = amax / 448.0  # Consider parameterizing the divisor if needed\n",
    "\n",
    "        # Determine the expected scales shape from the (possibly padded) input\n",
    "        expected_shape = list(input.shape)\n",
    "        expanded_scales = scales\n",
    "        print(scales.shape, expanded_scales.shape)\n",
    "        if block_sizes:\n",
    "            for dim, block_size in block_sizes.items():\n",
    "                # Convert negative indices to positive ones.\n",
    "                dim = dim if dim >= 0 else len(input.shape) + dim\n",
    "                # After padding, this should always hold.\n",
    "                assert input.shape[dim] % block_size == 0, (\n",
    "                    f\"Tensor dimension {dim}, {input.shape[dim]} is not divisible by {block_size} even after padding.\"\n",
    "                )\n",
    "                # The scales tensor is expected to have size equal to input.shape[dim] // block_size.\n",
    "                expected_shape[dim] = input.shape[dim] // block_size\n",
    "\n",
    "            # Verify that the provided scales shape matches the expected shape.\n",
    "            assert scales.shape == tuple(expected_shape), (\n",
    "                f\"Mismatch in expected scale shape: {scales.shape} vs {tuple(expected_shape)}\"\n",
    "            )\n",
    "            \n",
    "            # Expand scales along each block dimension for broadcasting.\n",
    "            for dim, block_size in block_sizes.items():\n",
    "                expanded_scales = expanded_scales.repeat_interleave(block_size, dim=dim)\n",
    "                print(scales.shape, expanded_scales.shape)\n",
    "\n",
    "        # Perform quantization using FP8 (E4M3) format.\n",
    "        quantized_data = (input / expanded_scales).to(torch.float8_e4m3fn)\n",
    "\n",
    "        # Crop quantized_data back to the original shape (if padding was added).\n",
    "        slices = tuple(slice(0, dim) for dim in original_input.shape)\n",
    "        quantized_data_cropped = quantized_data[slices]\n",
    "\n",
    "\n",
    "import tracemalloc\n",
    "tracemalloc.start()\n",
    "t = torch.randn(4, 9000, 9000)\n",
    "block_sizes = {-1:128}\n",
    "quantize(t, block_sizes=block_sizes)\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current: {current / 1024:.2f} KB; Peak: {peak / 1024:.2f} KB\")\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Union\n",
    "from modelopt.torch.quantization.utils import reduce_block_padding, reduce_amax, reduce_block_amax, convert_quantization_axis_to_reduce_axis\n",
    "\n",
    "def int_quantize(input: torch.Tensor, block_size: int) -> torch.Tensor:\n",
    "        \"\"\"Converting a tensor to a quantized format based on INT4 (AWQ) quantization.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): The input tensor to be quantized.\n",
    "            block_size (int): The size of each block for quantization.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains quantized data, input quantization config, and scale quantization config.\n",
    "        \"\"\"\n",
    "\n",
    "        scale_quant_maxbound = 2 ** (4 - 1) - 1\n",
    "\n",
    "        # pad the input if needed\n",
    "        original_input = input\n",
    "        input = reduce_block_padding(input.view(-1), block_sizes={-1: block_size})\n",
    "\n",
    "        print(input.shape)\n",
    "        # get scales for each block\n",
    "        block_input = input.view(-1, block_size)\n",
    "        print(block_input.shape)\n",
    "        scales = scale_quant_maxbound / reduce_amax(block_input, -1)\n",
    "        print(scales.shape)\n",
    "        # expand scalers to match shape of input\n",
    "        scales = scales.view(block_input.shape[0], -1)  # shape: (block_input.shape[0], 1)\n",
    "\n",
    "        scaled_blocks = block_input * scales\n",
    "        flattened = scaled_blocks.flatten()\n",
    "        # uint4: 0 - 15\n",
    "        flattened = flattened.round().clamp(\n",
    "            -(scale_quant_maxbound + 1), scale_quant_maxbound\n",
    "        ) + (scale_quant_maxbound + 1)\n",
    "        flattened = flattened.to(torch.uint8)\n",
    "\n",
    "        packed_output_uint8 = torch.empty(\n",
    "            input.numel() // 2, dtype=torch.uint8, device=input.device\n",
    "        )\n",
    "        # pack the int4 weights into a uint8 tensor\n",
    "        # packing format: w0, w1, w2, w3, w4, w5, ...\n",
    "        #               | byte  | byte  | byte  |\n",
    "        packed_output_uint8 = flattened[::2] << 4 | flattened[1::2]\n",
    "\n",
    "\n",
    "import tracemalloc\n",
    "tracemalloc.start()\n",
    "t = torch.randn(4, 9000, 9000)\n",
    "block_sizes = {-1:128}\n",
    "int_quantize(t, block_sizes[-1])\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current: {current / 1024:.2f} KB; Peak: {peak / 1024:.2f} KB\")\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(1024, 1024)\n",
    "        self.l2 = nn.Linear(1024, 4096)\n",
    "        self.l3 = nn.Linear(4096, 1024)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l3(self.l2(self.l1(x)))\n",
    "\n",
    "model = DummyModel()\n",
    "\n",
    "# get memory footprint\n",
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total += param.numel() * param.element_size()\n",
    "total/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelopt.torch.quantization as mtq\n",
    "\n",
    "config = mtq.FP8_DEFAULT_CFG\n",
    "config['quant_cfg']['*weight_quantizer']['fake_quant'] = False\n",
    "print(config)\n",
    "model = mtq.quantize(model, config)\n",
    "mtq.compress(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total += param.numel() * param.element_size()\n",
    "total/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "class AttentionSeva(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_dim: int,\n",
    "        context_dim: int | None = None,\n",
    "        heads: int = 8,\n",
    "        dim_head: int = 64,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = context_dim or query_dim\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, context: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        q = self.to_q(x)\n",
    "        context = context if context is not None else x\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b l (h d) -> b h l d\", h=self.heads),\n",
    "            (q, k, v),\n",
    "        )\n",
    "        with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "            out = F.scaled_dot_product_attention(q, k, v)\n",
    "        out = rearrange(out, \"b h l d -> b l (h d)\")\n",
    "        out = self.to_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.attention import Attention\n",
    "\n",
    "model1 = AttentionSeva(1024, 1024, 8, 128)\n",
    "model2 = Attention(1024, 1024, 8, dim_head=128, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "x = torch.randn(16, 64, 64)\n",
    "y = x.view(x.shape[0]//4, 4*x.shape[1],x.shape[2])\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "x = rearrange(x, \"(b t) (h w) c -> b (t h w) c\", t=4, h=8, w=8)\n",
    "print(x.shape)\n",
    "\n",
    "torch.allclose(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the base folder to search\n",
    "BASE_DIR = Path(\"./src/diffusers/pipelines\")\n",
    "\n",
    "class PipelinePropertyExtractor(ast.NodeVisitor):\n",
    "    def __init__(self):\n",
    "        self.pipeline_classes = {}\n",
    "\n",
    "    def visit_ClassDef(self, node):\n",
    "        inherits_from_pipeline = any(\n",
    "            isinstance(base, ast.Name) and base.id == \"DiffusionPipeline\"\n",
    "            for base in node.bases\n",
    "        )\n",
    "        if inherits_from_pipeline:\n",
    "            properties = []\n",
    "            for item in node.body:\n",
    "                if isinstance(item, ast.FunctionDef):\n",
    "                    for decorator in item.decorator_list:\n",
    "                        if isinstance(decorator, ast.Name) and decorator.id == \"property\":\n",
    "                            properties.append(item.name)\n",
    "            self.pipeline_classes[node.name] = properties\n",
    "        self.generic_visit(node)\n",
    "\n",
    "def extract_pipeline_properties_from_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        tree = ast.parse(f.read(), filename=str(file_path))\n",
    "        extractor = PipelinePropertyExtractor()\n",
    "        extractor.visit(tree)\n",
    "        return extractor.pipeline_classes\n",
    "\n",
    "all_pipeline_properties = {}\n",
    "\n",
    "for py_file in BASE_DIR.rglob(\"*.py\"):\n",
    "    props = extract_pipeline_properties_from_file(py_file)\n",
    "    if props:\n",
    "        all_pipeline_properties[str(py_file)] = props\n",
    "\n",
    "master = {\n",
    "    \"_guidance_scale\":  1.0,\n",
    "    \"clip_skip\":  None,\n",
    "    \"_interrupt\":  False,\n",
    "    \"_cross_attention_kwargs\":  None,\n",
    "    \"_num_timesteps\":  0,\n",
    "    \"_attention_kwargs\":  None,\n",
    "    \"_guidance_rescale\":  0.0,\n",
    "    \"_denoising_start\":  None,\n",
    "    \"_denoising_end\":  None,\n",
    "    \"_joint_attention_kwargs\":  None,\n",
    "    \"_pag_scale\":  0.0,\n",
    "    \"_adaptive_pag_scale\":  0.0,\n",
    "    \"_current_timestep\":  None\n",
    "}\n",
    "def process_file(file_path, cls_name, block):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    modified_lines = []\n",
    "    inside_target_class = False\n",
    "    inside_init = False\n",
    "    indent = \"\"\n",
    "    found_super = False\n",
    "    inside_call = False\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if stripped.startswith(\"class \"+cls_name):\n",
    "            inside_target_class = True\n",
    "            cls_i = i\n",
    "\n",
    "        if inside_target_class and stripped.startswith(\"def __init__\"):\n",
    "            inside_init = True\n",
    "            \n",
    "        if inside_init and \"super().__init__()\" in stripped:\n",
    "            indent = line[:line.index(\"super()\")]\n",
    "            modified_lines.append(line)\n",
    "            for val in block:\n",
    "                modified_lines.append(f\"{indent}{val}\\n\")\n",
    "            found_super = True\n",
    "            continue\n",
    "\n",
    "        if inside_init and stripped.startswith(\"def \") and \"def __init__\" not in stripped:\n",
    "            inside_init = False\n",
    "\n",
    "        if not inside_init and inside_target_class and \"def __call__\" in stripped:\n",
    "            inside_call = True\n",
    "            interrupt_cnt = 0\n",
    "            current_timestep_cnt = 0\n",
    "\n",
    "        if inside_call:\n",
    "            if stripped.startswith('self._interrupt = False') and interrupt_cnt == 0:\n",
    "                interrupt_cnt += 1\n",
    "                continue\n",
    "            if stripped.startswith('self._current_timestep = None') and current_timestep_cnt == 0:\n",
    "                current_timestep_cnt += 1\n",
    "                continue\n",
    "            if interrupt_cnt == 1 and current_timestep_cnt == 1:\n",
    "                inside_call = False\n",
    "            \n",
    "\n",
    "        if inside_target_class and stripped.startswith(\"class \") and i != cls_i:\n",
    "            inside_target_class = False\n",
    "            inside_init = False\n",
    "\n",
    "        modified_lines.append(line)\n",
    "\n",
    "    if found_super:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(modified_lines)\n",
    "        print(f\"âœ… Modified: {file_path}\")\n",
    "\n",
    "# Print results\n",
    "for file, classes in all_pipeline_properties.items():\n",
    "    print(f\"\\n{file}:\")\n",
    "    for cls, props in classes.items():\n",
    "        print(f\"  Class `{cls}`:\")\n",
    "        block = []\n",
    "        for prop in props:\n",
    "            if '_'+prop in master:\n",
    "                block.append(\"self._\"+prop+\"=\"+str(master.get(\"_\"+prop)))\n",
    "            print(f\"    - @{prop}\")\n",
    "        process_file(file, cls, block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import SanaTransformer2DModel\n",
    "from diffusers.quantizers.quantization_config import NVIDIAModelOptConfig\n",
    "\n",
    "checkpoint = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "model_cls = SanaTransformer2DModel\n",
    "input = lambda _: torch.randn((2, 32, 32, 32), dtype=torch.bfloat16), torch.randn((2,10,300,2304), dtype=torch.bfloat16), torch.Tensor([0,0])\n",
    "\n",
    "quant_config_fp8 = {\"quant_type\": \"FP8\", \"quant_method\": \"modelopt\"}\n",
    "quant_config_int4 = {\"quant_type\": \"INT4\", \"quant_method\": \"modelopt\", \"block_quantize\": 128, \"channel_quantize\": -1}\n",
    "quant_config_nvfp4 = {\"quant_type\": \"NVFP4\", \"quant_method\": \"modelopt\", \"block_quantize\": 128, \"channel_quantize\": -1, \"modules_to_not_convert\": [\"conv\"]}\n",
    "\n",
    "def test_quantization(config, checkpoint, model_cls):\n",
    "    quant_config = NVIDIAModelOptConfig(**config)\n",
    "    print(quant_config.get_config_from_quant_type())\n",
    "    quant_model = model_cls.from_pretrained(checkpoint, subfolder=\"transformer\", quantization_config=quant_config, torch_dtype=torch.bfloat16).to('cuda')\n",
    "    print(f\"Quant {config['quant_type']} Model Memory Footprint: \", quant_model.get_memory_footprint() / 1e6)\n",
    "    return quant_model\n",
    "\n",
    "def test_quant_inference(model, input, iter=100):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    inference_memory = 0\n",
    "    for _ in range(iter):\n",
    "        hidden.to('cuda'), encoder.to('cuda'), timestep.to('cuda') = input()\n",
    "        with torch.no_grad():\n",
    "            output = model(hidden, encoder, timestep)\n",
    "        inference_memory += torch.cuda.max_memory_allocated()\n",
    "    inference_memory /= iter\n",
    "    print(\"Inference Memory: \", inference_memory / 1e6)\n",
    "\n",
    "\n",
    "test_quant_inference(test_quantization(quant_config_fp8, checkpoint, model_cls), input)\n",
    "# test_quant_inference(test_quantization(quant_config_int4, checkpoint, model_cls), input)\n",
    "# test_quant_inference(test_quantization(quant_config_nvfp4, checkpoint, model_cls), input)\n",
    "# test_quant_inference(model_cls.from_pretrained(checkpoint, subfolder=\"transformer\", torch_dtype=torch.bfloat16).to('cuda'), input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from typing import Union, Tuple\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "\n",
    "def get_resizing_factor(\n",
    "    target_shape: Tuple[int, int],  # H, W\n",
    "    current_shape: Tuple[int, int],  # H, W\n",
    "    cover_target: bool = True,\n",
    "    # If True, the output shape will fully cover the target shape.\n",
    "    # If No, the target shape will fully cover the output shape.\n",
    ") -> float:\n",
    "    r_bound = target_shape[1] / target_shape[0]\n",
    "    aspect_r = current_shape[1] / current_shape[0]\n",
    "    if r_bound >= 1.0:\n",
    "        if cover_target:\n",
    "            if aspect_r >= r_bound:\n",
    "                factor = min(target_shape) / min(current_shape)\n",
    "            elif aspect_r < 1.0:\n",
    "                factor = max(target_shape) / min(current_shape)\n",
    "            else:\n",
    "                factor = max(target_shape) / max(current_shape)\n",
    "        else:\n",
    "            if aspect_r >= r_bound:\n",
    "                factor = max(target_shape) / max(current_shape)\n",
    "            elif aspect_r < 1.0:\n",
    "                factor = min(target_shape) / max(current_shape)\n",
    "            else:\n",
    "                factor = min(target_shape) / min(current_shape)\n",
    "    else:\n",
    "        if cover_target:\n",
    "            if aspect_r <= r_bound:\n",
    "                factor = min(target_shape) / min(current_shape)\n",
    "            elif aspect_r > 1.0:\n",
    "                factor = max(target_shape) / min(current_shape)\n",
    "            else:\n",
    "                factor = max(target_shape) / max(current_shape)\n",
    "        else:\n",
    "            if aspect_r <= r_bound:\n",
    "                factor = max(target_shape) / max(current_shape)\n",
    "            elif aspect_r > 1.0:\n",
    "                factor = min(target_shape) / max(current_shape)\n",
    "            else:\n",
    "                factor = min(target_shape) / min(current_shape)\n",
    "    return factor\n",
    "def get_wh_with_fixed_shortest_side(w, h, size):\n",
    "    # size is smaller or equal to zero, we return original w h\n",
    "    if size is None or size <= 0:\n",
    "        return w, h\n",
    "    if w < h:\n",
    "        new_w = size\n",
    "        new_h = int(size * h / w)\n",
    "    else:\n",
    "        new_h = size\n",
    "        new_w = int(size * w / h)\n",
    "    return new_w, new_h\n",
    "\n",
    "def transform_img_and_K(\n",
    "    image: torch.Tensor,\n",
    "    size: Union[int, Tuple[int, int]],\n",
    "    scale: float = 1.0,\n",
    "    center: Tuple[float, float] = (0.5, 0.5),\n",
    "    K: torch.Tensor | None = None,\n",
    "    size_stride: int = 1,\n",
    "    mode: str = \"crop\",\n",
    "):\n",
    "    assert mode in [\n",
    "        \"crop\",\n",
    "        \"pad\",\n",
    "        \"stretch\",\n",
    "    ], f\"mode should be one of ['crop', 'pad', 'stretch'], got {mode}\"\n",
    "\n",
    "    h, w = image.shape[-2:]\n",
    "    if isinstance(size, (tuple, list)):\n",
    "        # => if size is a tuple or list, we first rescale to fully cover the `size`\n",
    "        # area and then crop the `size` area from the rescale image\n",
    "        W, H = size\n",
    "    else:\n",
    "        # => if size is int, we rescale the image to fit the shortest side to size\n",
    "        # => if size is None, no rescaling is applied\n",
    "        W, H = get_wh_with_fixed_shortest_side(w, h, size)\n",
    "    W, H = (\n",
    "        math.floor(W / size_stride + 0.5) * size_stride,\n",
    "        math.floor(H / size_stride + 0.5) * size_stride,\n",
    "    )\n",
    "\n",
    "    if mode == \"stretch\":\n",
    "        rh, rw = H, W\n",
    "    else:\n",
    "        rfs = get_resizing_factor(\n",
    "            (H, W),\n",
    "            (h, w),\n",
    "            cover_target=mode != \"pad\",\n",
    "        )\n",
    "        (rh, rw) = [int(np.ceil(rfs * s)) for s in (h, w)]\n",
    "\n",
    "    rh, rw = int(rh / scale), int(rw / scale)\n",
    "    image = torch.nn.functional.interpolate(\n",
    "        image, (rh, rw), mode=\"area\", antialias=False\n",
    "    )\n",
    "\n",
    "    cy_center = int(center[1] * image.shape[-2])\n",
    "    cx_center = int(center[0] * image.shape[-1])\n",
    "    if mode != \"pad\":\n",
    "        ct = max(0, cy_center - H // 2)\n",
    "        cl = max(0, cx_center - W // 2)\n",
    "        ct = min(ct, image.shape[-2] - H)\n",
    "        cl = min(cl, image.shape[-1] - W)\n",
    "        image = TF.crop(image, top=ct, left=cl, height=H, width=W)\n",
    "        pl, pt = 0, 0\n",
    "    else:\n",
    "        pt = max(0, H // 2 - cy_center)\n",
    "        pl = max(0, W // 2 - cx_center)\n",
    "        pb = max(0, H - pt - image.shape[-2])\n",
    "        pr = max(0, W - pl - image.shape[-1])\n",
    "        image = TF.pad(\n",
    "            image,\n",
    "            [pl, pt, pr, pb],\n",
    "        )\n",
    "        cl, ct = 0, 0\n",
    "\n",
    "    if K is not None:\n",
    "        K = K.clone()\n",
    "        # K[:, :2, 2] += K.new_tensor([pl, pt])\n",
    "        if torch.all(K[:, :2, -1] >= 0) and torch.all(K[:, :2, -1] <= 1):\n",
    "            K[:, :2] *= K.new_tensor([rw, rh])[None, :, None]  # normalized K\n",
    "        else:\n",
    "            K[:, :2] *= K.new_tensor([rw / w, rh / h])[None, :, None]  # unnormalized K\n",
    "        K[:, :2, 2] += K.new_tensor([pl - cl, pt - ct])\n",
    "\n",
    "    return image, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.image_processor import VaeImageProcessor, SevaImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import imageio.v3 as iio\n",
    "\n",
    "path = \"/home/ishan/stable-virtual-camera/data/vgg-lab-4_0.png\"\n",
    "\n",
    "img = Image.open(path)\n",
    "height, width = (576, 576)\n",
    "image_processor = VaeImageProcessor(\n",
    "    vae_scale_factor=64,\n",
    "    do_normalize=False,\n",
    ")\n",
    "image_processor1 = SevaImageProcessor(\n",
    "    vae_scale_factor=64,\n",
    "    do_normalize=False,\n",
    ")\n",
    "img1 = image_processor.preprocess(img, resize_mode=\"fill\", height=height, width=width)\n",
    "\n",
    "img = [torch.as_tensor(\n",
    "    iio.imread(path) / 255.0, dtype=torch.float32\n",
    ")]\n",
    "img = torch.stack(img, dim=0)\n",
    "img2 = transform_img_and_K(\n",
    "    img.permute(0, 3, 1, 2),\n",
    "    (576, 576),\n",
    "    K=None,\n",
    "    size_stride=64,\n",
    "    mode=\"pad\"\n",
    ")[0]\n",
    "\n",
    "print(img1.shape, img2.shape)\n",
    "\n",
    "print(torch.allclose(img1, img2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.seva.geometry import get_default_intrinsics\n",
    "\n",
    "input_Ks = get_default_intrinsics(\n",
    "    aspect_ratio=0.74,\n",
    "    focal_length=1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ishan/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdiffusers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipelines\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseva\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline_seva\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m test\n\u001b[32m      3\u001b[39m t = torch.randn((\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m,\u001b[32m1024\u001b[39m,\u001b[32m1024\u001b[39m))\n\u001b[32m      4\u001b[39m test(t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/diffusers/pipelines/seva/pipeline_seva.py:81\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     63\u001b[39m EXAMPLE_DOC_STRING = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[33m    Examples:\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[33m        ```py\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33m        ```\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m image_encoder = \u001b[43mCLIPVisionModelWithProjection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlaion/CLIP-ViT-H-14-laion2B-s32B-b79K\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     83\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_timesteps\u001b[39m(\n\u001b[32m     88\u001b[39m     scheduler,\n\u001b[32m     89\u001b[39m     num_inference_steps: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     93\u001b[39m     **kwargs,\n\u001b[32m     94\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4317\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4313\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4315\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4317\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4319\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4324\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4330\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4333\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4336\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1027\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1013\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1014\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1015\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1016\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1026\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1029\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1031\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1032\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    439\u001b[39m         snapshot_download(\n\u001b[32m    440\u001b[39m             path_or_repo_id,\n\u001b[32m    441\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    451\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:961\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    941\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    942\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    943\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m         local_files_only=local_files_only,\n\u001b[32m    959\u001b[39m     )\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1112\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1110\u001b[39m Path(lock_path).parent.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1125\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1675\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1669\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1670\u001b[39m             logger.warning(\n\u001b[32m   1671\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1672\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1673\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1674\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1675\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1680\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1681\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1682\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1684\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1685\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:449\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    447\u001b[39m new_resume_size = resume_size\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    451\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/urllib3/response.py:628\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp):\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m    631\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/urllib3/response.py:567\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    564\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    569\u001b[39m         flush_decoder = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode-workspace/diffusers/.venv/lib/python3.12/site-packages/urllib3/response.py:533\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer.getvalue()\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.8/lib/python3.12/ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from diffusers.pipelines.seva.pipeline_seva import test\n",
    "\n",
    "t = torch.randn((1,3,1024,1024))\n",
    "test(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
